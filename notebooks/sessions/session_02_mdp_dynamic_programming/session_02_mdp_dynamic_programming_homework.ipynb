{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e75615cf",
   "metadata": {},
   "source": [
    "![Logo](https://raw.githubusercontent.com/BartaZoltan/deep-reinforcement-learning-course/main/notebooks/shared_assets/logo.png)\n",
    "\n",
    "# Practice 2 Homework: Jack's Car Rental (Policy Iteration)\n",
    "\n",
    "**Developers:** Domonkos Nagy, Balazs Nagy, Zoltan Barta  \n",
    "**Date:** 2026-02-25  \n",
    "**Version:** 2025-26/2\n",
    "\n",
    "[<img src=\"https://colab.research.google.com/assets/colab-badge.svg\">](https://colab.research.google.com/github/BartaZoltan/deep-reinforcement-learning-course/blob/main/notebooks/sessions/session_02_mdp_dynamic_programming/session_02_mdp_dynamic_programming_homework.ipynb)\n",
    "\n",
    "## Summary\n",
    "\n",
    "This homework implements Policy Iteration for Jack's Car Rental (Sutton & Barto, Ch. 4).\n",
    "\n",
    "Content outline:\n",
    "- Poisson demand/return model,\n",
    "- expected return computation for state-action pairs,\n",
    "- iterative policy evaluation,\n",
    "- greedy policy improvement,\n",
    "- final policy/value visualization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c71a83",
   "metadata": {},
   "source": [
    "## Task Description: Jack's Car Rental\n",
    "\n",
    "Jack manages two car rental locations. Every night he can move cars between locations (max 5 cars per night). During the next day, rentals and returns happen stochastically at both locations.\n",
    "\n",
    "Your task is to solve this finite MDP with **Policy Iteration**:\n",
    "1. implement expected return for a given state-action pair,\n",
    "2. implement policy evaluation,\n",
    "3. implement greedy policy improvement,\n",
    "4. iterate until the policy becomes stable.\n",
    "\n",
    "### Model details\n",
    "- State: `(cars_at_A, cars_at_B)` with each value in `[0, 20]`.\n",
    "- Action: cars moved overnight from A to B, integer in `[-5, 5]`.\n",
    "- Reward: `$10` per rented car, movement cost `$2` per moved car.\n",
    "- Extension rules (same as lecture notebook):\n",
    "  - If action > 0 (move A->B), one moved car is free.\n",
    "  - Parking cost `$4` at each location if cars there exceed 10 after moving.\n",
    "- Discount factor: `gamma = 0.9`.\n",
    "\n",
    "This assignment is aligned with Chapter 4 of Sutton & Barto {cite}`sutton2018`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c98bf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "\n",
    "np.set_printoptions(precision=1, suppress=True, linewidth=160)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4cc46d",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "The next cells define constants and helper functions used by all tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37a96b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "\n",
    "MAX_CARS = 20\n",
    "MAX_MOVE = 5\n",
    "GAMMA = 0.9\n",
    "\n",
    "RENT_REWARD = 10\n",
    "MOVE_COST = 2\n",
    "PARKING_COST = 4\n",
    "\n",
    "LAMBDA_RENTALS_A = 3\n",
    "LAMBDA_RENTALS_B = 4\n",
    "LAMBDA_RETURNS_A = 3\n",
    "LAMBDA_RETURNS_B = 2\n",
    "\n",
    "POISSON_LIMIT = 11  # probability mass above this is ignored\n",
    "\n",
    "N_STATES = MAX_CARS + 1\n",
    "ACTIONS = np.arange(-MAX_MOVE, MAX_MOVE + 1)\n",
    "\n",
    "V = np.zeros((N_STATES, N_STATES), dtype=float)\n",
    "policy = np.zeros((N_STATES, N_STATES), dtype=int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ab425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "\n",
    "poisson_cache = {}\n",
    "\n",
    "def poisson_prob(lmbda: int, n: int) -> float:\n",
    "    key = (lmbda, n)\n",
    "    if key not in poisson_cache:\n",
    "        poisson_cache[key] = (lmbda ** n) * math.exp(-lmbda) / math.factorial(n)\n",
    "    return poisson_cache[key]\n",
    "\n",
    "for lam in [LAMBDA_RENTALS_A, LAMBDA_RENTALS_B, LAMBDA_RETURNS_A, LAMBDA_RETURNS_B]:\n",
    "    for n in range(POISSON_LIMIT):\n",
    "        poisson_prob(lam, n)\n",
    "\n",
    "def is_action_legal(state, action) -> bool:\n",
    "    cars_a, cars_b = state\n",
    "    cars_a_after_move = cars_a - action\n",
    "    cars_b_after_move = cars_b + action\n",
    "    return (\n",
    "        0 <= cars_a_after_move <= MAX_CARS\n",
    "        and 0 <= cars_b_after_move <= MAX_CARS\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efb07b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "\n",
    "def plot_policy(policy_arr, title=\"Policy (cars moved A->B)\"):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(\n",
    "        np.flip(policy_arr, axis=0),\n",
    "        cmap=\"vlag\",\n",
    "        linecolor=\"white\",\n",
    "        linewidths=0.05,\n",
    "        square=True,\n",
    "        yticklabels=np.arange(MAX_CARS, -1, -1),\n",
    "        cbar_kws={\"label\": \"Action\"},\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.ylabel(\"Cars at A\")\n",
    "    plt.xlabel(\"Cars at B\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_values(V_arr, title=\"State-value function\"):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(\n",
    "        np.flip(V_arr, axis=0),\n",
    "        cmap=\"viridis\",\n",
    "        linecolor=\"white\",\n",
    "        linewidths=0.05,\n",
    "        square=True,\n",
    "        yticklabels=np.arange(MAX_CARS, -1, -1),\n",
    "        cbar_kws={\"label\": \"V(s)\"},\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.ylabel(\"Cars at A\")\n",
    "    plt.xlabel(\"Cars at B\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504224f8",
   "metadata": {},
   "source": [
    "## Task 1: Implement Expected Return for `(state, action)`\n",
    "\n",
    "Complete `expected_return(state, action, V)` using the Poisson loops.\n",
    "\n",
    "Requirements:\n",
    "- illegal actions return `-np.inf`,\n",
    "- apply movement reward/cost with extension rules,\n",
    "- compute expected one-step reward + discounted next-state value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a0e638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_return(state, action, V_table):\n",
    "    ########################################################################\n",
    "    # TODO: implement expected return for one state-action pair\n",
    "    # Hint: follow the structure from lecture notebook: rentals -> returns loops\n",
    "    # and aggregate prob * (reward + gamma * V(next_state)).\n",
    "\n",
    "    # 1) Check legality\n",
    "    # 2) Compute action-dependent immediate costs/rewards\n",
    "    # 3) Sum over rentals and returns (Poisson)\n",
    "\n",
    "    raise NotImplementedError(\"Task 1: implement expected_return\")\n",
    "    ########################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e8cf41",
   "metadata": {},
   "source": [
    "## Task 2: Implement Policy Evaluation\n",
    "\n",
    "Evaluate the current deterministic policy until `delta < theta`.\n",
    "\n",
    "Use in-place updates over all states:\n",
    "- `old_v = V[s]`\n",
    "- `V[s] = expected_return(s, policy[s], V)`\n",
    "- `delta = max(delta, abs(old_v - V[s]))`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4eeac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(V_table, policy_table, theta=1e-2):\n",
    "    ########################################################################\n",
    "    # TODO: iterative policy evaluation\n",
    "\n",
    "    raise NotImplementedError(\"Task 2: implement policy_evaluation\")\n",
    "    ########################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d478e0b8",
   "metadata": {},
   "source": [
    "## Task 3: Implement Policy Improvement and Full Policy Iteration\n",
    "\n",
    "Improve policy greedily with respect to current `V`, then repeat evaluation+improvement until stable.\n",
    "\n",
    "Tie-handling rule: if multiple actions are exactly best, selecting any of them is acceptable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0786a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(V_table, policy_table):\n",
    "    ########################################################################\n",
    "    # TODO: greedy policy improvement over all states\n",
    "    # Return True if policy is stable, else False\n",
    "\n",
    "    raise NotImplementedError(\"Task 3: implement policy_improvement\")\n",
    "    ########################################################################\n",
    "\n",
    "\n",
    "# After completing the tasks above, run policy iteration below.\n",
    "# You can reduce max_iterations while debugging.\n",
    "max_iterations = 20\n",
    "\n",
    "for it in range(1, max_iterations + 1):\n",
    "    display.clear_output(wait=True)\n",
    "    print(f\"Policy Iteration outer loop: {it}\")\n",
    "\n",
    "    policy_evaluation(V, policy, theta=1e-2)\n",
    "    stable = policy_improvement(V, policy)\n",
    "\n",
    "    if stable:\n",
    "        print(\"Policy converged.\")\n",
    "        break\n",
    "\n",
    "plot_policy(policy, title=\"Optimal policy (cars moved A->B)\")\n",
    "plot_values(V, title=\"Estimated optimal state values\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef9c549",
   "metadata": {},
   "source": [
    "## Optional checks\n",
    "\n",
    "- Verify your policy heatmap has structured regions (not random noise).\n",
    "- Try changing `theta` and compare runtime vs final policy changes.\n",
    "- Try removing extension rules (free transfer / parking penalty) and compare policies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97382885",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction* (2nd ed.), Chapter 4.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
