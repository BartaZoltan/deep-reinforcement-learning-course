{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "926317d8",
      "metadata": {},
      "source": [
        "![Logo](https://github.com/BartaZoltan/deep-reinforcement-learning-course/blob/main/website/assets/logo.png?raw=1)\n\nMade by **Zoltán Barta**\n\n[<img src=\"https://colab.research.google.com/assets/colab-badge.svg\">](https://colab.research.google.com/github/BartaZoltan/deep-reinforcement-learning-course/blob/main/notebooks/sessions/session_02_mdp_dynamic_programming/session_02_mdp_dynamic_programming.ipynb)\n\n# Markov Decision Processes\n\nThis practice session follows Chapters 3–4 of *Reinforcement Learning: An Introduction (2nd ed.)* by Sutton & Barto.\n\nFocus: finite MDPs + planning with a known model via **Dynamic Programming**:\n- Iterative policy evaluation\n- Policy iteration\n- Value iteration\n\nWe’ll test these on:\n- A small **Gridworld** (deterministic dynamics, step cost)\n- The **Gambler’s Problem** (value iteration + optimal policy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b951461",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple, Iterable, Optional\n",
        "\n",
        "np.set_printoptions(precision=3, suppress=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fa333b0",
      "metadata": {},
      "source": [
        "## A tiny tabular MDP representation\n",
        "\n",
        "We’ll store the dynamics as a list of transitions per $(s,a)$:\n",
        "- each transition is $(p, s', r, done)$\n",
        "- terminal states are handled via `done=True` transitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5e64076",
      "metadata": {},
      "outputs": [],
      "source": [
        "Transition = Tuple[float, int, float, bool]  # (p, s_next, reward, done)\n",
        "\n",
        "@dataclass\n",
        "class TabularMDP:\n",
        "    nS: int\n",
        "    nA: int\n",
        "    P: List[List[List[Transition]]]  # P[s][a] = list of transitions\n",
        "    gamma: float = 1.0\n",
        "\n",
        "    def transitions(self, s: int, a: int) -> List[Transition]:\n",
        "        return self.P[s][a]\n",
        "\n",
        "\n",
        "def is_stochastic_policy(policy: np.ndarray, nS: int, nA: int) -> None:\n",
        "    assert policy.shape == (nS, nA)\n",
        "    row_sums = policy.sum(axis=1)\n",
        "    if not np.allclose(row_sums, 1.0):\n",
        "        raise ValueError('Policy rows must sum to 1.')\n",
        "    if np.any(policy < -1e-12):\n",
        "        raise ValueError('Policy must have non-negative probabilities.')\n",
        "\n",
        "\n",
        "def greedy_policy_from_q(Q: np.ndarray, tie_break: str = 'uniform') -> np.ndarray:\n",
        "    \"\"\"Return a deterministic (or uniform-tie) greedy policy from Q[s,a].\"\"\"\n",
        "    nS, nA = Q.shape\n",
        "    policy = np.zeros((nS, nA), dtype=float)\n",
        "    best_actions = (Q == Q.max(axis=1, keepdims=True))\n",
        "    if tie_break == 'uniform':\n",
        "        counts = best_actions.sum(axis=1, keepdims=True)\n",
        "        policy[best_actions] = (1.0 / counts)[best_actions]\n",
        "    elif tie_break == 'first':\n",
        "        a_star = np.argmax(Q, axis=1)\n",
        "        policy[np.arange(nS), a_star] = 1.0\n",
        "    else:\n",
        "        raise ValueError(\"tie_break must be 'uniform' or 'first'\")\n",
        "    return policy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab9ca7cb",
      "metadata": {},
      "source": [
        "## Dynamic Programming algorithms (planning with a known model)\n",
        "\n",
        "All methods below assume the MDP dynamics $p(s',r\\mid s,a)$ are known."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92b7c06f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def policy_evaluation(\n",
        "    mdp: TabularMDP,\n",
        "    policy: np.ndarray,\n",
        "    theta: float = 1e-10,\n",
        "    max_iterations: int = 1_000_000,\n",
        "    V0: Optional[np.ndarray] = None,\n",
        "    in_place: bool = True,\n",
        "):\n",
        "    \"\"\"Iterative policy evaluation for V^pi.\n",
        "\n",
        "    Args:\n",
        "        mdp: TabularMDP\n",
        "        policy: shape (nS, nA), stochastic policy\n",
        "        theta: stopping tolerance on max|V_new - V_old|\n",
        "        max_iterations: safety cap\n",
        "        V0: optional initial value function\n",
        "        in_place: if True, update V(s) immediately (Gauss-Seidel-ish)\n",
        "    \"\"\"\n",
        "    nS, nA = mdp.nS, mdp.nA\n",
        "    is_stochastic_policy(policy, nS, nA)\n",
        "    V = np.zeros(nS) if V0 is None else V0.astype(float).copy()\n",
        "    for it in range(max_iterations):\n",
        "        delta = 0.0\n",
        "        if in_place:\n",
        "            for s in range(nS):\n",
        "                v_old = V[s]\n",
        "                v_new = 0.0\n",
        "                for a in range(nA):\n",
        "                    pi = policy[s, a]\n",
        "                    if pi == 0.0:\n",
        "                        continue\n",
        "                    for p, s2, r, done in mdp.transitions(s, a):\n",
        "                        v_new += pi * p * (r + (0.0 if done else mdp.gamma * V[s2]))\n",
        "                V[s] = v_new\n",
        "                delta = max(delta, abs(v_old - v_new))\n",
        "        else:\n",
        "            V_new = V.copy()\n",
        "            for s in range(nS):\n",
        "                v_new = 0.0\n",
        "                for a in range(nA):\n",
        "                    pi = policy[s, a]\n",
        "                    if pi == 0.0:\n",
        "                        continue\n",
        "                    for p, s2, r, done in mdp.transitions(s, a):\n",
        "                        v_new += pi * p * (r + (0.0 if done else mdp.gamma * V[s2]))\n",
        "                V_new[s] = v_new\n",
        "            delta = np.max(np.abs(V_new - V))\n",
        "            V = V_new\n",
        "        if delta < theta:\n",
        "            return V, it + 1\n",
        "    return V, max_iterations\n",
        "\n",
        "\n",
        "def q_from_v(mdp: TabularMDP, V: np.ndarray) -> np.ndarray:\n",
        "    nS, nA = mdp.nS, mdp.nA\n",
        "    Q = np.zeros((nS, nA), dtype=float)\n",
        "    for s in range(nS):\n",
        "        for a in range(nA):\n",
        "            q = 0.0\n",
        "            for p, s2, r, done in mdp.transitions(s, a):\n",
        "                q += p * (r + (0.0 if done else mdp.gamma * V[s2]))\n",
        "            Q[s, a] = q\n",
        "    return Q\n",
        "\n",
        "\n",
        "def policy_iteration(\n",
        "    mdp: TabularMDP,\n",
        "    theta: float = 1e-10,\n",
        "    max_policy_iterations: int = 10_000,\n",
        "    tie_break: str = 'uniform',\n",
        "):\n",
        "    \"\"\"Classic policy iteration: evaluate -> greedy improvement until stable.\"\"\"\n",
        "    nS, nA = mdp.nS, mdp.nA\n",
        "    policy = np.ones((nS, nA), dtype=float) / nA\n",
        "    V = np.zeros(nS, dtype=float)\n",
        "    eval_iters_total = 0\n",
        "\n",
        "    for pi_it in range(max_policy_iterations):\n",
        "        V, eval_iters = policy_evaluation(mdp, policy, theta=theta, V0=V, in_place=True)\n",
        "        eval_iters_total += eval_iters\n",
        "        Q = q_from_v(mdp, V)\n",
        "        new_policy = greedy_policy_from_q(Q, tie_break=tie_break)\n",
        "        if np.allclose(new_policy, policy):\n",
        "            return policy, V, pi_it + 1, eval_iters_total\n",
        "        policy = new_policy\n",
        "    return policy, V, max_policy_iterations, eval_iters_total\n",
        "\n",
        "\n",
        "def value_iteration(\n",
        "    mdp: TabularMDP,\n",
        "    theta: float = 1e-10,\n",
        "    max_iterations: int = 1_000_000,\n",
        "):\n",
        "    \"\"\"Value iteration for V* (Bellman optimality updates).\"\"\"\n",
        "    nS, nA = mdp.nS, mdp.nA\n",
        "    V = np.zeros(nS, dtype=float)\n",
        "    for it in range(max_iterations):\n",
        "        delta = 0.0\n",
        "        for s in range(nS):\n",
        "            v_old = V[s]\n",
        "            q_sa = []\n",
        "            for a in range(nA):\n",
        "                q = 0.0\n",
        "                for p, s2, r, done in mdp.transitions(s, a):\n",
        "                    q += p * (r + (0.0 if done else mdp.gamma * V[s2]))\n",
        "                q_sa.append(q)\n",
        "            V[s] = float(np.max(q_sa))\n",
        "            delta = max(delta, abs(v_old - V[s]))\n",
        "        if delta < theta:\n",
        "            break\n",
        "    Q = q_from_v(mdp, V)\n",
        "    policy = greedy_policy_from_q(Q, tie_break='uniform')\n",
        "    return policy, V, it + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d911974",
      "metadata": {},
      "outputs": [],
      "source": [
        "ACTION_SYMBOLS = {0: '↑', 1: '→', 2: '↓', 3: '←'}\n",
        "\n",
        "def as_grid(arr: np.ndarray, rows: int, cols: int) -> np.ndarray:\n",
        "    return arr.reshape(rows, cols)\n",
        "\n",
        "\n",
        "def print_value_grid(V: np.ndarray, rows: int, cols: int, title: str = 'V') -> None:\n",
        "    grid = as_grid(V, rows, cols)\n",
        "    print(title)\n",
        "    for r in range(rows):\n",
        "        row = ' '.join([f\"{grid[r, c]:7.2f}\" for c in range(cols)])\n",
        "        print(row)\n",
        "\n",
        "\n",
        "def policy_to_arrow_grid(policy: np.ndarray, rows: int, cols: int, terminals: Iterable[int] = ()) -> List[List[str]]:\n",
        "    nS, nA = policy.shape\n",
        "    assert nA == 4, 'Arrow rendering expects 4 actions: up,right,down,left'\n",
        "    terminals = set(terminals)\n",
        "    arrows = [['' for _ in range(cols)] for _ in range(rows)]\n",
        "    for s in range(nS):\n",
        "        r, c = divmod(s, cols)\n",
        "        if s in terminals:\n",
        "            arrows[r][c] = 'T'\n",
        "            continue\n",
        "        best = np.flatnonzero(policy[s] == policy[s].max())\n",
        "        if len(best) == 1:\n",
        "            arrows[r][c] = ACTION_SYMBOLS[int(best[0])]\n",
        "        else:\n",
        "            arrows[r][c] = ''.join(ACTION_SYMBOLS[int(a)] for a in best)\n",
        "    return arrows\n",
        "\n",
        "\n",
        "def print_policy_grid(policy: np.ndarray, rows: int, cols: int, terminals: Iterable[int] = (), title: str = 'policy') -> None:\n",
        "    arrows = policy_to_arrow_grid(policy, rows, cols, terminals=terminals)\n",
        "    print(title)\n",
        "    for r in range(rows):\n",
        "        print(' '.join([f\"{arrows[r][c]:4s}\" for c in range(cols)]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21d224d5",
      "metadata": {},
      "source": [
        "## Example 1 — 4×4 Gridworld (Sutton & Barto-style)\n",
        "\n",
        "- States are grid cells; two terminal states in the corners.\n",
        "- Actions: up/right/down/left (deterministic).\n",
        "- Reward: `-1` per step until termination.\n",
        "- Discount: $\\gamma = 1$ (episodic)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c904134",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_gridworld_mdp(\n",
        "    rows: int = 4,\n",
        "    cols: int = 4,\n",
        "    terminal_states: Optional[Iterable[int]] = None,\n",
        "    step_reward: float = -1.0,\n",
        "    gamma: float = 1.0,\n",
        ") -> Tuple[TabularMDP, List[int]]:\n",
        "    \"\"\"Deterministic gridworld with absorbing terminal states.\"\"\"\n",
        "    nS = rows * cols\n",
        "    nA = 4  # up, right, down, left\n",
        "    if terminal_states is None:\n",
        "        terminal_states = [0, nS - 1]\n",
        "    terminal_states = list(terminal_states)\n",
        "    terminal_set = set(terminal_states)\n",
        "    P: List[List[List[Transition]]] = [[[] for _ in range(nA)] for _ in range(nS)]\n",
        "\n",
        "    def move(s: int, a: int) -> int:\n",
        "        r, c = divmod(s, cols)\n",
        "        if a == 0:\n",
        "            r2, c2 = max(r - 1, 0), c\n",
        "        elif a == 1:\n",
        "            r2, c2 = r, min(c + 1, cols - 1)\n",
        "        elif a == 2:\n",
        "            r2, c2 = min(r + 1, rows - 1), c\n",
        "        elif a == 3:\n",
        "            r2, c2 = r, max(c - 1, 0)\n",
        "        else:\n",
        "            raise ValueError('Invalid action')\n",
        "        return r2 * cols + c2\n",
        "\n",
        "    for s in range(nS):\n",
        "        for a in range(nA):\n",
        "            if s in terminal_set:\n",
        "                P[s][a] = [(1.0, s, 0.0, True)]\n",
        "                continue\n",
        "            s2 = move(s, a)\n",
        "            done = s2 in terminal_set\n",
        "            P[s][a] = [(1.0, s2, step_reward, done)]\n",
        "\n",
        "    return TabularMDP(nS=nS, nA=nA, P=P, gamma=gamma), terminal_states\n",
        "\n",
        "\n",
        "grid_mdp, grid_terminals = build_gridworld_mdp()\n",
        "grid_rows, grid_cols = 4, 4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1280d98",
      "metadata": {},
      "source": [
        "### 1) Iterative policy evaluation (uniform random policy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a0c885f",
      "metadata": {},
      "outputs": [],
      "source": [
        "random_policy = np.ones((grid_mdp.nS, grid_mdp.nA), dtype=float) / grid_mdp.nA\n",
        "V_pi, n_eval_iters = policy_evaluation(grid_mdp, random_policy, theta=1e-12)\n",
        "print(f\"Policy evaluation iterations: {n_eval_iters}\")\n",
        "print_value_grid(V_pi, grid_rows, grid_cols, title='V under uniform random policy')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fda629f9",
      "metadata": {},
      "source": [
        "### 2) Policy iteration (compute an optimal policy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe425bb0",
      "metadata": {},
      "outputs": [],
      "source": [
        "pi_star, V_star_pi, n_pi_iters, n_eval_total = policy_iteration(grid_mdp, theta=1e-12)\n",
        "print(f\"Policy iteration outer loops: {n_pi_iters}, total eval sweeps: {n_eval_total}\")\n",
        "print_value_grid(V_star_pi, grid_rows, grid_cols, title='V* (from policy iteration)')\n",
        "print_policy_grid(pi_star, grid_rows, grid_cols, terminals=grid_terminals, title='π* (policy iteration)')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70d0bec4",
      "metadata": {},
      "source": [
        "### 3) Value iteration (compute an optimal value function directly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73848b2a",
      "metadata": {},
      "outputs": [],
      "source": [
        "pi_vi, V_vi, n_vi_iters = value_iteration(grid_mdp, theta=1e-12)\n",
        "print(f\"Value iteration sweeps: {n_vi_iters}\")\n",
        "print_value_grid(V_vi, grid_rows, grid_cols, title='V* (from value iteration)')\n",
        "print_policy_grid(pi_vi, grid_rows, grid_cols, terminals=grid_terminals, title='π* (value iteration)')\n",
        "\n",
        "print('Max |V_pi_iter - V_value_iter| =', np.max(np.abs(V_star_pi - V_vi)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dab00d6",
      "metadata": {},
      "source": [
        "## Example 2 — The Gambler’s Problem (Value Iteration)\n",
        "\n",
        "A gambler has capital $s\\in\\{0,1,\\dots,100\\}$.\n",
        "- $s=0$ and $s=100$ are terminal.\n",
        "- At each step, choose a stake $a \\in \\{1,\\dots,\\min(s, 100-s)\\}$.\n",
        "- With probability $p_h$ you win and $s \\leftarrow s+a$, otherwise $s \\leftarrow s-a$.\n",
        "- Reward is 1 only upon reaching $s=100$ (so the value is the probability of eventual success)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b49625e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def gamblers_value_iteration(p_h: float = 0.4, theta: float = 1e-12, max_iterations: int = 1_000_000):\n",
        "    \"\"\"Value iteration for the Gambler's Problem (Sutton & Barto, Ch. 4).\n",
        "    Returns (V, policy) where policy[s] is the chosen stake for capital s.\n",
        "    \"\"\"\n",
        "    if not (0.0 < p_h < 1.0):\n",
        "        raise ValueError('p_h must be in (0,1)')\n",
        "\n",
        "    goal = 100\n",
        "    V = np.zeros(goal + 1, dtype=float)\n",
        "    V[goal] = 1.0  # reaching the goal yields reward 1; value is prob of success\n",
        "    policy = np.zeros(goal + 1, dtype=int)\n",
        "\n",
        "    for it in range(max_iterations):\n",
        "        delta = 0.0\n",
        "        for s in range(1, goal):\n",
        "            stakes = np.arange(1, min(s, goal - s) + 1)\n",
        "            if stakes.size == 0:\n",
        "                continue\n",
        "            # expected return: no living reward; terminal value already encodes success reward\n",
        "            action_returns = p_h * V[s + stakes] + (1.0 - p_h) * V[s - stakes]\n",
        "            v_new = np.max(action_returns)\n",
        "            delta = max(delta, abs(v_new - V[s]))\n",
        "            V[s] = v_new\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "    # derive greedy policy from converged V\n",
        "    for s in range(1, goal):\n",
        "        stakes = np.arange(1, min(s, goal - s) + 1)\n",
        "        if stakes.size == 0:\n",
        "            continue\n",
        "        action_returns = p_h * V[s + stakes] + (1.0 - p_h) * V[s - stakes]\n",
        "        best = stakes[np.flatnonzero(action_returns == action_returns.max())]\n",
        "        policy[s] = int(best[0])  # pick smallest stake among ties for determinism\n",
        "    return V, policy, it + 1\n",
        "\n",
        "\n",
        "V_g, pi_g, n_g_iters = gamblers_value_iteration(p_h=0.4, theta=1e-12)\n",
        "print(f\"Gambler's value iteration sweeps: {n_g_iters}\")\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
        "ax[0].plot(V_g)\n",
        "ax[0].set_title('Gambler: V(s)')\n",
        "ax[0].set_xlabel('capital s')\n",
        "ax[0].set_ylabel('value')\n",
        "\n",
        "ax[1].step(np.arange(len(pi_g)), pi_g, where='mid')\n",
        "ax[1].set_title('Gambler: greedy stake π(s)')\n",
        "ax[1].set_xlabel('capital s')\n",
        "ax[1].set_ylabel('stake')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91c08998",
      "metadata": {},
      "source": [
        "## A bit more depth: Bellman equations + convergence\n",
        "\n",
        "### Bellman expectation equation (policy evaluation)\n",
        "For a fixed policy $\\pi$, the value function is the unique solution to:\n",
        "$$\n",
        "V^{\\pi}(s)=\\sum_a \\pi(a\\mid s)\\sum_{s',r} p(s',r\\mid s,a)\\big[r + \\gamma V^{\\pi}(s')\\big].\n",
        "$$\n",
        "\n",
        "### Bellman optimality equation (control)\n",
        "The optimal value function satisfies:\n",
        "$$\n",
        "V^*(s)=\\max_a \\sum_{s',r} p(s',r\\mid s,a)\\big[r + \\gamma V^*(s')\\big].\n",
        "$$\n",
        "\n",
        "Both policy evaluation and value iteration are iterative methods that repeatedly apply these “backup” operators until the maximum change is below a tolerance `theta`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d89e2caa",
      "metadata": {},
      "outputs": [],
      "source": [
        "def policy_evaluation_with_deltas(\n",
        "    mdp: TabularMDP,\n",
        "    policy: np.ndarray,\n",
        "    theta: float = 1e-10,\n",
        "    max_iterations: int = 1_000_000,\n",
        "):\n",
        "    \"\"\"Same as policy_evaluation, but also returns per-iteration deltas for diagnostics.\"\"\"\n",
        "    nS, nA = mdp.nS, mdp.nA\n",
        "    is_stochastic_policy(policy, nS, nA)\n",
        "    V = np.zeros(nS, dtype=float)\n",
        "    deltas = []\n",
        "    for _ in range(max_iterations):\n",
        "        delta = 0.0\n",
        "        for s in range(nS):\n",
        "            v_old = V[s]\n",
        "            v_new = 0.0\n",
        "            for a in range(nA):\n",
        "                pi = policy[s, a]\n",
        "                if pi == 0.0:\n",
        "                    continue\n",
        "                for p, s2, r, done in mdp.transitions(s, a):\n",
        "                    v_new += pi * p * (r + (0.0 if done else mdp.gamma * V[s2]))\n",
        "            V[s] = v_new\n",
        "            delta = max(delta, abs(v_old - v_new))\n",
        "        deltas.append(delta)\n",
        "        if delta < theta:\n",
        "            break\n",
        "    return V, np.array(deltas)\n",
        "\n",
        "\n",
        "def value_iteration_with_deltas(\n",
        "    mdp: TabularMDP,\n",
        "    theta: float = 1e-10,\n",
        "    max_iterations: int = 1_000_000,\n",
        "):\n",
        "    \"\"\"Same as value_iteration, but returns per-iteration deltas for diagnostics.\"\"\"\n",
        "    nS, nA = mdp.nS, mdp.nA\n",
        "    V = np.zeros(nS, dtype=float)\n",
        "    deltas = []\n",
        "    for _ in range(max_iterations):\n",
        "        delta = 0.0\n",
        "        for s in range(nS):\n",
        "            v_old = V[s]\n",
        "            q_sa = []\n",
        "            for a in range(nA):\n",
        "                q = 0.0\n",
        "                for p, s2, r, done in mdp.transitions(s, a):\n",
        "                    q += p * (r + (0.0 if done else mdp.gamma * V[s2]))\n",
        "                q_sa.append(q)\n",
        "            V[s] = float(np.max(q_sa))\n",
        "            delta = max(delta, abs(v_old - V[s]))\n",
        "        deltas.append(delta)\n",
        "        if delta < theta:\n",
        "            break\n",
        "    Q = q_from_v(mdp, V)\n",
        "    policy = greedy_policy_from_q(Q, tie_break='uniform')\n",
        "    return policy, V, np.array(deltas)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "995799e7",
      "metadata": {},
      "source": [
        "### Convergence diagnostics on Gridworld\n",
        "\n",
        "Let’s look at how the max-update `delta` decays over sweeps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b999fee3",
      "metadata": {},
      "outputs": [],
      "source": [
        "V_tmp, deltas_eval = policy_evaluation_with_deltas(grid_mdp, random_policy, theta=1e-12)\n",
        "_, _, deltas_vi = value_iteration_with_deltas(grid_mdp, theta=1e-12)\n",
        "\n",
        "plt.figure(figsize=(8, 3))\n",
        "plt.semilogy(deltas_eval, label='policy evaluation (random π)')\n",
        "plt.semilogy(deltas_vi, label='value iteration')\n",
        "plt.xlabel('sweep')\n",
        "plt.ylabel('max state update (delta)')\n",
        "plt.title('Convergence diagnostics (Gridworld)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04a04952",
      "metadata": {},
      "source": [
        "### Better visualization of the Gridworld value function\n",
        "\n",
        "Printing numbers is fine, but a heatmap is easier to scan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a249083",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_value_heatmap(V: np.ndarray, rows: int, cols: int, title: str):\n",
        "    grid = as_grid(V, rows, cols)\n",
        "    plt.figure(figsize=(4.2, 3.6))\n",
        "    im = plt.imshow(grid, cmap='viridis')\n",
        "    plt.title(title)\n",
        "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
        "    for r in range(rows):\n",
        "        for c in range(cols):\n",
        "            plt.text(c, r, f\"{grid[r,c]:.1f}\", ha='center', va='center', color='white', fontsize=9)\n",
        "    plt.xticks(range(cols))\n",
        "    plt.yticks(range(rows))\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_value_heatmap(V_pi, grid_rows, grid_cols, title='Gridworld: V under random π')\n",
        "plot_value_heatmap(V_vi, grid_rows, grid_cols, title='Gridworld: V* (value iteration)')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "786d31a7",
      "metadata": {},
      "source": [
        "### Sensitivity experiment: change $\\gamma$ and step reward\n",
        "\n",
        "DP makes it easy to see how modeling choices (discounting and living reward) change the optimal values/policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34778607",
      "metadata": {},
      "outputs": [],
      "source": [
        "settings = [\n",
        "    {'gamma': 1.0, 'step_reward': -1.0, 'label': 'γ=1.0, r=-1'},\n",
        "    {'gamma': 0.9, 'step_reward': -1.0, 'label': 'γ=0.9, r=-1'},\n",
        "    {'gamma': 1.0, 'step_reward': -0.1, 'label': 'γ=1.0, r=-0.1'},\n",
        "]\n",
        "\n",
        "fig, axes = plt.subplots(1, len(settings), figsize=(12, 3.6))\n",
        "for ax, cfg in zip(axes, settings):\n",
        "    mdp_cfg, terminals_cfg = build_gridworld_mdp(gamma=cfg['gamma'], step_reward=cfg['step_reward'])\n",
        "    _, V_cfg, _ = value_iteration(mdp_cfg, theta=1e-12)\n",
        "    grid = as_grid(V_cfg, 4, 4)\n",
        "    im = ax.imshow(grid, cmap='viridis')\n",
        "    ax.set_title(cfg['label'])\n",
        "    ax.set_xticks(range(4))\n",
        "    ax.set_yticks(range(4))\n",
        "    for r in range(4):\n",
        "        for c in range(4):\n",
        "            ax.text(c, r, f\"{grid[r,c]:.1f}\", ha='center', va='center', color='white', fontsize=8)\n",
        "fig.colorbar(im, ax=axes, fraction=0.046, pad=0.04)\n",
        "plt.suptitle('Gridworld: how modeling choices change V*')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00270d9a",
      "metadata": {},
      "source": [
        "## Suggested exercises (keep it interactive)\n",
        "\n",
        "1. Gridworld: change terminal states (e.g., add more terminals) and see how $V^*$ changes.\n",
        "2. Gridworld: make actions stochastic (e.g., 0.8 intended move, 0.2 slip) and re-run value iteration.\n",
        "3. Gambler: try different $p_h$ values (e.g., 0.25, 0.55) and compare the optimal stake patterns.\n",
        "4. Compare tolerances: run value iteration with `theta=1e-2, 1e-6, 1e-12` and compare sweeps + resulting value error."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e502ce7a",
      "metadata": {},
      "source": [
        "## Example 3 — Stochastic Gridworld (\"slip\" dynamics)\n",
        "\n",
        "A common next step is to make actions **stochastic**: with probability $(1-\\epsilon)$ you move as intended, and with probability $\\epsilon$ you “slip” into one of the other actions. This keeps the problem tabular, but makes it feel more like real environments (uncertainty in control)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80b3ac50",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_gridworld_mdp_slip(\n",
        "    rows: int = 4,\n",
        "    cols: int = 4,\n",
        "    terminal_states: Optional[Iterable[int]] = None,\n",
        "    step_reward: float = -1.0,\n",
        "    gamma: float = 1.0,\n",
        "    slip_epsilon: float = 0.2,\n",
        ") -> Tuple[TabularMDP, List[int]]:\n",
        "    \"\"\"Gridworld where actions slip to other actions with probability slip_epsilon.\"\"\"\n",
        "    if not (0.0 <= slip_epsilon <= 1.0):\n",
        "        raise ValueError('slip_epsilon must be in [0,1]')\n",
        "    nS = rows * cols\n",
        "    nA = 4\n",
        "    if terminal_states is None:\n",
        "        terminal_states = [0, nS - 1]\n",
        "    terminal_states = list(terminal_states)\n",
        "    terminal_set = set(terminal_states)\n",
        "    P: List[List[List[Transition]]] = [[[] for _ in range(nA)] for _ in range(nS)]\n",
        "\n",
        "    def move(s: int, a: int) -> int:\n",
        "        r, c = divmod(s, cols)\n",
        "        if a == 0:\n",
        "            r2, c2 = max(r - 1, 0), c\n",
        "        elif a == 1:\n",
        "            r2, c2 = r, min(c + 1, cols - 1)\n",
        "        elif a == 2:\n",
        "            r2, c2 = min(r + 1, rows - 1), c\n",
        "        elif a == 3:\n",
        "            r2, c2 = r, max(c - 1, 0)\n",
        "        else:\n",
        "            raise ValueError('Invalid action')\n",
        "        return r2 * cols + c2\n",
        "\n",
        "    for s in range(nS):\n",
        "        for a in range(nA):\n",
        "            if s in terminal_set:\n",
        "                P[s][a] = [(1.0, s, 0.0, True)]\n",
        "                continue\n",
        "            transitions: Dict[Tuple[int, float, bool], float] = {}\n",
        "            # intended action\n",
        "            intended = move(s, a)\n",
        "            done_intended = intended in terminal_set\n",
        "            key = (intended, step_reward, done_intended)\n",
        "            transitions[key] = transitions.get(key, 0.0) + (1.0 - slip_epsilon)\n",
        "            # slip to other actions uniformly\n",
        "            if slip_epsilon > 0.0:\n",
        "                others = [aa for aa in range(nA) if aa != a]\n",
        "                p_other = slip_epsilon / len(others)\n",
        "                for aa in others:\n",
        "                    s2 = move(s, aa)\n",
        "                    done = s2 in terminal_set\n",
        "                    key = (s2, step_reward, done)\n",
        "                    transitions[key] = transitions.get(key, 0.0) + p_other\n",
        "            P[s][a] = [(p, s2, r, done) for (s2, r, done), p in transitions.items()]\n",
        "\n",
        "    return TabularMDP(nS=nS, nA=nA, P=P, gamma=gamma), terminal_states\n",
        "\n",
        "\n",
        "slip_mdp, slip_terminals = build_gridworld_mdp_slip(slip_epsilon=0.2)\n",
        "pi_slip, V_slip, n_slip = value_iteration(slip_mdp, theta=1e-12)\n",
        "print(f\"Slippery gridworld value iteration sweeps: {n_slip}\")\n",
        "plot_value_heatmap(V_slip, 4, 4, title='Slippery Gridworld (ε=0.2): V*')\n",
        "print_policy_grid(pi_slip, 4, 4, terminals=slip_terminals, title='Slippery Gridworld: π*')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30ec86d6",
      "metadata": {},
      "source": [
        "## Modified Policy Iteration (optional, but great practice)\n",
        "\n",
        "Sutton & Barto discuss *modified policy iteration*: instead of evaluating a policy to full convergence each time, do only a small number of evaluation sweeps, then improve the policy again. This often gives a good speed/compute tradeoff."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad717018",
      "metadata": {},
      "outputs": [],
      "source": [
        "def modified_policy_iteration(\n",
        "    mdp: TabularMDP,\n",
        "    eval_sweeps: int = 3,\n",
        "    theta: float = 1e-10,\n",
        "    max_iterations: int = 10_000,\n",
        "    tie_break: str = 'uniform',\n",
        "):\n",
        "    nS, nA = mdp.nS, mdp.nA\n",
        "    policy = np.ones((nS, nA), dtype=float) / nA\n",
        "    V = np.zeros(nS, dtype=float)\n",
        "\n",
        "    for it in range(max_iterations):\n",
        "        # truncated evaluation: do a fixed number of sweeps\n",
        "        for _ in range(eval_sweeps):\n",
        "            V, _ = policy_evaluation(mdp, policy, theta=0.0, max_iterations=1, V0=V, in_place=True)\n",
        "        # improvement\n",
        "        Q = q_from_v(mdp, V)\n",
        "        new_policy = greedy_policy_from_q(Q, tie_break=tie_break)\n",
        "        if np.allclose(new_policy, policy):\n",
        "            # optional final cleanup evaluation\n",
        "            V, _ = policy_evaluation(mdp, policy, theta=theta, V0=V, in_place=True)\n",
        "            return policy, V, it + 1\n",
        "        policy = new_policy\n",
        "    return policy, V, max_iterations\n",
        "\n",
        "\n",
        "mpi_policy, mpi_V, mpi_iters = modified_policy_iteration(grid_mdp, eval_sweeps=3, theta=1e-12)\n",
        "print(f\"Modified policy iteration outer loops: {mpi_iters}\")\n",
        "print('Max |V_MPI - V_PI| =', np.max(np.abs(mpi_V - V_star_pi)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86a8e29c",
      "metadata": {},
      "source": [
        "## Example 4 — Jack’s Car Rental (\"car moving\" problem)\n",
        "\n",
        "This is the classic DP control problem from Sutton & Barto Ch. 4 (often misremembered as a “car salesman” task):\n",
        "- Two locations with car rental demand and returns.\n",
        "- Each night you can move cars between locations at a cost.\n",
        "- During the day, cars are rented out stochastically, producing reward.\n",
        "\n",
        "The original book uses `max_cars=20` and Poisson demand/returns; that can be slow in pure Python. Below is a **scaled-down but faithful** implementation that runs fast, and you can increase sizes later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abbcdc14",
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "from functools import lru_cache\n",
        "\n",
        "def poisson_pmf_truncated(lam: float, n_max: int) -> np.ndarray:\n",
        "    \"\"\"Return pmf[0..n_max] with tail probability folded into pmf[n_max].\"\"\"\n",
        "    pmf = np.zeros(n_max + 1, dtype=float)\n",
        "    pmf[0] = math.exp(-lam)\n",
        "    for k in range(1, n_max + 1):\n",
        "        pmf[k] = pmf[k - 1] * lam / k\n",
        "    # fold tail into last bucket\n",
        "    tail = max(0.0, 1.0 - pmf.sum())\n",
        "    pmf[-1] += tail\n",
        "    return pmf\n",
        "\n",
        "\n",
        "def precompute_location_tables(\n",
        "    max_cars: int,\n",
        "    lam_req: float,\n",
        "    lam_ret: float,\n",
        "    poisson_max: int,\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Precompute per-location transition probabilities and (rentals * prob) numerators.\n",
        "\n",
        "    Returns:\n",
        "        p_next[c, c2] = P(next_cars=c2 | start=c)\n",
        "        rent_num[c, c2] = E[rentals * 1{next_cars=c2} | start=c]\n",
        "    \"\"\"\n",
        "    req_pmf = poisson_pmf_truncated(lam_req, poisson_max)\n",
        "    ret_pmf = poisson_pmf_truncated(lam_ret, poisson_max)\n",
        "    p_next = np.zeros((max_cars + 1, max_cars + 1), dtype=float)\n",
        "    rent_num = np.zeros((max_cars + 1, max_cars + 1), dtype=float)\n",
        "\n",
        "    for c in range(max_cars + 1):\n",
        "        for req, p_req in enumerate(req_pmf):\n",
        "            rentals = min(c, req)\n",
        "            c_after_rent = c - rentals\n",
        "            for ret, p_ret in enumerate(ret_pmf):\n",
        "                c2 = min(max_cars, c_after_rent + ret)\n",
        "                p = p_req * p_ret\n",
        "                p_next[c, c2] += p\n",
        "                rent_num[c, c2] += rentals * p\n",
        "        # normalize minor numeric drift\n",
        "        s = p_next[c].sum()\n",
        "        if not np.isclose(s, 1.0):\n",
        "            p_next[c] /= s\n",
        "            rent_num[c] /= s\n",
        "    return p_next, rent_num\n",
        "\n",
        "\n",
        "def jacks_policy_evaluation(\n",
        "    V: np.ndarray,\n",
        "    policy: np.ndarray,\n",
        "    p1: np.ndarray,\n",
        "    n1: np.ndarray,\n",
        "    p2: np.ndarray,\n",
        "    n2: np.ndarray,\n",
        "    rental_reward: float,\n",
        "    move_cost: float,\n",
        "    gamma: float,\n",
        "    max_move: int,\n",
        "    theta: float = 1e-4,\n",
        "    max_iterations: int = 10_000,\n",
        "    in_place: bool = True,\n",
        ") -> Tuple[np.ndarray, int]:\n",
        "    \"\"\"Policy evaluation for Jack's Car Rental using precomputed per-location tables.\"\"\"\n",
        "    max_cars = V.shape[0] - 1\n",
        "    V = V.copy().astype(float)\n",
        "    actions = np.arange(-max_move, max_move + 1)\n",
        "    nS = (max_cars + 1) ** 2\n",
        "    for it in range(max_iterations):\n",
        "        delta = 0.0\n",
        "        V_old = V.copy() if not in_place else None\n",
        "        for i in range(max_cars + 1):\n",
        "            for j in range(max_cars + 1):\n",
        "                v_prev = V[i, j]\n",
        "                a = int(policy[i, j])\n",
        "                a = int(np.clip(a, -min(max_move, j), min(max_move, i)))\n",
        "                i1, j1 = i - a, j + a\n",
        "                # expected return under action a\n",
        "                total = 0.0\n",
        "                for i2 in range(max_cars + 1):\n",
        "                    p_i = p1[i1, i2]\n",
        "                    if p_i == 0.0:\n",
        "                        continue\n",
        "                    for j2 in range(max_cars + 1):\n",
        "                        p_j = p2[j1, j2]\n",
        "                        if p_j == 0.0:\n",
        "                            continue\n",
        "                        prob = p_i * p_j\n",
        "                        reward_weighted = rental_reward * (n1[i1, i2] * p_j + n2[j1, j2] * p_i) - move_cost * abs(a) * prob\n",
        "                        v_next = (0.0 if V_old is None else V_old[i2, j2])\n",
        "                        if V_old is None:\n",
        "                            v_next = V[i2, j2]\n",
        "                        total += reward_weighted + gamma * prob * v_next\n",
        "                V[i, j] = total\n",
        "                delta = max(delta, abs(v_prev - total))\n",
        "        if delta < theta:\n",
        "            return V, it + 1\n",
        "    return V, max_iterations\n",
        "\n",
        "\n",
        "def jacks_policy_improvement(\n",
        "    V: np.ndarray,\n",
        "    p1: np.ndarray,\n",
        "    n1: np.ndarray,\n",
        "    p2: np.ndarray,\n",
        "    n2: np.ndarray,\n",
        "    rental_reward: float,\n",
        "    move_cost: float,\n",
        "    gamma: float,\n",
        "    max_move: int,\n",
        ") -> Tuple[np.ndarray, bool]:\n",
        "    \"\"\"Greedy improvement step. Returns (new_policy, stable).\"\"\"\n",
        "    max_cars = V.shape[0] - 1\n",
        "    new_policy = np.zeros((max_cars + 1, max_cars + 1), dtype=int)\n",
        "    stable = True\n",
        "    for i in range(max_cars + 1):\n",
        "        for j in range(max_cars + 1):\n",
        "            allowed = np.arange(-min(max_move, j), min(max_move, i) + 1)\n",
        "            best_a = 0\n",
        "            best_val = -1e100\n",
        "            for a in allowed:\n",
        "                i1, j1 = i - a, j + a\n",
        "                total = 0.0\n",
        "                for i2 in range(max_cars + 1):\n",
        "                    p_i = p1[i1, i2]\n",
        "                    if p_i == 0.0:\n",
        "                        continue\n",
        "                    for j2 in range(max_cars + 1):\n",
        "                        p_j = p2[j1, j2]\n",
        "                        if p_j == 0.0:\n",
        "                            continue\n",
        "                        prob = p_i * p_j\n",
        "                        reward_weighted = rental_reward * (n1[i1, i2] * p_j + n2[j1, j2] * p_i) - move_cost * abs(a) * prob\n",
        "                        total += reward_weighted + gamma * prob * V[i2, j2]\n",
        "                if total > best_val + 1e-12:\n",
        "                    best_val = total\n",
        "                    best_a = int(a)\n",
        "            new_policy[i, j] = best_a\n",
        "    return new_policy, stable  # stability checked in policy iteration loop\n",
        "\n",
        "\n",
        "def jacks_policy_iteration(\n",
        "    max_cars: int = 10,\n",
        "    max_move: int = 5,\n",
        "    poisson_max: int = 8,\n",
        "    lam_req1: float = 3.0,\n",
        "    lam_req2: float = 4.0,\n",
        "    lam_ret1: float = 3.0,\n",
        "    lam_ret2: float = 2.0,\n",
        "    rental_reward: float = 10.0,\n",
        "    move_cost: float = 2.0,\n",
        "    gamma: float = 0.9,\n",
        "    theta: float = 1e-3,\n",
        "    max_outer: int = 50,\n",
        "    eval_in_place: bool = True,\n",
        ") -> Tuple[np.ndarray, np.ndarray, List[float]]:\n",
        "    \"\"\"Run policy iteration for Jack's Car Rental (scaled-down).\"\"\"\n",
        "    p1, n1 = precompute_location_tables(max_cars, lam_req1, lam_ret1, poisson_max)\n",
        "    p2, n2 = precompute_location_tables(max_cars, lam_req2, lam_ret2, poisson_max)\n",
        "\n",
        "    V = np.zeros((max_cars + 1, max_cars + 1), dtype=float)\n",
        "    policy = np.zeros((max_cars + 1, max_cars + 1), dtype=int)\n",
        "    history = []\n",
        "    for outer in range(max_outer):\n",
        "        V, eval_iters = jacks_policy_evaluation(V, policy, p1, n1, p2, n2, rental_reward, move_cost, gamma, max_move, theta=theta, in_place=eval_in_place)\n",
        "        new_policy, _ = jacks_policy_improvement(V, p1, n1, p2, n2, rental_reward, move_cost, gamma, max_move)\n",
        "        change = np.max(np.abs(new_policy - policy))\n",
        "        history.append(float(change))\n",
        "        if np.array_equal(new_policy, policy):\n",
        "            break\n",
        "        policy = new_policy\n",
        "    return policy, V, history\n",
        "\n",
        "\n",
        "def plot_jacks_policy_and_value(policy: np.ndarray, V: np.ndarray, title_prefix: str = \"Jack's\") -> None:\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    im1 = plt.imshow(V.T, origin='lower', cmap='viridis')\n",
        "    plt.colorbar(im1, fraction=0.046, pad=0.04)\n",
        "    plt.title(f\"{title_prefix}: value V(i,j)\")\n",
        "    plt.xlabel('cars at loc 1 (i)')\n",
        "    plt.ylabel('cars at loc 2 (j)')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    im2 = plt.imshow(policy.T, origin='lower', cmap='coolwarm')\n",
        "    plt.colorbar(im2, fraction=0.046, pad=0.04)\n",
        "    plt.title(f\"{title_prefix}: policy (move from 1→2)\")\n",
        "    plt.xlabel('cars at loc 1 (i)')\n",
        "    plt.ylabel('cars at loc 2 (j)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Demo run (scaled-down for speed). You can increase max_cars / poisson_max later.\n",
        "jacks_policy, jacks_V, jacks_hist = jacks_policy_iteration(max_cars=10, max_move=5, poisson_max=8, theta=1e-3)\n",
        "print('Jack\\'s policy iteration outer loops:', len(jacks_hist))\n",
        "plot_jacks_policy_and_value(jacks_policy, jacks_V, title_prefix=\"Jack's (scaled)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0284fbb2",
      "metadata": {},
      "source": [
        "## More tasks (ideas to extend this notebook)\n",
        "\n",
        "If you want “the more material the better”, here are additional DP/MDP practice tasks that still stay in the Sutton & Barto Ch. 3–4 zone:\n",
        "\n",
        "1. **Policy evaluation speed**: compare `in_place=True` vs `False` for Gridworld and plot deltas.\n",
        "2. **Asynchronous updates**: sweep states in random order and compare convergence.\n",
        "3. **State aggregation**: group Gridworld states (e.g., by Manhattan distance to terminal) and evaluate an aggregated approximate value function.\n",
        "4. **Jack’s scaling study**: run Jack’s with `max_cars=8,10,12` and compare runtime and policy structure.\n",
        "5. **Parameter sweeps**: for Gambler and Jack’s, sweep $p_h$ (or Poisson rates) and track how the greedy policy changes."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
