{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e768595b",
   "metadata": {},
   "source": [
    "![Logo](https://raw.githubusercontent.com/BartaZoltan/deep-reinforcement-learning-course/main/notebooks/shared_assets/logo.png)\n",
    "\n",
    "\n",
    "**Developers:** Domonkos Nagy, Balazs Nagy, Zoltan Barta  \n",
    "**Date:** 2026-02-23  \n",
    "**Version:** 2025-26/2\n",
    "\n",
    "[<img src=\"https://colab.research.google.com/assets/colab-badge.svg\">](https://colab.research.google.com/github/BartaZoltan/deep-reinforcement-learning-course/blob/main/notebooks/sessions/session_02_mdp_dynamic_programming/session_02_mdp_dynamic_programming.ipynb)\n",
    "\n",
    "# Practice 2: MDP Dynamic Programming\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook introduces **tabular Markov Decision Processes (MDPs)** and **Dynamic Programming** control methods in a practical, coding-first format.\n",
    "\n",
    "Content outline:\n",
    "- agent-environment interface and reusable GridWorld MDP design,\n",
    "- Value Iteration implementation and convergence analysis,\n",
    "- Policy Iteration implementation and behavior analysis,\n",
    "- larger-map stress tests for scalability and robustness,\n",
    "- transfer to Gymnasium FrozenLake,\n",
    "- optional extension: Gambler's Problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0568b4",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This practical session develops tabular Dynamic Programming from first principles and then tests it across increasingly realistic environments. Following Sutton and Barto (Ch. 3-4), we start with a fully transparent custom GridWorld to formalize the agent-environment interface, state and action spaces, transition-reward dynamics, and the Markov property. On top of this model, we implement and analyze the two classical planning algorithms for finite MDPs: Value Iteration and Policy Iteration. The focus is not only on obtaining a final policy, but on understanding algorithmic behavior through convergence curves, value-function evolution, policy snapshots, sensitivity studies (e.g., $\\gamma$, stopping thresholds, stochasticity), and rollout-based validation. After baseline experiments on small maps, we stress-test both methods on larger layouts to study scalability, robustness, and computational trade-offs. We then transfer the same workflow to Gymnasium FrozenLake to connect custom tabular implementations with standard RL tooling and interface conventions (`reset`, `step`, `terminated`, `truncated`, and transition model access in toy-text environments). As an optional extension, we include Gambler’s Problem to broaden intuition beyond GridWorld and show how the same DP ideas apply to a different tabular structure. By the end of the notebook, you should be able to build a clean tabular MDP, implement both DP control methods correctly, interpret their dynamics with meaningful diagnostics, and move confidently between custom and Gymnasium-based environments.\n",
    "\n",
    "This notebook follows Chapter 3-4 of Sutton & Barto {cite}`sutton2018`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350bcda8",
   "metadata": {},
   "source": [
    "## Markov Decision Process, Agent-Environment interface, and GridWorld\n",
    "\n",
    "\n",
    "In reinforcement learning, the interaction loop is:\n",
    "- agent observes state $S_t$\n",
    "- agent takes action $A_t$\n",
    "- environment returns reward $R_{t+1}$ and next state $S_{t+1}$\n",
    "\n",
    "The **MDP assumption** is that the future depends on the current state-action pair, not the full history:\n",
    "$$\n",
    "P(S_{t+1}, R_{t+1} \\mid S_t, A_t)\n",
    "$$\n",
    "\n",
    "For planning with Dynamic Programming later, we need an explicit model:\n",
    "- state space $\\mathcal{S}$\n",
    "- action space $\\mathcal{A}$\n",
    "- transition-reward dynamics $p(s', r \\mid s, a)$\n",
    "- discount factor $\\gamma$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe72594",
   "metadata": {},
   "source": [
    "### Chess as an MDP: a Markov assumption example\n",
    "\n",
    "#### MDP framing\n",
    "- **State (S):** what the agent observes/keeps as the “current situation”\n",
    "- **Action (A):** a legal chess move\n",
    "- **Transition (P):** how the position changes after the move (plus rules)\n",
    "- **Reward (R):** e.g., checkmate = +1, loss = −1, otherwise 0 (example)\n",
    "\n",
    "#### Markov assumption (informal)\n",
    "The future depends only on the **current state** and the **chosen action**, not on the whole past:\n",
    " $$ P(s_{t+1}, r_{t+1}\\mid s_t, a_t, \\text{history}) = P(s_{t+1}, r_{t+1}\\mid s_t, a_t) $$\n",
    "\n",
    "#### A “good” Markov state for chess\n",
    "A state representation that *is* Markov must include all information needed to determine:\n",
    "1) which moves are legal, and  \n",
    "2) how rules apply next step.\n",
    "\n",
    "Example Markov state:\n",
    "- **board position** (piece placement)\n",
    "- **side to move**\n",
    "- **castling rights** (KQkq flags)\n",
    "- **en passant target square** (or “none”)\n",
    "- *(optionally)* **halfmove clock** and **fullmove number** (for draw rules)\n",
    "\n",
    "With this, legality and next-position dynamics can be computed **without knowing earlier moves**.\n",
    "\n",
    "#### Why a “board-only” state is NOT Markov\n",
    "If you define the state as only:\n",
    "- **S = piece placement on the board**\n",
    "\n",
    "then the process becomes non-Markov because the same board layout can correspond to different legal moves depending on history.\n",
    "\n",
    "#### Counterexample 1: Castling\n",
    "Two histories can lead to the same piece placement, but:\n",
    "- in one, the king/rook moved earlier → **castling is illegal**\n",
    "- in the other, they never moved → **castling is legal**\n",
    "\n",
    "So legality (and thus transitions) depends on the past unless castling rights are in the state.\n",
    "\n",
    "#### Counterexample 2: En passant\n",
    "The same board layout may occur, but:\n",
    "- if the opponent just advanced a pawn two squares last move → **en passant is legal now**\n",
    "- otherwise → **en passant is not legal**\n",
    "\n",
    "So you must store the en passant target square (or equivalent) to satisfy the Markov property.\n",
    "\n",
    "#### Takeaway\n",
    "- Chess is Markov **given a sufficiently rich state** (board + turn + castling rights + en passant + …).\n",
    "- If you omit “hidden memory” variables and keep only the board, the environment behaves like a **POMDP** (history matters)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee6f5a5",
   "metadata": {},
   "source": [
    "So in this part we build a GridWorld that supports both:\n",
    "- a simulator-like API (`reset`, `step`) for rollouts,\n",
    "- and a tabular model (`P[s][a]`) for Value/Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115110b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from pathlib import Path\n",
    "import time\n",
    "import base64\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "\n",
    "from IPython.display import Image as DisplayImage, display, HTML\n",
    "\n",
    "try:\n",
    "    from PIL import Image as PILImage\n",
    "except Exception:\n",
    "    PILImage = None\n",
    "\n",
    "try:\n",
    "    import imageio.v2 as imageio\n",
    "except Exception:\n",
    "    imageio = None\n",
    "\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "except Exception:\n",
    "    gym = None\n",
    "\n",
    "SEED = 42\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "# Action encoding for consistency across all later parts\n",
    "UP, RIGHT, DOWN, LEFT = 0, 1, 2, 3\n",
    "ACTIONS = {\n",
    "    UP: (-1, 0),\n",
    "    RIGHT: (0, 1),\n",
    "    DOWN: (1, 0),\n",
    "    LEFT: (0, -1),\n",
    "}\n",
    "ACTION_SYMBOLS = {UP: \"↑\", RIGHT: \"→\", DOWN: \"↓\", LEFT: \"←\"}\n",
    "Transition = Tuple[float, int, float, bool]  # (prob, next_state, reward, done)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b54202",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GridWorldConfig:\n",
    "    char_map: List[str]\n",
    "    step_reward: float = -1.0\n",
    "    goal_reward: float = 10.0\n",
    "    hole_reward: float = -10.0\n",
    "    slip_prob: float = 0.0  # 0.0 = deterministic dynamics\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TabularMDP:\n",
    "    nS: int\n",
    "    nA: int\n",
    "    P: Dict[int, Dict[int, List[Transition]]]\n",
    "    state_to_pos: Dict[int, Tuple[int, int]]\n",
    "    pos_to_state: Dict[Tuple[int, int], int]\n",
    "    terminal_states: set\n",
    "    start_state: int\n",
    "    grid_chars: np.ndarray\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab386673",
   "metadata": {},
   "source": [
    "### GridWorld implementation details\n",
    "\n",
    "This class exposes:\n",
    "- `reset()` and `step(action)` for interaction,\n",
    "- `as_mdp()` that returns a full tabular model for planning algorithms.\n",
    "\n",
    "This keeps one source of truth for dynamics and avoids mismatch bugs later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f68cdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorldEnv:\n",
    "    def __init__(self, cfg: GridWorldConfig):\n",
    "        self.cfg = cfg\n",
    "        self.grid = np.array([list(row) for row in cfg.char_map], dtype=\"<U1\")\n",
    "        self.H, self.W = self.grid.shape\n",
    "\n",
    "        self.pos_to_state: Dict[Tuple[int, int], int] = {}\n",
    "        self.state_to_pos: Dict[int, Tuple[int, int]] = {}\n",
    "\n",
    "        s_idx = 0\n",
    "        for r in range(self.H):\n",
    "            for c in range(self.W):\n",
    "                if self.grid[r, c] != \"#\":\n",
    "                    self.pos_to_state[(r, c)] = s_idx\n",
    "                    self.state_to_pos[s_idx] = (r, c)\n",
    "                    s_idx += 1\n",
    "\n",
    "        self.nS = s_idx\n",
    "        self.nA = 4\n",
    "\n",
    "        self.terminal_states = set()\n",
    "        self.start_state: Optional[int] = None\n",
    "\n",
    "        for (r, c), s in self.pos_to_state.items():\n",
    "            cell = self.grid[r, c]\n",
    "            if cell in (\"G\", \"H\"):\n",
    "                self.terminal_states.add(s)\n",
    "            if cell == \"S\":\n",
    "                if self.start_state is not None:\n",
    "                    raise ValueError(\"Map must contain exactly one start cell 'S'.\")\n",
    "                self.start_state = s\n",
    "\n",
    "        if self.start_state is None:\n",
    "            raise ValueError(\"Map must contain one start cell 'S'.\")\n",
    "\n",
    "        self._P = self._build_transition_model()\n",
    "        self._state = self.start_state\n",
    "\n",
    "    def _move(self, r: int, c: int, a: int) -> Tuple[int, int]:\n",
    "        dr, dc = ACTIONS[a]\n",
    "        nr, nc = r + dr, c + dc\n",
    "        if nr < 0 or nr >= self.H or nc < 0 or nc >= self.W or self.grid[nr, nc] == \"#\":\n",
    "            return r, c\n",
    "        return nr, nc\n",
    "\n",
    "    def _cell_reward_done(self, r: int, c: int) -> Tuple[float, bool]:\n",
    "        cell = self.grid[r, c]\n",
    "        if cell == \"G\":\n",
    "            return self.cfg.goal_reward, True\n",
    "        if cell == \"H\":\n",
    "            return self.cfg.hole_reward, True\n",
    "        return self.cfg.step_reward, False\n",
    "\n",
    "    def _build_transition_model(self) -> Dict[int, Dict[int, List[Transition]]]:\n",
    "        left_of = {UP: LEFT, RIGHT: UP, DOWN: RIGHT, LEFT: DOWN}\n",
    "        right_of = {UP: RIGHT, RIGHT: DOWN, DOWN: LEFT, LEFT: UP}\n",
    "\n",
    "        P: Dict[int, Dict[int, List[Transition]]] = {\n",
    "            s: {a: [] for a in range(self.nA)} for s in range(self.nS)\n",
    "        }\n",
    "\n",
    "        for s, (r, c) in self.state_to_pos.items():\n",
    "            if s in self.terminal_states:\n",
    "                for a in range(self.nA):\n",
    "                    P[s][a] = [(1.0, s, 0.0, True)]\n",
    "                continue\n",
    "\n",
    "            for a in range(self.nA):\n",
    "                outcomes = [(1.0 - self.cfg.slip_prob, a)]\n",
    "                if self.cfg.slip_prob > 0:\n",
    "                    outcomes.append((self.cfg.slip_prob / 2.0, left_of[a]))\n",
    "                    outcomes.append((self.cfg.slip_prob / 2.0, right_of[a]))\n",
    "\n",
    "                acc: Dict[Tuple[int, float, bool], float] = {}\n",
    "                for p, a_eff in outcomes:\n",
    "                    nr, nc = self._move(r, c, a_eff)\n",
    "                    s_next = self.pos_to_state[(nr, nc)]\n",
    "                    reward, done = self._cell_reward_done(nr, nc)\n",
    "                    key = (s_next, reward, done)\n",
    "                    acc[key] = acc.get(key, 0.0) + p\n",
    "\n",
    "                P[s][a] = [(p, s_next, reward, done) for (s_next, reward, done), p in acc.items()]\n",
    "\n",
    "        return P\n",
    "\n",
    "    def reset(self) -> int:\n",
    "        ########################################################################\n",
    "        self._state = self.start_state\n",
    "        ########################################################################\n",
    "        return self._state\n",
    "\n",
    "    def step(self, action: int) -> Tuple[int, float, bool, dict]:\n",
    "        ########################################################################\n",
    "        if action not in ACTIONS:\n",
    "            raise ValueError(f\"Invalid action {action}. Must be in {list(ACTIONS.keys())}.\")\n",
    "\n",
    "        transitions = self._P[self._state][action]\n",
    "        probs = np.array([p for p, _, _, _ in transitions], dtype=float)\n",
    "        idx = int(rng.choice(len(transitions), p=probs))\n",
    "        _, s_next, reward, done = transitions[idx]\n",
    "\n",
    "        self._state = s_next\n",
    "        ########################################################################\n",
    "        return s_next, reward, done, {}\n",
    "\n",
    "    def as_mdp(self) -> TabularMDP:\n",
    "        return TabularMDP(\n",
    "            nS=self.nS,\n",
    "            nA=self.nA,\n",
    "            P=self._P,\n",
    "            state_to_pos=self.state_to_pos,\n",
    "            pos_to_state=self.pos_to_state,\n",
    "            terminal_states=self.terminal_states,\n",
    "            start_state=self.start_state,\n",
    "            grid_chars=self.grid,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cc6c86",
   "metadata": {},
   "source": [
    "### GridWorld diagnostics and sanity checks\n",
    "\n",
    "Before running any algorithms, we should verify the environment logic visually:\n",
    "- map rendering,\n",
    "- greedy/random rollout trace,\n",
    "- transition probability sanity check (`sum_p = 1`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2f3e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _draw_cell_grid(ax, H: int, W: int) -> None:\n",
    "    # Draw grid lines on cell borders (not through cell centers)\n",
    "    ax.set_xticks(np.arange(-0.5, W, 1), minor=True)\n",
    "    ax.set_yticks(np.arange(-0.5, H, 1), minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"gray\", linewidth=0.8)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "    ax.set_xlim(-0.5, W - 0.5)\n",
    "    ax.set_ylim(H - 0.5, -0.5)\n",
    "\n",
    "\n",
    "def _map_code_array(grid_chars: np.ndarray) -> np.ndarray:\n",
    "    # Supports custom GridWorld and FrozenLake chars\n",
    "    code_map = {\"#\": 0, \".\": 1, \"F\": 1, \"S\": 2, \"G\": 3, \"H\": 4}\n",
    "    arr = np.full(grid_chars.shape, 1, dtype=int)\n",
    "    for r in range(grid_chars.shape[0]):\n",
    "        for c in range(grid_chars.shape[1]):\n",
    "            arr[r, c] = code_map.get(grid_chars[r, c], 1)\n",
    "    return arr\n",
    "\n",
    "\n",
    "def values_to_grid(mdp: TabularMDP, V: np.ndarray) -> np.ndarray:\n",
    "    H, W = mdp.grid_chars.shape\n",
    "    grid = np.full((H, W), np.nan, dtype=float)\n",
    "    for s, (r, c) in mdp.state_to_pos.items():\n",
    "        grid[r, c] = V[s]\n",
    "    return grid\n",
    "\n",
    "\n",
    "def _text_color_for_value(v: float, vmin: float, vmax: float, threshold: float = 0.55) -> str:\n",
    "    if not np.isfinite(v):\n",
    "        return \"black\"\n",
    "    if vmax <= vmin:\n",
    "        return \"black\"\n",
    "    norm = (v - vmin) / (vmax - vmin)\n",
    "    return \"white\" if norm < threshold else \"black\"\n",
    "\n",
    "\n",
    "def render_map(env: GridWorldEnv, title: str = \"GridWorld map\") -> None:\n",
    "    arr = _map_code_array(env.grid)\n",
    "    cmap = ListedColormap([\"black\", \"white\", \"#A7D3F5\", \"#B7E4C7\", \"#F8B4B4\"])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    ax.imshow(arr, cmap=cmap, vmin=0, vmax=4)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(range(env.W))\n",
    "    ax.set_yticks(range(env.H))\n",
    "    _draw_cell_grid(ax, env.H, env.W)\n",
    "\n",
    "    for r in range(env.H):\n",
    "        for c in range(env.W):\n",
    "            txt = \" \" if env.grid[r, c] in (\".\", \"F\") else env.grid[r, c]\n",
    "            ax.text(c, r, txt, ha=\"center\", va=\"center\", color=\"black\", fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def render_policy_arrows(mdp: TabularMDP, greedy_actions: np.ndarray, title: str = \"Policy arrows\") -> None:\n",
    "    H, W = mdp.grid_chars.shape\n",
    "    bg = np.zeros((H, W), dtype=float)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    ax.imshow(bg, cmap=ListedColormap([\"#f7f7f7\"]))\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(range(W))\n",
    "    ax.set_yticks(range(H))\n",
    "    _draw_cell_grid(ax, H, W)\n",
    "\n",
    "    for r in range(H):\n",
    "        for c in range(W):\n",
    "            ch = mdp.grid_chars[r, c]\n",
    "            if ch == \"#\":\n",
    "                txt = \"#\"\n",
    "            elif ch in (\"G\", \"H\"):\n",
    "                txt = ch\n",
    "            else:\n",
    "                s = mdp.pos_to_state[(r, c)]\n",
    "                txt = ACTION_SYMBOLS[int(greedy_actions[s])]\n",
    "            ax.text(c, r, txt, ha=\"center\", va=\"center\", color=\"black\", fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def validate_transition_model(mdp: TabularMDP) -> None:\n",
    "    for s in range(mdp.nS):\n",
    "        for a in range(mdp.nA):\n",
    "            p_sum = sum(p for p, _, _, _ in mdp.P[s][a])\n",
    "            if not np.isclose(p_sum, 1.0):\n",
    "                raise AssertionError(f\"Transition probabilities do not sum to 1 at state={s}, action={a}: {p_sum}\")\n",
    "    print(\"Transition model sanity check passed (all action distributions sum to 1).\")\n",
    "\n",
    "\n",
    "def plot_visitation_heatmap(env: GridWorldEnv, counts: np.ndarray, title: str = \"Random policy visitation\"):\n",
    "    grid = np.full((env.H, env.W), np.nan, dtype=float)\n",
    "    for s, (r, c) in env.state_to_pos.items():\n",
    "        grid[r, c] = counts[s]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    im = ax.imshow(grid, cmap=\"magma\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(range(env.W))\n",
    "    ax.set_yticks(range(env.H))\n",
    "    _draw_cell_grid(ax, env.H, env.W)\n",
    "\n",
    "    max_v = np.nanmax(grid) if np.any(~np.isnan(grid)) else 1.0\n",
    "    for r in range(env.H):\n",
    "        for c in range(env.W):\n",
    "            ch = env.grid[r, c]\n",
    "            if ch == \"#\":\n",
    "                ax.text(c, r, \"#\", ha=\"center\", va=\"center\", color=\"white\", fontweight=\"bold\")\n",
    "            elif not np.isnan(grid[r, c]):\n",
    "                color = \"black\" if grid[r, c] > 0.45 * max_v else \"white\"\n",
    "                ax.text(c, r, f\"{int(grid[r, c])}\", ha=\"center\", va=\"center\", color=color, fontsize=9)\n",
    "\n",
    "    fig.colorbar(im, ax=ax, shrink=0.8, label=\"visit count\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_value_heatmap(mdp: TabularMDP, V: np.ndarray, title: str = \"Value heatmap\"):\n",
    "    grid = values_to_grid(mdp, V)\n",
    "    H, W = grid.shape\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    im = ax.imshow(grid, cmap=\"viridis\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(range(W))\n",
    "    ax.set_yticks(range(H))\n",
    "    _draw_cell_grid(ax, H, W)\n",
    "\n",
    "    finite_vals = grid[np.isfinite(grid)]\n",
    "    vmin = float(np.min(finite_vals)) if finite_vals.size else 0.0\n",
    "    vmax = float(np.max(finite_vals)) if finite_vals.size else 1.0\n",
    "\n",
    "    for r in range(H):\n",
    "        for c in range(W):\n",
    "            ch = mdp.grid_chars[r, c]\n",
    "            if ch == \"#\":\n",
    "                ax.text(c, r, \"#\", ha=\"center\", va=\"center\", color=\"white\", fontweight=\"bold\")\n",
    "            elif np.isfinite(grid[r, c]):\n",
    "                color = _text_color_for_value(grid[r, c], vmin, vmax, threshold=0.55)\n",
    "                ax.text(c, r, f\"{grid[r, c]:.1f}\", ha=\"center\", va=\"center\", color=color, fontsize=9)\n",
    "\n",
    "    fig.colorbar(im, ax=ax, shrink=0.8, label=\"V(s)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_vi_convergence(deltas: list, title: str = \"Value Iteration convergence\"):\n",
    "    fig, ax = plt.subplots(figsize=(6, 3.5))\n",
    "    ax.plot(deltas, lw=2)\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_xlabel(\"Sweep\")\n",
    "    ax.set_ylabel(\"max update delta\")\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_vi_snapshots(mdp: TabularMDP, history: list, k_list=None, title_prefix: str = \"VI snapshot\"):\n",
    "    if not history:\n",
    "        return\n",
    "    if k_list is None:\n",
    "        last = len(history) - 1\n",
    "        mid = last // 2\n",
    "        k_list = sorted(set([0, mid, last]))\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(k_list), figsize=(4 * len(k_list), 4))\n",
    "    if len(k_list) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    all_grids = [values_to_grid(mdp, history[k]) for k in k_list]\n",
    "    all_vals = np.concatenate([g[np.isfinite(g)] for g in all_grids if np.any(np.isfinite(g))])\n",
    "    vmin = float(np.min(all_vals)) if all_vals.size else 0.0\n",
    "    vmax = float(np.max(all_vals)) if all_vals.size else 1.0\n",
    "\n",
    "    for ax, k in zip(axes, k_list):\n",
    "        grid = values_to_grid(mdp, history[k])\n",
    "        im = ax.imshow(grid, cmap=\"viridis\")\n",
    "        ax.set_title(f\"{title_prefix} k={k+1}\")\n",
    "        ax.set_xticks(range(grid.shape[1]))\n",
    "        ax.set_yticks(range(grid.shape[0]))\n",
    "        _draw_cell_grid(ax, grid.shape[0], grid.shape[1])\n",
    "\n",
    "        for r in range(grid.shape[0]):\n",
    "            for c in range(grid.shape[1]):\n",
    "                ch = mdp.grid_chars[r, c]\n",
    "                if ch == \"#\":\n",
    "                    ax.text(c, r, \"#\", ha=\"center\", va=\"center\", color=\"white\", fontweight=\"bold\")\n",
    "                elif np.isfinite(grid[r, c]):\n",
    "                    color = _text_color_for_value(grid[r, c], vmin, vmax, threshold=0.55)\n",
    "                    ax.text(c, r, f\"{grid[r, c]:.1f}\", ha=\"center\", va=\"center\", color=color, fontsize=8)\n",
    "\n",
    "    fig.colorbar(im, ax=axes, shrink=0.75)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def _draw_policy_on_axis(ax, mdp: TabularMDP, actions: np.ndarray, title: str):\n",
    "    arr = np.zeros(mdp.grid_chars.shape, dtype=float)\n",
    "    ax.imshow(arr, cmap=ListedColormap([\"#f7f7f7\"]))\n",
    "    ax.set_title(title, fontsize=11)\n",
    "    ax.set_xticks(range(arr.shape[1]))\n",
    "    ax.set_yticks(range(arr.shape[0]))\n",
    "    _draw_cell_grid(ax, arr.shape[0], arr.shape[1])\n",
    "\n",
    "    for r in range(arr.shape[0]):\n",
    "        for c in range(arr.shape[1]):\n",
    "            ch = mdp.grid_chars[r, c]\n",
    "            if ch == \"#\":\n",
    "                txt = \"#\"\n",
    "            elif ch in (\"G\", \"H\"):\n",
    "                txt = ch\n",
    "            else:\n",
    "                s = mdp.pos_to_state[(r, c)]\n",
    "                txt = ACTION_SYMBOLS[int(actions[s])]\n",
    "            ax.text(c, r, txt, ha=\"center\", va=\"center\", color=\"black\", fontsize=11, fontweight=\"bold\")\n",
    "\n",
    "\n",
    "def _draw_value_on_axis(ax, mdp: TabularMDP, V: np.ndarray, title: str):\n",
    "    grid = values_to_grid(mdp, V)\n",
    "    im = ax.imshow(grid, cmap=\"viridis\")\n",
    "    ax.set_title(title, fontsize=11)\n",
    "    ax.set_xticks(range(grid.shape[1]))\n",
    "    ax.set_yticks(range(grid.shape[0]))\n",
    "    _draw_cell_grid(ax, grid.shape[0], grid.shape[1])\n",
    "\n",
    "    finite_vals = grid[np.isfinite(grid)]\n",
    "    vmin = float(np.min(finite_vals)) if finite_vals.size else 0.0\n",
    "    vmax = float(np.max(finite_vals)) if finite_vals.size else 1.0\n",
    "\n",
    "    for r in range(grid.shape[0]):\n",
    "        for c in range(grid.shape[1]):\n",
    "            ch = mdp.grid_chars[r, c]\n",
    "            if ch == \"#\":\n",
    "                ax.text(c, r, \"#\", ha=\"center\", va=\"center\", color=\"white\", fontweight=\"bold\")\n",
    "            elif np.isfinite(grid[r, c]):\n",
    "                color = _text_color_for_value(grid[r, c], vmin, vmax, threshold=0.55)\n",
    "                ax.text(c, r, f\"{grid[r, c]:.1f}\", ha=\"center\", va=\"center\", color=color, fontsize=8)\n",
    "\n",
    "    return im\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _display_saved_media(path: Path) -> None:\n",
    "    display(DisplayImage(filename=str(path)))\n",
    "\n",
    "\n",
    "\n",
    "def _in_colab() -> bool:\n",
    "    try:\n",
    "        import google.colab  # type: ignore\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def _display_media_row(paths: List[Path], width_px: int = 420) -> None:\n",
    "    tags = []\n",
    "    colab = _in_colab()\n",
    "\n",
    "    for p in paths:\n",
    "        p = Path(p)\n",
    "        ext = p.suffix.lower().lstrip('.')\n",
    "        mime = 'image/gif' if ext == 'gif' else f\"image/{'jpeg' if ext in ('jpg', 'jpeg') else ext}\"\n",
    "\n",
    "        if colab:\n",
    "            b64 = base64.b64encode(p.read_bytes()).decode('utf-8')\n",
    "            src = f'data:{mime};base64,{b64}'\n",
    "        else:\n",
    "            src = p.as_posix()\n",
    "\n",
    "        tags.append(f'<img src=\"{src}\" style=\"width:{width_px}px;max-width:48%;height:auto;\"/>')\n",
    "\n",
    "    html = \"<div style='display:flex;gap:12px;align-items:flex-start;flex-wrap:wrap;'>\" + \"\".join(tags) + \"</div>\"\n",
    "    display(HTML(html))\n",
    "\n",
    "\n",
    "def _save_or_show_fig(fig, save: bool, path: Path | None = None, showcase: bool = True) -> None:\n",
    "    if save:\n",
    "        if path is None:\n",
    "            raise ValueError(\"path must be provided when save=True\")\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        fig.savefig(path, dpi=160, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "        print(f\"Saved PNG: {path}\")\n",
    "        if showcase:\n",
    "            _display_saved_media(path)\n",
    "    else:\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def _finalize_gif_animation(\n",
    "    ani,\n",
    "    fig,\n",
    "    save: bool = False,\n",
    "    gif_path: Path | None = None,\n",
    "    fps: int = 6,\n",
    "    showcase: bool = True,\n",
    "):\n",
    "    if save:\n",
    "        if gif_path is None:\n",
    "            raise ValueError(\"gif_path must be provided when save=True\")\n",
    "        gif_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        ani.save(gif_path, writer=PillowWriter(fps=fps))\n",
    "        plt.close(fig)\n",
    "        print(f\"Saved GIF: {gif_path}\")\n",
    "        if showcase:\n",
    "            _display_saved_media(gif_path)\n",
    "    else:\n",
    "        display(HTML(ani.to_jshtml()))\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "def save_vi_value_gif(\n",
    "    mdp: TabularMDP,\n",
    "    history: list,\n",
    "    gif_path: Path | None = None,\n",
    "    title_prefix: str = \"VI value evolution\",\n",
    "    save: bool = False,\n",
    "    fps: int = 6,\n",
    "    showcase: bool = True,\n",
    "):\n",
    "    if not history:\n",
    "        print(\"No history available; GIF was not created.\")\n",
    "        return\n",
    "\n",
    "    grids = [values_to_grid(mdp, V) for V in history]\n",
    "    H, W = grids[0].shape\n",
    "    all_vals = np.concatenate([g[np.isfinite(g)] for g in grids if np.any(np.isfinite(g))])\n",
    "    vmin = float(np.min(all_vals)) if all_vals.size else 0.0\n",
    "    vmax = float(np.max(all_vals)) if all_vals.size else 1.0\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    im = ax.imshow(grids[0], cmap=\"viridis\")\n",
    "    txt = ax.set_title(f\"{title_prefix} | sweep=1\")\n",
    "    ax.set_xticks(range(W))\n",
    "    ax.set_yticks(range(H))\n",
    "    _draw_cell_grid(ax, H, W)\n",
    "\n",
    "    text_art = {}\n",
    "    for r in range(H):\n",
    "        for c in range(W):\n",
    "            ch = mdp.grid_chars[r, c]\n",
    "            if ch == \"#\":\n",
    "                text_art[(r, c)] = ax.text(c, r, \"#\", ha=\"center\", va=\"center\", color=\"white\", fontweight=\"bold\")\n",
    "            elif ch in (\"G\", \"H\"):\n",
    "                text_art[(r, c)] = ax.text(c, r, ch, ha=\"center\", va=\"center\", color=\"black\", fontweight=\"bold\")\n",
    "            elif np.isfinite(grids[0][r, c]):\n",
    "                color = _text_color_for_value(grids[0][r, c], vmin, vmax, threshold=0.55)\n",
    "                text_art[(r, c)] = ax.text(c, r, f\"{grids[0][r, c]:.1f}\", ha=\"center\", va=\"center\", color=color, fontsize=8)\n",
    "\n",
    "    def update(frame_idx):\n",
    "        g = grids[frame_idx]\n",
    "        im.set_data(g)\n",
    "        txt.set_text(f\"{title_prefix} | sweep={frame_idx + 1}\")\n",
    "        for (r, c), artist in text_art.items():\n",
    "            ch = mdp.grid_chars[r, c]\n",
    "            if ch in (\"#\", \"G\", \"H\"):\n",
    "                continue\n",
    "            if np.isfinite(g[r, c]):\n",
    "                artist.set_text(f\"{g[r, c]:.1f}\")\n",
    "                artist.set_color(_text_color_for_value(g[r, c], vmin, vmax, threshold=0.55))\n",
    "        return (im, txt, *text_art.values())\n",
    "\n",
    "    ani = FuncAnimation(fig, update, frames=len(grids), interval=180, blit=False)\n",
    "    _finalize_gif_animation(ani, fig, save=save, gif_path=gif_path, fps=fps, showcase=showcase)\n",
    "\n",
    "\n",
    "def save_vi_convergence_png(\n",
    "\n",
    "    deltas: list,\n",
    "    png_path: Path | None = None,\n",
    "    title: str = \"VI convergence\",\n",
    "    save: bool = False,\n",
    "    showcase: bool = True,\n",
    "):\n",
    "    if not deltas:\n",
    "        print(\"No deltas available; PNG was not created.\")\n",
    "        return\n",
    "\n",
    "    if save and png_path is None:\n",
    "        raise ValueError(\"png_path must be provided when save=True\")\n",
    "\n",
    "    if save:\n",
    "        png_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 3.5))\n",
    "    ax.plot(deltas, lw=2)\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_xlabel(\"Sweep\")\n",
    "    ax.set_ylabel(\"max update delta\")\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    fig.tight_layout()\n",
    "    if save:\n",
    "        fig.savefig(png_path, dpi=160, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "        print(f\"Saved PNG: {png_path}\")\n",
    "        if showcase:\n",
    "            _display_saved_media(png_path)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def save_trajectory_gif(\n",
    "    env,\n",
    "    states: List[int],\n",
    "    gif_path: Path | None = None,\n",
    "    title: str = \"Trajectory evolution\",\n",
    "    fps: int = 6,\n",
    "    interval_ms: int = 170,\n",
    "    trail_len: int = 18,\n",
    "    save: bool = False,\n",
    "    showcase: bool = True,\n",
    "):\n",
    "\n",
    "    pts = np.array([env.state_to_pos[s] for s in states], dtype=float)\n",
    "    ys, xs = pts[:, 0], pts[:, 1]\n",
    "\n",
    "    arr = _map_code_array(env.grid)\n",
    "    cmap = ListedColormap([\"black\", \"white\", \"#A7D3F5\", \"#B7E4C7\", \"#F8B4B4\"])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    ax.imshow(arr, cmap=cmap, vmin=0, vmax=4)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(range(env.W))\n",
    "    ax.set_yticks(range(env.H))\n",
    "    _draw_cell_grid(ax, env.H, env.W)\n",
    "\n",
    "    for r in range(env.H):\n",
    "        for c in range(env.W):\n",
    "            txt = \" \" if env.grid[r, c] in (\".\", \"F\") else env.grid[r, c]\n",
    "            ax.text(c, r, txt, ha=\"center\", va=\"center\", color=\"black\", fontsize=10, fontweight=\"bold\")\n",
    "\n",
    "    lc = LineCollection([], linewidths=2.5, capstyle=\"round\")\n",
    "    ax.add_collection(lc)\n",
    "\n",
    "    ax.scatter(xs[0], ys[0], color=\"orange\", s=90, zorder=6, label=\"start\")\n",
    "    head = ax.scatter([], [], color=\"red\", s=75, zorder=7, label=\"current\")\n",
    "    ax.legend(loc=\"upper right\")\n",
    "\n",
    "    base_rgb = np.array([31/255, 119/255, 180/255])\n",
    "\n",
    "    def update(i):\n",
    "        if i == 0:\n",
    "            lc.set_segments([])\n",
    "            head.set_offsets([[xs[0], ys[0]]])\n",
    "            return lc, head\n",
    "\n",
    "        start_idx = max(0, i - trail_len)\n",
    "        segs, cols = [], []\n",
    "        window = i - start_idx\n",
    "\n",
    "        for j in range(start_idx, i):\n",
    "            segs.append([(xs[j], ys[j]), (xs[j + 1], ys[j + 1])])\n",
    "            age = i - j\n",
    "            alpha = max(0.08, 1.0 - (age - 1) / max(1, window))\n",
    "            cols.append((base_rgb[0], base_rgb[1], base_rgb[2], alpha))\n",
    "\n",
    "        lc.set_segments(segs)\n",
    "        lc.set_color(cols)\n",
    "        head.set_offsets([[xs[i], ys[i]]])\n",
    "        return lc, head\n",
    "\n",
    "    ani = FuncAnimation(fig, update, frames=len(states), interval=interval_ms, blit=False)\n",
    "    _finalize_gif_animation(ani, fig, save=save, gif_path=gif_path, fps=fps, showcase=showcase)\n",
    "\n",
    "\n",
    "\n",
    "def plot_value_pair(\n",
    "    mdp_left: TabularMDP,\n",
    "    V_left: np.ndarray,\n",
    "    title_left: str,\n",
    "    mdp_right: TabularMDP,\n",
    "    V_right: np.ndarray,\n",
    "    title_right: str,\n",
    "    save: bool = False,\n",
    "    png_path: Path | None = None,\n",
    "    showcase: bool = True,\n",
    "):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4.5))\n",
    "    im_l = _draw_value_on_axis(axes[0], mdp_left, V_left, title_left)\n",
    "    im_r = _draw_value_on_axis(axes[1], mdp_right, V_right, title_right)\n",
    "    fig.colorbar(im_l, ax=axes[0], shrink=0.8, label='V(s)')\n",
    "    fig.colorbar(im_r, ax=axes[1], shrink=0.8, label='V(s)')\n",
    "    _save_or_show_fig(fig, save=save, path=png_path if save else None, showcase=showcase)\n",
    "\n",
    "\n",
    "def plot_vi_convergence_pair(\n",
    "    deltas_det: list,\n",
    "    deltas_slip: list,\n",
    "    save: bool = False,\n",
    "    png_path: Path | None = None,\n",
    "    title: str = 'VI convergence comparison',\n",
    "    showcase: bool = True,\n",
    "):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(11, 3.8), sharey=True)\n",
    "    curves = [\n",
    "        (deltas_det, 'Deterministic'),\n",
    "        (deltas_slip, 'Slippery'),\n",
    "    ]\n",
    "    for ax, (deltas, name) in zip(axes, curves):\n",
    "        ax.plot(deltas, lw=2)\n",
    "        ax.set_yscale('log')\n",
    "        ax.set_xlabel('Sweep')\n",
    "        ax.set_title(name)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    axes[0].set_ylabel('max update delta')\n",
    "    fig.suptitle(title)\n",
    "    fig.tight_layout()\n",
    "    _save_or_show_fig(fig, save=save, path=png_path if save else None, showcase=showcase)\n",
    "\n",
    "\n",
    "def plot_pi_convergence_pair(\n",
    "    pi_det: dict,\n",
    "    pi_slip: dict,\n",
    "    save: bool = False,\n",
    "    png_path: Path | None = None,\n",
    "    title: str = 'PI convergence comparison',\n",
    "    showcase: bool = True,\n",
    "):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(11, 3.8), sharey=True)\n",
    "    curves = [\n",
    "        (pi_det, 'Deterministic'),\n",
    "        (pi_slip, 'Slippery'),\n",
    "    ]\n",
    "    for ax, (res, name) in zip(axes, curves):\n",
    "        ax.plot(res['eval_sweeps'], marker='o', lw=2)\n",
    "        ax.set_xlabel('Outer loop')\n",
    "        ax.set_title(name)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    axes[0].set_ylabel('Policy-evaluation sweeps')\n",
    "    fig.suptitle(title)\n",
    "    fig.tight_layout()\n",
    "    _save_or_show_fig(fig, save=save, path=png_path if save else None, showcase=showcase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96303201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example map intended for later Value/Policy Iteration experiments\n",
    "char_map = [\n",
    "    \"S...\",\n",
    "    \".#..\",\n",
    "    \"..H.\",\n",
    "    \"...G\",\n",
    "]\n",
    "\n",
    "cfg = GridWorldConfig(\n",
    "    char_map=char_map,\n",
    "    step_reward=-1.0,\n",
    "    goal_reward=10.0,\n",
    "    hole_reward=-10.0,\n",
    "    slip_prob=0.10,\n",
    ")\n",
    "\n",
    "env = GridWorldEnv(cfg)\n",
    "mdp = env.as_mdp()\n",
    "\n",
    "render_map(env, \"GridWorld\")\n",
    "validate_transition_model(mdp)\n",
    "print(f\"nS={mdp.nS}, nA={mdp.nA}, start={mdp.start_state}, terminal_states={sorted(mdp.terminal_states)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Embedded output asset(s) -->\n",
    "![cell-output](assets/web_outputs/session_02_mdp_dynamic_programming_cell011_out00.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dded41e",
   "metadata": {},
   "source": [
    "This map defines the full planning problem that the dynamic-programming algorithms will solve. Walls shape the feasible state transitions, the goal provides positive terminal reward, and holes end the episode with negative payoff. Reading this map first is important because all later value and policy patterns are direct consequences of this transition structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5a5a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick rollout demo with a random policy to validate reset/step interface\n",
    "state = env.reset()\n",
    "trajectory = [state]\n",
    "rewards = []\n",
    "\n",
    "for t in range(25):\n",
    "    action = int(rng.integers(0, env.nA))\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    trajectory.append(next_state)\n",
    "    rewards.append(reward)\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(\"Rollout length:\", len(trajectory) - 1)\n",
    "print(\"Total reward:\", float(np.sum(rewards)))\n",
    "print(\"Visited states:\", trajectory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bd1b4a",
   "metadata": {},
   "source": [
    "### Experiments - Random walk and diagnostics\n",
    "\n",
    "These experiments deepen intuition before dynamic programming algorithms.\n",
    "\n",
    "1. Random-walk baseline performance.\n",
    "2. Monte Carlo visitation heatmap.\n",
    "3. Transition probability sanity probes.\n",
    "4. Reward sensitivity sweep.\n",
    "5. Single-episode trajectory visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540ce43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_random_episode(env: GridWorldEnv, max_steps: int = 200):\n",
    "    state = env.reset()\n",
    "    states = [state]\n",
    "    actions = []\n",
    "    rewards = []\n",
    "\n",
    "    done = False\n",
    "    for _ in range(max_steps):\n",
    "        a = int(rng.integers(0, env.nA))\n",
    "        s_next, r, done, _ = env.step(a)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        states.append(s_next)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        \"states\": states,\n",
    "        \"actions\": actions,\n",
    "        \"rewards\": rewards,\n",
    "        \"done\": done,\n",
    "        \"return\": float(np.sum(rewards)),\n",
    "        \"length\": len(actions),\n",
    "        \"terminal_state\": states[-1],\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_random_policy(env: GridWorldEnv, n_episodes: int = 2000, max_steps: int = 200):\n",
    "    success = 0\n",
    "    holes = 0\n",
    "    timeouts = 0\n",
    "    returns = []\n",
    "    lengths = []\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        ep = run_random_episode(env, max_steps=max_steps)\n",
    "        returns.append(ep[\"return\"])\n",
    "        lengths.append(ep[\"length\"])\n",
    "\n",
    "        if ep[\"done\"]:\n",
    "            r, c = env.state_to_pos[ep[\"terminal_state\"]]\n",
    "            cell = env.grid[r, c]\n",
    "            if cell == \"G\":\n",
    "                success += 1\n",
    "            elif cell == \"H\":\n",
    "                holes += 1\n",
    "        else:\n",
    "            timeouts += 1\n",
    "\n",
    "    return {\n",
    "        \"episodes\": n_episodes,\n",
    "        \"success_rate\": success / n_episodes,\n",
    "        \"hole_rate\": holes / n_episodes,\n",
    "        \"timeout_rate\": timeouts / n_episodes,\n",
    "        \"mean_return\": float(np.mean(returns)),\n",
    "        \"std_return\": float(np.std(returns)),\n",
    "        \"mean_length\": float(np.mean(lengths)),\n",
    "    }\n",
    "\n",
    "\n",
    "def print_baseline_report(title: str, report: dict):\n",
    "    print(title)\n",
    "    print(f\"  episodes:     {report['episodes']}\")\n",
    "    print(f\"  success_rate: {report['success_rate']*100:6.2f}%\")\n",
    "    print(f\"  hole_rate:    {report['hole_rate']*100:6.2f}%\")\n",
    "    print(f\"  timeout_rate: {report['timeout_rate']*100:6.2f}%\")\n",
    "    print(f\"  mean_return:  {report['mean_return']:7.3f} ± {report['std_return']:.3f}\")\n",
    "    print(f\"  mean_length:  {report['mean_length']:7.2f}\")\n",
    "\n",
    "\n",
    "cfg_det = GridWorldConfig(char_map=char_map, step_reward=-1.0, goal_reward=10.0, hole_reward=-10.0, slip_prob=0.0)\n",
    "cfg_slip = GridWorldConfig(char_map=char_map, step_reward=-1.0, goal_reward=10.0, hole_reward=-10.0, slip_prob=0.20)\n",
    "\n",
    "env_det = GridWorldEnv(cfg_det)\n",
    "env_slip = GridWorldEnv(cfg_slip)\n",
    "\n",
    "rep_det = evaluate_random_policy(env_det, n_episodes=1500, max_steps=120)\n",
    "rep_slip = evaluate_random_policy(env_slip, n_episodes=1500, max_steps=120)\n",
    "\n",
    "print_baseline_report(\"Random policy baseline | deterministic\", rep_det)\n",
    "print()\n",
    "print_baseline_report(\"Random policy baseline | slippery (slip_prob=0.20)\", rep_slip)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63df050f",
   "metadata": {},
   "source": [
    "Comparing deterministic and slippery variants isolates the effect of transition uncertainty while keeping the map itself fixed. In the deterministic case, one action leads to one predictable next state, so optimal behavior is usually sharper and faster to compute. In the slippery case, action outcomes branch, so values become smoother and planning requires more conservative decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363abc14",
   "metadata": {},
   "source": [
    "**Monte Carlo visitation heatmap**\n",
    "\n",
    "State visitation frequencies show which areas are naturally explored under random behavior.\n",
    "This is useful context before policy optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b213f9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visitation_counts_random(env: GridWorldEnv, n_episodes: int = 2000, max_steps: int = 200) -> np.ndarray:\n",
    "    counts = np.zeros(env.nS, dtype=float)\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        s = env.reset()\n",
    "        counts[s] += 1\n",
    "        for _ in range(max_steps):\n",
    "            a = int(rng.integers(0, env.nA))\n",
    "            s_next, _, done, _ = env.step(a)\n",
    "            counts[s_next] += 1\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    return counts\n",
    "\n",
    "\n",
    "counts_det = visitation_counts_random(env_det, n_episodes=2000, max_steps=120)\n",
    "counts_slip = visitation_counts_random(env_slip, n_episodes=2000, max_steps=120)\n",
    "\n",
    "plot_visitation_heatmap(env_det, counts_det, \"Visitation heatmap | deterministic\")\n",
    "plot_visitation_heatmap(env_slip, counts_slip, \"Visitation heatmap | slippery\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Embedded output asset(s) -->\n",
    "![cell-output](assets/web_outputs/session_02_mdp_dynamic_programming_cell018_out00.png)\n",
    "![cell-output](assets/web_outputs/session_02_mdp_dynamic_programming_cell018_out01.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1efb4e",
   "metadata": {},
   "source": [
    "This visitation heatmap is a baseline diagnostic before optimization. It reveals where random behavior naturally accumulates occupancy, which often corresponds to bottlenecks, dead-ends, or loops in the transition graph. The contrast between regions helps explain why some states become strategically important once VI or PI starts propagating value information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5055ca2",
   "metadata": {},
   "source": [
    "**Transition sanity probes**\n",
    "\n",
    "For selected states/actions, inspect the full $p(s', r, done \\mid s, a)$ entries.\n",
    "This validates edge handling, wall collisions, and slip dynamics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d946549a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_transition_probe(env: GridWorldEnv, state_pos: Tuple[int, int], action: int):\n",
    "    s = env.pos_to_state[state_pos]\n",
    "    print(f\"Probe at state={s}, pos={state_pos}, action={action} ({ACTION_SYMBOLS[action]})\")\n",
    "    rows = sorted(env.as_mdp().P[s][action], key=lambda x: (-x[0], x[1]))\n",
    "    for p, s_next, r, done in rows:\n",
    "        print(f\"  p={p:.3f} -> s'={s_next:2d}, pos'={env.state_to_pos[s_next]}, reward={r:5.1f}, done={done}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "print(\"Deterministic probes\")\n",
    "print_transition_probe(env_det, (0, 0), RIGHT)\n",
    "print_transition_probe(env_det, (0, 0), UP)\n",
    "print_transition_probe(env_det, (1, 0), RIGHT)  # attempts to move into wall at (1,1)\n",
    "\n",
    "print(\"Slippery probes\")\n",
    "print_transition_probe(env_slip, (0, 0), RIGHT)\n",
    "print_transition_probe(env_slip, (1, 0), RIGHT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cb1837",
   "metadata": {},
   "source": [
    "**Trajectory visualization**\n",
    "\n",
    "Plot a single random episode path on top of the grid for debugging and teaching.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9dcb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "ep = run_random_episode(env_slip, max_steps=60)\n",
    "print(f\"Trajectory length={ep['length']}, return={ep['return']:.2f}, done={ep['done']}\")\n",
    "save_trajectory_gif(\n",
    "    env_slip,\n",
    "    ep[\"states\"],\n",
    "    gif_path=Path(\"assets/web_outputs/random_walk_slippery_fading.gif\"),\n",
    "    title=\"Random walk trajectory (slippery, fading trail)\",\n",
    "    fps=6,\n",
    "    trail_len=18,\n",
    "    save=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Embedded output asset(s) -->\n",
    "![cell-output](assets/web_outputs/random_walk_slippery_fading.gif)\n",
    "![cell-output](assets/web_outputs/session_02_mdp_dynamic_programming_cell023_out01.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671b2ade",
   "metadata": {},
   "source": [
    "The trajectory animation makes random exploration dynamics concrete in state space. Instead of only seeing aggregate counts, you can follow how local action noise and map geometry interact step by step. The fading trail keeps temporal order visible while still allowing you to see the most recent motion clearly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102ca564",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "\n",
    "In *value iteration*, we start with an arbitrary initial value function and then iteratively improve it until it converges to the optimal value function. At each iteration, we update the value of each state based on the Bellman optimality equation, which states that the value of a state is equal to the immediate reward plus the discounted value of the successor states, weighted by the probability of transitioning to those states under the optimal policy.\n",
    "The algorithm uses the following update rule:\n",
    "\n",
    "$$ v_{k+1}(s) = \\max_a \\sum_{s', r}p(s', r | s, a) [r + \\gamma v_k(s')] $$\n",
    "\n",
    "By repeatedly applying the Bellman optimality equation, the value function converges to the optimal value function in the limit. In practice, we stop when the magnitude of the greatest update, $\\delta$ falls below a sufficiently low threshold, $\\theta$.\n",
    "The code below implements value iteration for this problem, and plots out the value function after each iteration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57aa4fb5",
   "metadata": {},
   "source": [
    "![Value Iteration Pseudocode](assets/web_outputs/value_iteration.png)\n",
    "\n",
    "*Sutton & Barto, Chapter 4/5 dynamic programming notation reference used in this session.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9087ace9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bellman_optimality_backup(mdp: TabularMDP, V: np.ndarray, s: int, gamma: float) -> float:\n",
    "    ########################################################################\n",
    "    if s in mdp.terminal_states:\n",
    "        return 0.0\n",
    "\n",
    "    q_vals = np.zeros(mdp.nA, dtype=float)\n",
    "    for a in range(mdp.nA):\n",
    "        for p, s_next, r, done in mdp.P[s][a]:\n",
    "            q_vals[a] += p * (r + gamma * (0.0 if done else V[s_next]))\n",
    "    ########################################################################\n",
    "    return float(np.max(q_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adce14dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def greedy_actions_from_values(mdp: TabularMDP, V: np.ndarray, gamma: float, theta: float = 1e-10) -> np.ndarray:\n",
    "    actions = np.zeros(mdp.nS, dtype=int)\n",
    "    for s in range(mdp.nS):\n",
    "        if s in mdp.terminal_states:\n",
    "            actions[s] = 0\n",
    "            continue\n",
    "\n",
    "        q_vals = np.zeros(mdp.nA, dtype=float)\n",
    "        for a in range(mdp.nA):\n",
    "            for p, s_next, r, done in mdp.P[s][a]:\n",
    "                q_vals[a] += p * (r + gamma * (0.0 if done else V[s_next]))\n",
    "        best_q = float(np.max(q_vals))\n",
    "        best_actions = np.flatnonzero(q_vals >= (best_q - theta))\n",
    "        actions[s] = int(best_actions[0])\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d9787b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(\n",
    "    mdp: TabularMDP,\n",
    "    gamma: float = 0.95,\n",
    "    theta: float = 1e-10,\n",
    "    max_sweeps: int = 10_000,\n",
    "    return_history: bool = False,\n",
    "):\n",
    "    V = np.zeros(mdp.nS, dtype=float)\n",
    "    deltas = []\n",
    "    history = []\n",
    "    ########################################################################\n",
    "    for sweep in range(1, max_sweeps + 1):\n",
    "        V_old = V.copy()\n",
    "        delta = 0.0\n",
    "\n",
    "        for s in range(mdp.nS):\n",
    "            v_new = bellman_optimality_backup(mdp, V_old, s, gamma)\n",
    "            V[s] = v_new\n",
    "            delta = max(delta, abs(V[s] - V_old[s]))\n",
    "\n",
    "        deltas.append(delta)\n",
    "        if return_history:\n",
    "            history.append(V.copy())\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    ########################################################################\n",
    "    greedy_actions = greedy_actions_from_values(mdp, V, gamma, theta=theta)\n",
    "    result = {\n",
    "        \"V\": V,\n",
    "        \"greedy_actions\": greedy_actions,\n",
    "        \"sweeps\": sweep,\n",
    "        \"deltas\": deltas,\n",
    "    }\n",
    "    if return_history:\n",
    "        result[\"history\"] = history\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d79ffe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_det = value_iteration(env_det.as_mdp(), gamma=0.95, theta=1e-10, return_history=True)\n",
    "res_slip = value_iteration(env_slip.as_mdp(), gamma=0.95, theta=1e-10, return_history=True)\n",
    "\n",
    "print(f\"Deterministic: sweeps={res_det['sweeps']}, final_delta={res_det['deltas'][-1]:.3e}\")\n",
    "print(f\"Slippery:      sweeps={res_slip['sweeps']}, final_delta={res_slip['deltas'][-1]:.3e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1762f87",
   "metadata": {},
   "source": [
    "**Value Iteration visual diagnostics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcb2282",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp_det = env_det.as_mdp()\n",
    "mdp_slip = env_slip.as_mdp()\n",
    "\n",
    "try:\n",
    "    out_dir = Path(\"assets/web_outputs\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # # 1) Final value tables (numbers in cells), side by side\n",
    "    # plot_value_pair(\n",
    "    #     mdp_det,\n",
    "    #     res_det[\"V\"],\n",
    "    #     \"VI final values | deterministic\",\n",
    "    #     mdp_slip,\n",
    "    #     res_slip[\"V\"],\n",
    "    #     \"VI final values | slippery\",\n",
    "    #     save=True,\n",
    "    #     png_path=out_dir / \"session2_vi_values_pair.png\",\n",
    "    # )\n",
    "\n",
    "    # 2) Only two VI evolution GIFs\n",
    "    vi_gif_det = out_dir / \"session2_vi_value_det.gif\"\n",
    "    vi_gif_slip = out_dir / \"session2_vi_value_slip.gif\"\n",
    "\n",
    "    save_vi_value_gif(\n",
    "        mdp_det,\n",
    "        res_det[\"history\"],\n",
    "        gif_path=vi_gif_det,\n",
    "        title_prefix=\"VI value evolution (deterministic)\",\n",
    "        save=True,\n",
    "        showcase=False,\n",
    "    )\n",
    "    save_vi_value_gif(\n",
    "        mdp_slip,\n",
    "        res_slip[\"history\"],\n",
    "        gif_path=vi_gif_slip,\n",
    "        title_prefix=\"VI value evolution (slippery)\",\n",
    "        save=True,\n",
    "        showcase=False,\n",
    "    )\n",
    "    _display_media_row([vi_gif_det, vi_gif_slip], width_px=430)\n",
    "\n",
    "    # 3) Convergence plots side by side\n",
    "    plot_vi_convergence_pair(\n",
    "        res_det[\"deltas\"],\n",
    "        res_slip[\"deltas\"],\n",
    "        save=True,\n",
    "        png_path=out_dir / \"session2_vi_convergence_pair.png\",\n",
    "        title=\"VI convergence | deterministic vs slippery\",\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"VI showcase export skipped: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Embedded output asset(s) -->\n",
    "![cell-output](assets/web_outputs/session2_vi_value_det.gif)\n",
    "![cell-output](assets/web_outputs/session2_vi_value_slip.gif)\n",
    "![cell-output](assets/web_outputs/session2_vi_convergence_pair.png)\n",
    "![cell-output](assets/web_outputs/session_02_mdp_dynamic_programming_cell032_out03.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da663dc1",
   "metadata": {},
   "source": [
    "These outputs summarize Value Iteration on deterministic and slippery dynamics in one aligned view. The value-evolution GIFs show how rewards propagate backward through reachable states as Bellman backups iterate, while the side-by-side convergence plot shows the computational impact of uncertainty. Together they connect algorithm mechanics, final values, and runtime behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ded83e3",
   "metadata": {},
   "source": [
    "### Value Iteration example: greedy-policy rollouts\n",
    "\n",
    "We execute episodes with the greedy policy returned by Value Iteration to inspect behavior, not only value tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580bc880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout_with_greedy_actions(env: GridWorldEnv, greedy_actions: np.ndarray, max_steps: int = 100):\n",
    "    s = env.reset()\n",
    "    states = [s]\n",
    "    rewards = []\n",
    "    done = False\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        a = int(greedy_actions[s])\n",
    "        s_next, r, done, _ = env.step(a)\n",
    "        states.append(s_next)\n",
    "        rewards.append(r)\n",
    "        s = s_next\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        \"states\": states,\n",
    "        \"return\": float(np.sum(rewards)),\n",
    "        \"length\": len(rewards),\n",
    "        \"done\": done,\n",
    "        \"terminal_state\": states[-1],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf713f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_greedy_policy(env: GridWorldEnv, greedy_actions: np.ndarray, n_episodes: int = 200):\n",
    "    returns = []\n",
    "    success = 0\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        ep = rollout_with_greedy_actions(env, greedy_actions, max_steps=120)\n",
    "        returns.append(ep[\"return\"])\n",
    "        if ep[\"done\"]:\n",
    "            r, c = env.state_to_pos[ep[\"terminal_state\"]]\n",
    "            if env.grid[r, c] == \"G\":\n",
    "                success += 1\n",
    "\n",
    "    return float(np.mean(returns)), success / n_episodes\n",
    "\n",
    "\n",
    "mean_ret_det, succ_det = evaluate_greedy_policy(env_det, res_det[\"greedy_actions\"], n_episodes=200)\n",
    "mean_ret_slip, succ_slip = evaluate_greedy_policy(env_slip, res_slip[\"greedy_actions\"], n_episodes=200)\n",
    "\n",
    "print(f\"Deterministic greedy policy: mean_return={mean_ret_det:.3f}, success_rate={succ_det*100:.1f}%\")\n",
    "print(f\"Slippery greedy policy:      mean_return={mean_ret_slip:.3f}, success_rate={succ_slip*100:.1f}%\")\n",
    "\n",
    "ep_det = rollout_with_greedy_actions(env_det, res_det[\"greedy_actions\"], max_steps=60)\n",
    "ep_slip = rollout_with_greedy_actions(env_slip, res_slip[\"greedy_actions\"], max_steps=60)\n",
    "\n",
    "\n",
    "try:\n",
    "    out_dir = Path(\"assets/web_outputs\")\n",
    "    gif_det = out_dir / \"session2_vi_greedy_traj_det.gif\"\n",
    "    gif_slip = out_dir / \"session2_vi_greedy_traj_slip.gif\"\n",
    "\n",
    "    save_trajectory_gif(\n",
    "        env_det,\n",
    "        ep_det[\"states\"],\n",
    "        gif_path=gif_det,\n",
    "        title=\"Greedy rollout evolution (deterministic)\",\n",
    "        save=True,\n",
    "        showcase=False,\n",
    "    )\n",
    "    save_trajectory_gif(\n",
    "        env_slip,\n",
    "        ep_slip[\"states\"],\n",
    "        gif_path=gif_slip,\n",
    "        title=\"Greedy rollout evolution (slippery)\",\n",
    "        save=True,\n",
    "        showcase=False,\n",
    "    )\n",
    "\n",
    "    _display_media_row([gif_det, gif_slip], width_px=430)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"VI trajectory GIF export skipped: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Embedded output asset(s) -->\n",
    "![cell-output](assets/web_outputs/session2_vi_greedy_traj_det.gif)\n",
    "![cell-output](assets/web_outputs/session2_vi_greedy_traj_slip.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d9138f",
   "metadata": {},
   "source": [
    "**Value Iteration experiment A: `gamma` sensitivity**\n",
    "\n",
    "This experiment shows how the discount factor $\\gamma$ changes the agent’s notion of “good.” In practice, $\\gamma$ is what decides whether the policy prefers short-term gains or is willing to sacrifice immediate reward for better long-term outcomes. That is why this is not just a tuning detail: different $\\gamma$ values can produce genuinely different behaviors, especially in maps where the shortest path is also riskier.\n",
    "\n",
    "We track convergence sweeps, the value of the start state, and policy changes across $\\gamma$ values. Together, these reveal both the behavioral effect (what the agent chooses) and the computational effect (how hard the planning problem is)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2323aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gamma-sensitivity showcase map:\n",
    "# short risky route near holes vs longer safer detour\n",
    "gamma_sensitive_map_10x10 = [\n",
    "    \"....S....\",\n",
    "    \"#...##..#\",\n",
    "    \"#...##..#\",\n",
    "    \"#H.H##..#\",\n",
    "    \"#H.H##..#\",\n",
    "    \"#HGH##..#\",\n",
    "    \"#...##..#\",\n",
    "    \"#...##..#\",\n",
    "    \".........\",\n",
    "]\n",
    "\n",
    "cfg_gamma_demo = GridWorldConfig(\n",
    "    char_map=gamma_sensitive_map_10x10,\n",
    "    step_reward=-0.2,\n",
    "    goal_reward=10.0,\n",
    "    hole_reward=-12.0,\n",
    "    slip_prob=0.15,\n",
    ")\n",
    "\n",
    "env_gamma_demo = GridWorldEnv(cfg_gamma_demo)\n",
    "mdp_gamma_demo = env_gamma_demo.as_mdp()\n",
    "\n",
    "render_map(env_gamma_demo, title=\"Gamma-sensitivity demo map (10x10)\")\n",
    "validate_transition_model(mdp_gamma_demo)\n",
    "\n",
    "print(\n",
    "    f\"nS={mdp_gamma_demo.nS}, nA={mdp_gamma_demo.nA}, \"\n",
    "    f\"start={mdp_gamma_demo.start_state}, terminals={sorted(mdp_gamma_demo.terminal_states)}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Embedded output asset(s) -->\n",
    "![cell-output](assets/web_outputs/session_02_mdp_dynamic_programming_cell038_out00.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5d1229",
   "metadata": {},
   "source": [
    "This map is intentionally structured to contain meaningful route trade-offs rather than one obviously dominant shortest path. Because of that, changing the discount factor can alter whether the policy prefers immediate local safety or delayed global reward. It is a better probe for gamma sensitivity than a trivial corridor map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c840360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_policies(actions_a: np.ndarray, actions_b: np.ndarray, terminal_states: set) -> float:\n",
    "    mask = np.ones_like(actions_a, dtype=bool)\n",
    "    for s in terminal_states:\n",
    "        mask[s] = False\n",
    "    return float(np.mean(actions_a[mask] == actions_b[mask]))\n",
    "\n",
    "\n",
    "gammas = [0.70, 0.85, 0.95, 0.99]\n",
    "rows = []\n",
    "policies = {}\n",
    "\n",
    "for g in gammas:\n",
    "    out = value_iteration(mdp_gamma_demo, gamma=g, theta=1e-10, return_history=False)\n",
    "    s0 = mdp_gamma_demo.start_state\n",
    "    rows.append((g, out[\"sweeps\"], out[\"V\"][s0]))\n",
    "    policies[g] = out[\"greedy_actions\"]\n",
    "\n",
    "print(\"gamma | sweeps | V(start)\")\n",
    "for g, sw, vs in rows:\n",
    "    print(f\"{g:4.2f} | {sw:6d} | {vs:8.3f}\")\n",
    "\n",
    "print(\"\\nPolicy agreement vs gamma=0.95\")\n",
    "base = policies[0.95]\n",
    "for g in gammas:\n",
    "    agree = compare_policies(base, policies[g], mdp_gamma_demo.terminal_states)\n",
    "    print(f\"gamma={g:4.2f} -> agreement={agree*100:6.2f}%\")\n",
    "\n",
    "plt.figure(figsize=(6, 3.5))\n",
    "plt.plot([r[0] for r in rows], [r[2] for r in rows], marker=\"o\")\n",
    "plt.xlabel(\"gamma\")\n",
    "plt.ylabel(\"V(start)\")\n",
    "plt.title(\"Gamma sensitivity (stochastic GridWorld)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Embedded output asset(s) -->\n",
    "![cell-output](assets/web_outputs/session_02_mdp_dynamic_programming_cell040_out01.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a946cc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low-vs-high gamma behavior showcase (trajectory GIF pair)\n",
    "try:\n",
    "    out_dir = Path(\"assets/web_outputs\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    g_low, g_high = 0.7, 0.99\n",
    "    vi_low = value_iteration(mdp_gamma_demo, gamma=g_low, theta=1e-10, return_history=False)\n",
    "    vi_high = value_iteration(mdp_gamma_demo, gamma=g_high, theta=1e-10, return_history=False)\n",
    "\n",
    "    ep_low = rollout_with_greedy_actions(env_gamma_demo, vi_low[\"greedy_actions\"], max_steps=180)\n",
    "    ep_high = rollout_with_greedy_actions(env_gamma_demo, vi_high[\"greedy_actions\"], max_steps=180)\n",
    "\n",
    "    gif_low = out_dir / \"session2_gamma_low_traj.gif\"\n",
    "    gif_high = out_dir / \"session2_gamma_high_traj.gif\"\n",
    "\n",
    "    save_trajectory_gif(\n",
    "        env_gamma_demo,\n",
    "        ep_low[\"states\"],\n",
    "        gif_path=gif_low,\n",
    "        title=f\"Greedy rollout | gamma={g_low:.2f}\",\n",
    "        trail_len=22,\n",
    "        save=True,\n",
    "        showcase=False,\n",
    "    )\n",
    "    save_trajectory_gif(\n",
    "        env_gamma_demo,\n",
    "        ep_high[\"states\"],\n",
    "        gif_path=gif_high,\n",
    "        title=f\"Greedy rollout | gamma={g_high:.2f}\",\n",
    "        trail_len=22,\n",
    "        save=True,\n",
    "        showcase=False,\n",
    "    )\n",
    "\n",
    "    print(f\"gamma={g_low:.2f}: return={ep_low['return']:.3f}, length={ep_low['length']}, done={ep_low['done']}\")\n",
    "    print(f\"gamma={g_high:.2f}: return={ep_high['return']:.3f}, length={ep_high['length']}, done={ep_high['done']}\")\n",
    "\n",
    "    _display_media_row([gif_low, gif_high], width_px=430)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Gamma rollout GIF showcase skipped: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Embedded output asset(s) -->\n",
    "![cell-output](assets/web_outputs/session2_gamma_low_traj.gif)\n",
    "![cell-output](assets/web_outputs/session2_gamma_high_traj.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dea0851",
   "metadata": {},
   "source": [
    "As gamma changes, both the optimization target and the planning dynamics change. The start-state value captures how strongly the agent values delayed outcomes, while agreement scores quantify whether the policy structure itself is robust or shifts across settings. Looking at both is essential: similar values can still hide different action choices, and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f044fc50",
   "metadata": {},
   "source": [
    "Policy agreement is not 100% across all settings, so the map is now expressing real decision trade-offs. In other words, changing $\\gamma$ changes what the agent considers optimal, not just the numeric value scale. That is exactly what we wanted from this experiment.\n",
    "\n",
    "The start-state value trend is also informative: the value is worst around intermediate-high $\\gamma$ here, then improves again at $\\gamma=0.99$, which suggests the objective is balancing longer-horizon gains against increased exposure to per-step penalties and stochastic risk in a nontrivial way. The relationship is not monotonic, and that is normal on maps with competing routes and slip dynamics.\n",
    "\n",
    "Sweep counts vary substantially, which confirms the computational effect too: different $\\gamma$ values induce different convergence behavior. The key takeaway is that $\\gamma$ is not a cosmetic hyperparameter; it changes both policy structure and optimization difficulty on this map."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9771ac",
   "metadata": {},
   "source": [
    "**Value Iteration experiment B: `theta` (stopping threshold) sensitivity**\n",
    "\n",
    "This experiment focuses on the practical convergence trade-off in Value Iteration. The threshold $\\theta$ decides when we stop iterating: a larger $\\theta$ stops earlier with a rougher approximation, while a smaller $\\theta$ pushes for a more precise value function but costs more computation. In real work, this is one of the key knobs for balancing speed and solution quality.\n",
    "\n",
    "We compare sweep count, runtime, and accuracy against a strict reference solution. This makes it clear where extra computation still improves the result and where it mostly adds cost without meaningful policy benefit.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273d6e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theta-sensitivity benchmark map (stochastic + long-horizon to make theta matter)\n",
    "theta_benchmark_map_10x10 = [\n",
    "    \"S...#.....\",\n",
    "    \".##.#.##..\",\n",
    "    \"...#...#..\",\n",
    "    \".#..##.#..\",\n",
    "    \"..#....#..\",\n",
    "    \".###.#.##.\",\n",
    "    \"...#...#..\",\n",
    "    \".#.###.#..\",\n",
    "    \".#...H.#..\",\n",
    "    \"...#....G.\",\n",
    "]\n",
    "\n",
    "cfg_theta_demo = GridWorldConfig(\n",
    "    char_map=theta_benchmark_map_10x10,\n",
    "    step_reward=-0.01,\n",
    "    goal_reward=1.0,\n",
    "    hole_reward=-1.0,\n",
    "    slip_prob=0.20,   # stochastic transitions -> smoother convergence, theta has visible effect\n",
    ")\n",
    "\n",
    "env_theta_demo = GridWorldEnv(cfg_theta_demo)\n",
    "mdp_theta_demo = env_theta_demo.as_mdp()\n",
    "\n",
    "render_map(env_theta_demo, title=\"Theta benchmark map (stochastic)\")\n",
    "print(\n",
    "    f\"theta demo: nS={mdp_theta_demo.nS}, nA={mdp_theta_demo.nA}, \"\n",
    "    f\"start={mdp_theta_demo.start_state}, terminals={sorted(mdp_theta_demo.terminal_states)}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Embedded output asset(s) -->\n",
    "![cell-output](assets/web_outputs/session_02_mdp_dynamic_programming_cell045_out00.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a790eb",
   "metadata": {},
   "source": [
    "This benchmark map is used to isolate stopping-threshold behavior from unrelated sources of variation. By controlling map structure and reward scale, theta can be interpreted primarily as a numerical-precision knob. That makes runtime and value-error differences more meaningful and easier to compare across runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a46bb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = [1e-4, 1e-6, 1e-8, 1e-10]\n",
    "ref = value_iteration(mdp_theta_demo, gamma=0.99, theta=1e-12, return_history=False)\n",
    "V_ref = ref[\"V\"]\n",
    "\n",
    "results = []\n",
    "for th in thetas:\n",
    "    t0 = time.perf_counter()\n",
    "    out = value_iteration(mdp_theta_demo, gamma=0.99, theta=th, return_history=False)\n",
    "    dt_ms = (time.perf_counter() - t0) * 1e3\n",
    "    linf = float(np.max(np.abs(out[\"V\"] - V_ref)))\n",
    "    results.append((th, out[\"sweeps\"], dt_ms, linf))\n",
    "\n",
    "print(\"theta | sweeps | runtime_ms | L_inf_to_ref\")\n",
    "for th, sw, dt, linf in results:\n",
    "    print(f\"{th:>5.0e} | {sw:6d} | {dt:10.3f} | {linf:11.3e}\")\n",
    "\n",
    "plt.figure(figsize=(6, 3.5))\n",
    "plt.plot([r[0] for r in results], [r[1] for r in results], marker=\"o\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"theta\")\n",
    "plt.ylabel(\"sweeps\")\n",
    "plt.title(\"Stopping threshold vs sweep count\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Embedded output asset(s) -->\n",
    "![cell-output](assets/web_outputs/session_02_mdp_dynamic_programming_cell047_out01.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e8b937",
   "metadata": {},
   "source": [
    "This table and plot quantify the core practical trade-off behind theta. A smaller threshold keeps iterative updates running until finer residual errors are removed, which usually improves approximation quality relative to a strict reference solution. The cost is increased sweeps and runtime, so this experiment helps choose a threshold that is precise enough without overcomputing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225e8c38",
   "metadata": {},
   "source": [
    "If runtime and sweeps rise sharply as $\\theta$ gets smaller but policy changes are minimal, you are already in a “good enough” regime at a looser threshold. If both value error and policy behavior still change noticeably, tighter convergence is justified. The point of this experiment is to identify a threshold that is accurate enough for decision quality without overpaying in computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdd5b66",
   "metadata": {},
   "source": [
    "These rollout GIFs validate that the VI policy is not only numerically optimal in value tables but also behaviorally coherent when executed. Seeing deterministic and slippery trajectories side by side makes risk handling visible: one path is crisp, the other may include corrective motion under stochastic transitions. This closes the loop between planning outputs and actual control behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2939476",
   "metadata": {},
   "source": [
    "## Policy Evaluation + Policy Iteration\n",
    "\n",
    "### Short theory recap (Sutton & Barto, Ch. 4)\n",
    "\n",
    "Policy Iteration alternates between two operators:\n",
    "\n",
    "1. **Policy Evaluation**: compute $V^{\\pi}$ for the current policy $\\pi$.\n",
    "2. **Policy Improvement**: update policy greedily with respect to the evaluated value function.\n",
    "\n",
    "Policy evaluation uses the Bellman expectation equation:\n",
    "$$\n",
    "V^{\\pi}(s)=\\sum_a \\pi(a\\mid s)\\sum_{s',r}p(s',r\\mid s,a)\\left[r+\\gamma V^{\\pi}(s')\\right]\n",
    "$$\n",
    "\n",
    "Policy improvement is greedy:\n",
    "$$\n",
    "\\pi_{new}(s)\\in\\argmax_a \\sum_{s',r}p(s',r\\mid s,a)\\left[r+\\gamma V^{\\pi}(s')\\right]\n",
    "$$\n",
    "\n",
    "With finite tabular MDPs, repeated evaluation + improvement converges to an optimal policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85f8197",
   "metadata": {},
   "source": [
    "![Policy Iteration Pseudocode](assets/web_outputs/policy_iteration.png)\n",
    "\n",
    "*Sutton & Barto policy evaluation and policy improvement loop reference used in this section.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7bc592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy_from_values(mdp: TabularMDP, V: np.ndarray, gamma: float = 0.95, theta: float = 1e-10) -> np.ndarray:\n",
    "    policy = np.zeros((mdp.nS, mdp.nA), dtype=float)\n",
    "\n",
    "    for s in range(mdp.nS):\n",
    "        if s in mdp.terminal_states:\n",
    "            policy[s, :] = 1.0 / mdp.nA\n",
    "            continue\n",
    "\n",
    "        q_vals = np.zeros(mdp.nA, dtype=float)\n",
    "        for a in range(mdp.nA):\n",
    "            for p, s_next, r, done in mdp.P[s][a]:\n",
    "                q_vals[a] += p * (r + gamma * (0.0 if done else V[s_next]))\n",
    "\n",
    "        best_q = float(np.max(q_vals))\n",
    "        best_actions = np.flatnonzero(q_vals >= (best_q - theta))\n",
    "        best_a = int(best_actions[0])\n",
    "        policy[s, best_a] = 1.0\n",
    "\n",
    "    return policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77f6133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(\n",
    "    mdp: TabularMDP,\n",
    "    policy: np.ndarray,\n",
    "    gamma: float = 0.95,\n",
    "    theta: float = 1e-10,\n",
    "    max_sweeps: int = 50_000,\n",
    "):\n",
    "    V = np.zeros(mdp.nS, dtype=float)\n",
    "    deltas = []\n",
    "    ########################################################################\n",
    "    for sweep in range(1, max_sweeps + 1):\n",
    "        V_old = V.copy()\n",
    "        delta = 0.0\n",
    "\n",
    "        for s in range(mdp.nS):\n",
    "            if s in mdp.terminal_states:\n",
    "                V[s] = 0.0\n",
    "                continue\n",
    "\n",
    "            v_new = 0.0\n",
    "            for a in range(mdp.nA):\n",
    "                pi_sa = policy[s, a]\n",
    "                if pi_sa == 0.0:\n",
    "                    continue\n",
    "                for p, s_next, r, done in mdp.P[s][a]:\n",
    "                    v_new += pi_sa * p * (r + gamma * (0.0 if done else V_old[s_next]))\n",
    "\n",
    "            V[s] = v_new\n",
    "            delta = max(delta, abs(V[s] - V_old[s]))\n",
    "\n",
    "        deltas.append(delta)\n",
    "        if delta < theta:\n",
    "            return V, sweep, deltas\n",
    "    ########################################################################\n",
    "    return V, max_sweeps, deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18536a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(\n",
    "    mdp: TabularMDP,\n",
    "    gamma: float = 0.95,\n",
    "    eval_theta: float = 1e-10,\n",
    "    max_outer_loops: int = 1_000,\n",
    "    return_history: bool = False,\n",
    "):\n",
    "    policy = np.ones((mdp.nS, mdp.nA), dtype=float) / mdp.nA\n",
    "\n",
    "    history = []\n",
    "    eval_sweeps_list = []\n",
    "    eval_delta_curves = []\n",
    "    ########################################################################\n",
    "    for outer in range(1, max_outer_loops + 1):\n",
    "        V, eval_sweeps, eval_deltas = policy_evaluation(\n",
    "            mdp,\n",
    "            policy,\n",
    "            gamma=gamma,\n",
    "            theta=eval_theta,\n",
    "        )\n",
    "\n",
    "        eval_sweeps_list.append(eval_sweeps)\n",
    "        eval_delta_curves.append(eval_deltas)\n",
    "\n",
    "        improved_policy = greedy_policy_from_values(mdp, V, gamma=gamma, theta=eval_theta)\n",
    "\n",
    "        stable = np.array_equal(np.argmax(policy, axis=1), np.argmax(improved_policy, axis=1))\n",
    "    ########################################################################\n",
    "        if return_history:\n",
    "            history.append(\n",
    "                {\n",
    "                    \"outer\": outer,\n",
    "                    \"V\": V.copy(),\n",
    "                    \"policy\": improved_policy.copy(),\n",
    "                    \"eval_sweeps\": eval_sweeps,\n",
    "                    \"eval_deltas\": eval_deltas,\n",
    "                    \"stable\": stable,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        policy = improved_policy\n",
    "        if stable:\n",
    "            break\n",
    "\n",
    "    result = {\n",
    "        \"policy\": policy,\n",
    "        \"V\": V,\n",
    "        \"outer_loops\": outer,\n",
    "        \"eval_sweeps\": eval_sweeps_list,\n",
    "        \"eval_delta_curves\": eval_delta_curves,\n",
    "    }\n",
    "    if return_history:\n",
    "        result[\"history\"] = history\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f16709",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_det = policy_iteration(mdp_det, gamma=0.95, eval_theta=1e-10, return_history=True)\n",
    "pi_slip = policy_iteration(mdp_slip, gamma=0.95, eval_theta=1e-10, return_history=True)\n",
    "\n",
    "print(f\"Policy Iteration (deterministic): outer_loops={pi_det['outer_loops']}, eval_sweeps_total={sum(pi_det['eval_sweeps'])}\")\n",
    "print(f\"Policy Iteration (slippery):      outer_loops={pi_slip['outer_loops']}, eval_sweeps_total={sum(pi_slip['eval_sweeps'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654ec3d3",
   "metadata": {},
   "source": [
    "### Policy Iteration visual diagnostics\n",
    "\n",
    "This section mirrors the Value Iteration diagnostics with a compact Policy Iteration view:\n",
    "1. PI evolution GIFs for deterministic and slippery settings (side by side),\n",
    "2. greedy action arrows overlaid at each outer iteration,\n",
    "3. PI convergence plots shown as a matched pair below.\n",
    "\n",
    "Using the same save-and-display flow as VI keeps the comparison clean and consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e58357",
   "metadata": {},
   "source": [
    "### Policy Iteration snapshots and GIF\n",
    "\n",
    "We capture intermediate policies and values across outer loops to show how policy improvement progresses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c3ee35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_policy_iteration_gif(\n",
    "    mdp: TabularMDP,\n",
    "    history: list,\n",
    "    gif_path: Path | None = None,\n",
    "    title_prefix: str = \"Policy Iteration\",\n",
    "    save: bool = False,\n",
    "    fps: int = 2,\n",
    "    showcase: bool = True,\n",
    "):\n",
    "    if not history:\n",
    "        print(\"No history available; GIF was not created.\")\n",
    "        return\n",
    "\n",
    "    value_grids = [values_to_grid(mdp, h[\"V\"]) for h in history]\n",
    "    action_grids = []\n",
    "    for h in history:\n",
    "        a = np.argmax(h[\"policy\"], axis=1)\n",
    "        g = np.full(mdp.grid_chars.shape, -1, dtype=int)\n",
    "        for s, (r, c) in mdp.state_to_pos.items():\n",
    "            if mdp.grid_chars[r, c] not in (\"G\", \"H\"):\n",
    "                g[r, c] = a[s]\n",
    "        action_grids.append(g)\n",
    "\n",
    "    vec = {\n",
    "        UP: (0.0, -0.35),\n",
    "        RIGHT: (0.35, 0.0),\n",
    "        DOWN: (0.0, 0.35),\n",
    "        LEFT: (-0.35, 0.0),\n",
    "    }\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    im = ax.imshow(value_grids[0], cmap=\"viridis\")\n",
    "    txt = ax.set_title(f\"{title_prefix} | outer=1\")\n",
    "    ax.set_xticks(range(value_grids[0].shape[1]))\n",
    "    ax.set_yticks(range(value_grids[0].shape[0]))\n",
    "    _draw_cell_grid(ax, value_grids[0].shape[0], value_grids[0].shape[1])\n",
    "\n",
    "    q = None\n",
    "\n",
    "    def update(frame_idx):\n",
    "        nonlocal q\n",
    "        im.set_data(value_grids[frame_idx])\n",
    "\n",
    "        if q is not None:\n",
    "            q.remove()\n",
    "\n",
    "        actions = action_grids[frame_idx]\n",
    "        X, Y, U, V = [], [], [], []\n",
    "        for r in range(actions.shape[0]):\n",
    "            for c in range(actions.shape[1]):\n",
    "                if mdp.grid_chars[r, c] in (\"#\", \"G\", \"H\"):\n",
    "                    continue\n",
    "                a = actions[r, c]\n",
    "                if a < 0:\n",
    "                    continue\n",
    "                u, v = vec[a]\n",
    "                X.append(c)\n",
    "                Y.append(r)\n",
    "                U.append(u)\n",
    "                V.append(v)\n",
    "\n",
    "        if X:\n",
    "            q = ax.quiver(X, Y, U, V, angles=\"xy\", scale_units=\"xy\", scale=1, color=\"white\", width=0.008)\n",
    "\n",
    "        outer = history[frame_idx][\"outer\"]\n",
    "        txt.set_text(f\"{title_prefix} | outer={outer}\")\n",
    "        return im, txt\n",
    "\n",
    "    ani = FuncAnimation(fig, update, frames=len(history), interval=550, blit=False)\n",
    "    _finalize_gif_animation(ani, fig, save=save, gif_path=gif_path, fps=fps, showcase=showcase)\n",
    "\n",
    "\n",
    "try:\n",
    "    out_dir = Path(\"assets/web_outputs\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pi_gif_det = out_dir / \"session2_pi_evolution_det.gif\"\n",
    "    pi_gif_slip = out_dir / \"session2_pi_evolution_slip.gif\"\n",
    "\n",
    "    save_policy_iteration_gif(\n",
    "        mdp_det,\n",
    "        pi_det[\"history\"],\n",
    "        pi_gif_det,\n",
    "        title_prefix=\"Policy Iteration deterministic\",\n",
    "        save=True,\n",
    "        showcase=False,\n",
    "    )\n",
    "    save_policy_iteration_gif(\n",
    "        mdp_slip,\n",
    "        pi_slip[\"history\"],\n",
    "        pi_gif_slip,\n",
    "        title_prefix=\"Policy Iteration slippery\",\n",
    "        save=True,\n",
    "        showcase=False,\n",
    "    )\n",
    "    _display_media_row([pi_gif_det, pi_gif_slip], width_px=430)\n",
    "\n",
    "    plot_pi_convergence_pair(\n",
    "        pi_det,\n",
    "        pi_slip,\n",
    "        save=True,\n",
    "        png_path=out_dir / \"session2_pi_convergence_pair.png\",\n",
    "        title=\"PI convergence | deterministic vs slippery\",\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"PI GIF export skipped: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Embedded output asset(s) -->\n",
    "![cell-output](assets/web_outputs/session2_pi_evolution_det.gif)\n",
    "![cell-output](assets/web_outputs/session2_pi_evolution_slip.gif)\n",
    "![cell-output](assets/web_outputs/session2_pi_convergence_pair.png)\n",
    "![cell-output](assets/web_outputs/session_02_mdp_dynamic_programming_cell059_out03.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9f80fd",
   "metadata": {},
   "source": [
    "Policy Iteration is shown in the same visual format as Value Iteration so the comparison stays fair and readable. The evolution GIFs emphasize policy-improvement steps over outer iterations, while convergence panels show policy-evaluation effort per iteration. This helps distinguish algorithmic behavior from environment effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3b9343",
   "metadata": {},
   "source": [
    "### Policy Iteration example: greedy-policy rollouts\n",
    "\n",
    "We run episodes with the final PI policy in deterministic and slippery environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f9a9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_actions_det = np.argmax(pi_det[\"policy\"], axis=1)\n",
    "pi_actions_slip = np.argmax(pi_slip[\"policy\"], axis=1)\n",
    "\n",
    "mean_ret_det_pi, succ_det_pi = evaluate_greedy_policy(env_det, pi_actions_det, n_episodes=200)\n",
    "mean_ret_slip_pi, succ_slip_pi = evaluate_greedy_policy(env_slip, pi_actions_slip, n_episodes=200)\n",
    "\n",
    "print(f\"PI deterministic policy: mean_return={mean_ret_det_pi:.3f}, success_rate={succ_det_pi*100:.1f}%\")\n",
    "print(f\"PI slippery policy:      mean_return={mean_ret_slip_pi:.3f}, success_rate={succ_slip_pi*100:.1f}%\")\n",
    "\n",
    "ep_det_pi = rollout_with_greedy_actions(env_det, pi_actions_det, max_steps=60)\n",
    "ep_slip_pi = rollout_with_greedy_actions(env_slip, pi_actions_slip, max_steps=60)\n",
    "\n",
    "try:\n",
    "    out_dir = Path(\"assets/web_outputs\")\n",
    "    gif_det = out_dir / \"session2_pi_greedy_traj_det.gif\"\n",
    "    gif_slip = out_dir / \"session2_pi_greedy_traj_slip.gif\"\n",
    "\n",
    "    save_trajectory_gif(\n",
    "        env_det,\n",
    "        ep_det_pi[\"states\"],\n",
    "        gif_det,\n",
    "        title=\"Greedy rollout evolution (PI deterministic)\",\n",
    "        save=True,\n",
    "        showcase=False,\n",
    "    )\n",
    "    save_trajectory_gif(\n",
    "        env_slip,\n",
    "        ep_slip_pi[\"states\"],\n",
    "        gif_slip,\n",
    "        title=\"Greedy rollout evolution (PI slippery)\",\n",
    "        save=True,\n",
    "        showcase=False,\n",
    "    )\n",
    "\n",
    "    _display_media_row([gif_det, gif_slip], width_px=430)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"PI trajectory GIF export skipped: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Embedded output asset(s) -->\n",
    "![cell-output](assets/web_outputs/session2_pi_greedy_traj_det.gif)\n",
    "![cell-output](assets/web_outputs/session2_pi_greedy_traj_slip.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ac7ac7",
   "metadata": {},
   "source": [
    "### Policy Iteration experiment A: `gamma` sensitivity\n",
    "\n",
    "What we analyze:\n",
    "- number of outer loops,\n",
    "- total policy-evaluation sweeps,\n",
    "- value at the start state,\n",
    "- policy agreement relative to $\\gamma=0.95$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b688ed92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same gamma-sensitive map as in Value Iteration for fair comparison\n",
    "gammas_pi = [0.70, 0.85, 0.95, 0.99]\n",
    "rows_pi = []\n",
    "policies_pi = {}\n",
    "\n",
    "for g in gammas_pi:\n",
    "    out = policy_iteration(mdp_gamma_demo, gamma=g, eval_theta=1e-10, return_history=False)\n",
    "    start_v = out[\"V\"][mdp_gamma_demo.start_state]\n",
    "    rows_pi.append((g, out[\"outer_loops\"], int(sum(out[\"eval_sweeps\"])), start_v))\n",
    "    policies_pi[g] = np.argmax(out[\"policy\"], axis=1)\n",
    "\n",
    "print(\"gamma | outer_loops | eval_sweeps_total | V(start)\")\n",
    "for g, ol, es, vs in rows_pi:\n",
    "    print(f\"{g:4.2f} | {ol:11d} | {es:17d} | {vs:8.3f}\")\n",
    "\n",
    "print(\"\\nPolicy agreement vs gamma=0.95\")\n",
    "base = policies_pi[0.95]\n",
    "for g in gammas_pi:\n",
    "    agree = compare_policies(base, policies_pi[g], mdp_gamma_demo.terminal_states)\n",
    "    print(f\"gamma={g:4.2f} -> agreement={agree*100:6.2f}%\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(11, 3.8))\n",
    "\n",
    "axes[0].plot([r[0] for r in rows_pi], [r[3] for r in rows_pi], marker=\"o\")\n",
    "axes[0].set_xlabel(\"gamma\")\n",
    "axes[0].set_ylabel(\"V(start)\")\n",
    "axes[0].set_title(\"PI gamma sensitivity: start value\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot([r[0] for r in rows_pi], [r[2] for r in rows_pi], marker=\"o\")\n",
    "axes[1].set_xlabel(\"gamma\")\n",
    "axes[1].set_ylabel(\"total eval sweeps\")\n",
    "axes[1].set_title(\"PI gamma sensitivity: computation\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Embedded output asset(s) -->\n",
    "![cell-output](assets/web_outputs/session_02_mdp_dynamic_programming_cell064_out01.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471e9b7a",
   "metadata": {},
   "source": [
    "Running PI gamma sensitivity on exactly the same map used for VI removes a major confound in interpretation. If behavior differs, that difference is much more likely due to algorithmic update structure rather than environment design. This is the right setup for a method-level comparison in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b674c1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Low-vs-high gamma behavior showcase for Policy Iteration (trajectory GIF pair)\n",
    "try:\n",
    "    out_dir = Path(\"assets/web_outputs\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    g_low, g_high = 0.70, 0.99\n",
    "    pi_low = policy_iteration(mdp_gamma_demo, gamma=g_low, eval_theta=1e-10, return_history=False)\n",
    "    pi_high = policy_iteration(mdp_gamma_demo, gamma=g_high, eval_theta=1e-10, return_history=False)\n",
    "\n",
    "    actions_low = np.argmax(pi_low[\"policy\"], axis=1)\n",
    "    actions_high = np.argmax(pi_high[\"policy\"], axis=1)\n",
    "\n",
    "    ep_low = rollout_with_greedy_actions(env_gamma_demo, actions_low, max_steps=180)\n",
    "    ep_high = rollout_with_greedy_actions(env_gamma_demo, actions_high, max_steps=180)\n",
    "\n",
    "    gif_low = out_dir / \"session2_pi_gamma_low_traj.gif\"\n",
    "    gif_high = out_dir / \"session2_pi_gamma_high_traj.gif\"\n",
    "\n",
    "    save_trajectory_gif(\n",
    "        env_gamma_demo,\n",
    "        ep_low[\"states\"],\n",
    "        gif_path=gif_low,\n",
    "        title=f\"PI greedy rollout | gamma={g_low:.2f}\",\n",
    "        trail_len=22,\n",
    "        save=True,\n",
    "        showcase=False,\n",
    "    )\n",
    "    save_trajectory_gif(\n",
    "        env_gamma_demo,\n",
    "        ep_high[\"states\"],\n",
    "        gif_path=gif_high,\n",
    "        title=f\"PI greedy rollout | gamma={g_high:.2f}\",\n",
    "        trail_len=22,\n",
    "        save=True,\n",
    "        showcase=False,\n",
    "    )\n",
    "\n",
    "    print(f\"gamma={g_low:.2f}: return={ep_low['return']:.3f}, length={ep_low['length']}, done={ep_low['done']}\")\n",
    "    print(f\"gamma={g_high:.2f}: return={ep_high['return']:.3f}, length={ep_high['length']}, done={ep_high['done']}\")\n",
    "\n",
    "    _display_media_row([gif_low, gif_high], width_px=430)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"PI gamma rollout GIF showcase skipped: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Embedded output asset(s) -->\n",
    "![cell-output](assets/web_outputs/session2_pi_gamma_low_traj.gif)\n",
    "![cell-output](assets/web_outputs/session2_pi_gamma_high_traj.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24153e2d",
   "metadata": {},
   "source": [
    "### Policy Iteration experiment B: evaluation-threshold sensitivity\n",
    "\n",
    "What we analyze:\n",
    "- how `eval_theta` affects computational effort,\n",
    "- whether the final greedy policy changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de2cb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_thetas = [1e-4, 1e-6, 1e-8, 1e-10]\n",
    "rows_theta = []\n",
    "pol_by_theta = {}\n",
    "\n",
    "for th in eval_thetas:\n",
    "    out = policy_iteration(mdp_det, gamma=0.95, eval_theta=th, return_history=False)\n",
    "    actions = np.argmax(out[\"policy\"], axis=1)\n",
    "    rows_theta.append((th, out[\"outer_loops\"], int(sum(out[\"eval_sweeps\"]))))\n",
    "    pol_by_theta[th] = actions\n",
    "\n",
    "print(\"eval_theta | outer_loops | eval_sweeps_total\")\n",
    "for th, ol, es in rows_theta:\n",
    "    print(f\"{th:>9.0e} | {ol:11d} | {es:17d}\")\n",
    "\n",
    "base = pol_by_theta[1e-10]\n",
    "print(\"\\nPolicy agreement vs eval_theta=1e-10\")\n",
    "for th in eval_thetas:\n",
    "    agree = compare_policies(base, pol_by_theta[th], mdp_det.terminal_states)\n",
    "    print(f\"eval_theta={th:>9.0e} -> agreement={agree*100:6.2f}%\")\n",
    "\n",
    "plt.figure(figsize=(6, 3.5))\n",
    "plt.plot([r[0] for r in rows_theta], [r[2] for r in rows_theta], marker=\"o\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"eval_theta\")\n",
    "plt.ylabel(\"total evaluation sweeps\")\n",
    "plt.title(\"Policy Iteration threshold sensitivity\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Embedded output asset(s) -->\n",
    "![cell-output](assets/web_outputs/session_02_mdp_dynamic_programming_cell068_out01.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bdbe73",
   "metadata": {},
   "source": [
    "Evaluation-threshold sensitivity in PI shows the same practical precision-versus-cost trade-off, but through a different mechanism than VI. Here the threshold controls how accurately each policy-evaluation phase is solved before improvement. The resulting pattern clarifies how inner-loop accuracy influences total outer-loop efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be965a96",
   "metadata": {},
   "source": [
    "These PI rollout animations are the behavioral counterpart of the PI convergence diagnostics. They show whether the final improved policy actually executes robustly in deterministic and slippery settings, not only whether residuals decreased during evaluation. This makes the success-rate numbers easier to trust and interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d93028",
   "metadata": {},
   "source": [
    "## Large-map experiments\n",
    "\n",
    "Before switching to Gym/FrozenLake, we stress-test Value Iteration and Policy Iteration on larger custom GridWorld maps.\n",
    "\n",
    "Focus:\n",
    "- scalability with increasing state count,\n",
    "- policy quality and agreement,\n",
    "- runtime/sweep trade-offs,\n",
    "- more visual intuition on complex layouts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9de730",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_maps = {\n",
    "    \"maze_8x8\": [\n",
    "        \"S..#....\",\n",
    "        \".##.#H#.\",\n",
    "        \"...#.#..\",\n",
    "        \".#...##.\",\n",
    "        \".#.H....\",\n",
    "        \".###.#..\",\n",
    "        \"...#..#.\",\n",
    "        \".H....G.\",\n",
    "    ],\n",
    "    \"corridor_10x10\": [\n",
    "        \"S...#.....\",\n",
    "        \".##.#.###.\",\n",
    "        \".#..#...#.\",\n",
    "        \".#.###.#..\",\n",
    "        \".H....#.#.\",\n",
    "        \"###.#.#.#.\",\n",
    "        \"...#...#..\",\n",
    "        \".#.###.##.\",\n",
    "        \".#...H...#\",\n",
    "        \"...##...G.\",\n",
    "    ],\n",
    "    \"rooms_12x12\": [\n",
    "        \"S...#......G\",\n",
    "        \".##.#..##H#H\",\n",
    "        \"...#.#...#..\",\n",
    "        \".#.#.###.#..\",\n",
    "        \".#...#......\",\n",
    "        \".###.#H###..\",\n",
    "        \"...#...#....\",\n",
    "        \".#.###.#.##.\",\n",
    "        \".#...H.#...#\",\n",
    "        \".###H#.###.#\",\n",
    "        \"...#...#....\",\n",
    "        \".H...#...#..\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def make_large_env(char_map, slip_prob=0.10, step_reward=-1.0, goal_reward=12.0, hole_reward=-12.0):\n",
    "    cfg = GridWorldConfig(\n",
    "        char_map=char_map,\n",
    "        step_reward=step_reward,\n",
    "        goal_reward=goal_reward,\n",
    "        hole_reward=hole_reward,\n",
    "        slip_prob=slip_prob,\n",
    "    )\n",
    "    return GridWorldEnv(cfg)\n",
    "\n",
    "\n",
    "def benchmark_vi_pi_on_env(env: GridWorldEnv, gamma: float = 0.95, theta: float = 1e-10):\n",
    "    mdp = env.as_mdp()\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    vi = value_iteration(mdp, gamma=gamma, theta=theta, return_history=True)\n",
    "    t_vi_ms = (time.perf_counter() - t0) * 1e3\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    pi = policy_iteration(mdp, gamma=gamma, eval_theta=theta, return_history=True)\n",
    "    t_pi_ms = (time.perf_counter() - t0) * 1e3\n",
    "\n",
    "    vi_actions = vi[\"greedy_actions\"]\n",
    "    pi_actions = np.argmax(pi[\"policy\"], axis=1)\n",
    "\n",
    "    agreement = compare_policies(vi_actions, pi_actions, mdp.terminal_states)\n",
    "    value_linf = float(np.max(np.abs(vi[\"V\"] - pi[\"V\"])))\n",
    "\n",
    "    vi_ret, vi_succ = evaluate_greedy_policy(env, vi_actions, n_episodes=200)\n",
    "    pi_ret, pi_succ = evaluate_greedy_policy(env, pi_actions, n_episodes=200)\n",
    "\n",
    "    return {\n",
    "        \"mdp\": mdp,\n",
    "        \"vi\": vi,\n",
    "        \"pi\": pi,\n",
    "        \"t_vi_ms\": t_vi_ms,\n",
    "        \"t_pi_ms\": t_pi_ms,\n",
    "        \"value_linf\": value_linf,\n",
    "        \"policy_agreement\": agreement,\n",
    "        \"vi_mean_return\": vi_ret,\n",
    "        \"vi_success_rate\": vi_succ,\n",
    "        \"pi_mean_return\": pi_ret,\n",
    "        \"pi_success_rate\": pi_succ,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137f010c",
   "metadata": {},
   "source": [
    "**Run VI vs PI on all larger maps**\n",
    "\n",
    "Metrics:\n",
    "- states/actions,\n",
    "- VI and PI runtime,\n",
    "- VI sweeps,\n",
    "- PI outer loops and total evaluation sweeps,\n",
    "- value-function $L_\\infty$ difference,\n",
    "- greedy-policy agreement,\n",
    "- rollout return and success rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940f087d",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_results = {}\n",
    "\n",
    "for name, char_map in large_maps.items():\n",
    "    env = make_large_env(char_map, slip_prob=0.0,step_reward=-0.1) # Negative -1 would go to hole because its less loss than reaching goal, so we use smaller step penalty to encourage reaching goal\n",
    "    \n",
    "    res = benchmark_vi_pi_on_env(env, gamma=0.95, theta=1e-10)\n",
    "    large_results[name] = {\"env\": env, **res}\n",
    "\n",
    "print(\n",
    "    \"map | nS | VI_ms | PI_ms | VI_sweeps | PI_outer | PI_eval_total | L_inf(VI-PI) | policy_agree | VI_succ | PI_succ\"\n",
    ")\n",
    "for name, data in large_results.items():\n",
    "    mdp = data[\"mdp\"]\n",
    "    vi = data[\"vi\"]\n",
    "    pi = data[\"pi\"]\n",
    "    print(\n",
    "        f\"{name:12s} | {mdp.nS:3d} | {data['t_vi_ms']:6.1f} | {data['t_pi_ms']:6.1f} | \"\n",
    "        f\"{vi['sweeps']:9d} | {pi['outer_loops']:8d} | {sum(pi['eval_sweeps']):13d} | \"\n",
    "        f\"{data['value_linf']:11.2e} | {data['policy_agreement']*100:11.1f}% | \"\n",
    "        f\"{data['vi_success_rate']*100:6.1f}% | {data['pi_success_rate']*100:6.1f}%\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed5a679",
   "metadata": {},
   "source": [
    "**Experiment L3 - Minimal per-map showcase**\n",
    "For each large map we show only:\n",
    "- one static map image,\n",
    "- VI value-evolution GIF and PI evolution GIF (side by side),\n",
    "- VI greedy-rollout GIF and PI greedy-rollout GIF (side by side)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fd359a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ensure_large_showcase_assets(name: str, data: dict, out_dir: Path):\n",
    "    env = data[\"env\"]\n",
    "    mdp = data[\"mdp\"]\n",
    "    vi = data[\"vi\"]\n",
    "    pi = data[\"pi\"]\n",
    "\n",
    "    p_vi_value = out_dir / f\"session2_large_{name}_vi_value.gif\"\n",
    "    p_pi_evo = out_dir / f\"session2_large_{name}_pi_evolution.gif\"\n",
    "    p_vi_traj = out_dir / f\"session2_large_{name}_vi_traj.gif\"\n",
    "    p_pi_traj = out_dir / f\"session2_large_{name}_pi_traj.gif\"\n",
    "\n",
    "    save_vi_value_gif(\n",
    "        mdp,\n",
    "        vi[\"history\"],\n",
    "        gif_path=p_vi_value,\n",
    "        title_prefix=f\"VI value evolution ({name})\",\n",
    "        save=True,\n",
    "        showcase=False,\n",
    "    )\n",
    "    save_policy_iteration_gif(\n",
    "        mdp,\n",
    "        pi[\"history\"],\n",
    "        gif_path=p_pi_evo,\n",
    "        title_prefix=f\"PI evolution ({name})\",\n",
    "        save=True,\n",
    "        showcase=False,\n",
    "    )\n",
    "\n",
    "    ep_vi = rollout_with_greedy_actions(env, vi[\"greedy_actions\"], max_steps=220)\n",
    "    ep_pi = rollout_with_greedy_actions(env, np.argmax(pi[\"policy\"], axis=1), max_steps=220)\n",
    "\n",
    "    save_trajectory_gif(\n",
    "        env,\n",
    "        ep_vi[\"states\"],\n",
    "        gif_path=p_vi_traj,\n",
    "        title=f\"VI greedy rollout ({name})\",\n",
    "        trail_len=22,\n",
    "        save=True,\n",
    "        showcase=False,\n",
    "    )\n",
    "    save_trajectory_gif(\n",
    "        env,\n",
    "        ep_pi[\"states\"],\n",
    "        gif_path=p_pi_traj,\n",
    "        title=f\"PI greedy rollout ({name})\",\n",
    "        trail_len=22,\n",
    "        save=True,\n",
    "        showcase=False,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"vi_value\": p_vi_value,\n",
    "        \"pi_evolution\": p_pi_evo,\n",
    "        \"vi_traj\": p_vi_traj,\n",
    "        \"pi_traj\": p_pi_traj,\n",
    "    }\n",
    "\n",
    "\n",
    "def show_large_map_minimal_showcase(selected_maps, large_results):\n",
    "    out_dir = Path(\"assets/web_outputs\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for name in selected_maps:\n",
    "        data = large_results[name]\n",
    "        env = data[\"env\"]\n",
    "\n",
    "        print(f\"\\n=== {name} ===\")\n",
    "\n",
    "        # 1) static map image\n",
    "        render_map(env, title=f\"Map layout | {name}\")\n",
    "\n",
    "        # 2-3) precompute and show only required GIF pairs\n",
    "        paths = _ensure_large_showcase_assets(name, data, out_dir)\n",
    "\n",
    "        print(\"Evolution GIFs: VI vs PI\")\n",
    "        _display_media_row([paths[\"vi_value\"], paths[\"pi_evolution\"]], width_px=430)\n",
    "\n",
    "        print(\"Greedy rollout GIFs: VI vs PI\")\n",
    "        _display_media_row([paths[\"vi_traj\"], paths[\"pi_traj\"]], width_px=430)\n",
    "\n",
    "    print(f\"\\nSaved large-map GIFs into: {out_dir}\")\n",
    "\n",
    "\n",
    "selected_maps = [\"maze_8x8\", \"corridor_10x10\", \"rooms_12x12\"]\n",
    "show_large_map_minimal_showcase(selected_maps, large_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Embedded output asset(s) -->\n",
    "![cell-output](assets/web_outputs/session_02_mdp_dynamic_programming_cell076_out01.png)\n",
    "![cell-output](assets/web_outputs/session2_large_maze_8x8_vi_value.gif)\n",
    "![cell-output](assets/web_outputs/session2_large_maze_8x8_pi_evolution.gif)\n",
    "![cell-output](assets/web_outputs/session2_large_maze_8x8_vi_traj.gif)\n",
    "![cell-output](assets/web_outputs/session2_large_maze_8x8_pi_traj.gif)\n",
    "![cell-output](assets/web_outputs/session_02_mdp_dynamic_programming_cell076_out07.png)\n",
    "![cell-output](assets/web_outputs/session2_large_corridor_10x10_vi_value.gif)\n",
    "![cell-output](assets/web_outputs/session2_large_corridor_10x10_pi_evolution.gif)\n",
    "![cell-output](assets/web_outputs/session2_large_corridor_10x10_vi_traj.gif)\n",
    "![cell-output](assets/web_outputs/session2_large_corridor_10x10_pi_traj.gif)\n",
    "![cell-output](assets/web_outputs/session_02_mdp_dynamic_programming_cell076_out13.png)\n",
    "![cell-output](assets/web_outputs/session2_large_rooms_12x12_vi_value.gif)\n",
    "![cell-output](assets/web_outputs/session2_large_rooms_12x12_pi_evolution.gif)\n",
    "![cell-output](assets/web_outputs/session2_large_rooms_12x12_vi_traj.gif)\n",
    "![cell-output](assets/web_outputs/session2_large_rooms_12x12_pi_traj.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d3d59f",
   "metadata": {},
   "source": [
    "For each larger map, this compact showcase keeps only the visuals that matter for algorithm comparison. The static map provides structural context, the VI/PI evolution pair shows how planning information develops, and the rollout pair shows resulting control behavior. Removing extra plots keeps attention on scale effects instead of presentation noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0260b2d3",
   "metadata": {},
   "source": [
    "## FrozenLake + Gymnasium interface essentials\n",
    "\n",
    "This section merges the Gymnasium API basics with the FrozenLake transfer example so the full planning workflow is in one place.\n",
    "Goal: clearly connect MDP notation (states, actions, rewards, transitions, terminal logic) to Gymnasium's `Env` interface and then reuse our VI/PI code on `FrozenLake-v1`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46176a11",
   "metadata": {},
   "source": [
    "### Gymnasium API essentials in the FrozenLake context\n",
    "\n",
    "Core interaction pattern:\n",
    "- `env = gym.make(...)`\n",
    "- `obs, info = env.reset(seed=...)`\n",
    "- `obs, reward, terminated, truncated, info = env.step(action)`\n",
    "\n",
    "Space objects (how Gymnasium represents valid states/actions):\n",
    "- `env.observation_space`: set of valid observations (FrozenLake: `Discrete(16)` for 4x4)\n",
    "- `env.action_space`: set of valid actions (FrozenLake: `Discrete(4)`)\n",
    "- `env.action_space.sample()`: random valid action (useful for sanity checks)\n",
    "\n",
    "Semantics of step outputs:\n",
    "- `obs`: next state id\n",
    "- `reward`: scalar transition reward\n",
    "- `terminated`: episode ended due to task terminal condition (goal/hole)\n",
    "- `truncated`: episode ended due to external cutoff (e.g., time limit)\n",
    "- `info`: auxiliary diagnostics\n",
    "\n",
    "MDP notation to Gymnasium mapping:\n",
    "- $A_t$ -> `action`\n",
    "- $S_{t+1}$ -> `obs`\n",
    "- $R_{t+1}$ -> `reward`\n",
    "- $\\mathcal{A}$ -> `env.action_space`\n",
    "- $\\mathcal{S}$ -> `env.observation_space`\n",
    "\n",
    "For tabular planning, toy-text environments expose the model as:\n",
    "- `env.unwrapped.P[s][a] = [(prob, s_next, reward, done), ...]`\n",
    "\n",
    "FrozenLake action indexing:\n",
    "- `0: Left`, `1: Down`, `2: Right`, `3: Up`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c600db09",
   "metadata": {},
   "outputs": [],
   "source": [
    "if gym is None:\n",
    "    print(\"Gymnasium is not installed. Install with: pip install gymnasium[toy-text]\")\n",
    "else:\n",
    "    fl_api_env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False)\n",
    "    obs, info = fl_api_env.reset(seed=SEED)\n",
    "\n",
    "    print(\"Gymnasium import successful.\")\n",
    "    print(f\"Observation space: {fl_api_env.observation_space}\")\n",
    "    print(f\"Action space: {fl_api_env.action_space}\")\n",
    "    print(f\"Reset -> obs={obs}, info={info}\")\n",
    "\n",
    "    sampled_action = int(fl_api_env.action_space.sample())\n",
    "    next_obs, reward, terminated, truncated, step_info = fl_api_env.step(sampled_action)\n",
    "    print(\n",
    "        f\"One step with sampled action {sampled_action}: \"\n",
    "        f\"obs={next_obs}, reward={reward}, terminated={terminated}, truncated={truncated}, info={step_info}\"\n",
    "    )\n",
    "\n",
    "    # Minimal random rollout to show done logic in practice\n",
    "    obs, _ = fl_api_env.reset(seed=SEED)\n",
    "    done = False\n",
    "    t = 0\n",
    "    max_steps = 30\n",
    "    while not done and t < max_steps:\n",
    "        a = int(fl_api_env.action_space.sample())\n",
    "        obs, reward, terminated, truncated, _ = fl_api_env.step(a)\n",
    "        done = terminated or truncated\n",
    "        t += 1\n",
    "    print(f\"Random rollout finished after {t} steps (terminated={terminated}, truncated={truncated}).\")\n",
    "    fl_api_env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc1c69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GYM_ACTION_SYMBOLS = {0: \"←\", 1: \"↓\", 2: \"→\", 3: \"↑\"}\n",
    "GYM_ACTION_VECTORS = {\n",
    "    0: (-0.35, 0.0),   # LEFT\n",
    "    1: (0.0, 0.35),    # DOWN\n",
    "    2: (0.35, 0.0),    # RIGHT\n",
    "    3: (0.0, -0.35),   # UP\n",
    "}\n",
    "\n",
    "\n",
    "def frozenlake_desc_to_grid(desc) -> np.ndarray:\n",
    "    # desc is byte array in Gymnasium toy-text envs\n",
    "    arr = np.array(\n",
    "        [[ch.decode(\"utf-8\") if isinstance(ch, (bytes, bytearray)) else str(ch) for ch in row] for row in desc],\n",
    "        dtype=\"<U1\",\n",
    "    )\n",
    "    return arr\n",
    "\n",
    "\n",
    "def mdp_from_frozenlake_env(env) -> TabularMDP:\n",
    "    p_raw = env.unwrapped.P\n",
    "    nS = env.observation_space.n\n",
    "    nA = env.action_space.n\n",
    "\n",
    "    desc = frozenlake_desc_to_grid(env.unwrapped.desc)\n",
    "    H, W = desc.shape\n",
    "\n",
    "    state_to_pos = {s: (s // W, s % W) for s in range(nS)}\n",
    "    pos_to_state = {(s // W, s % W): s for s in range(nS)}\n",
    "\n",
    "    terminal_states = set()\n",
    "    start_state = 0\n",
    "\n",
    "    for s, (r, c) in state_to_pos.items():\n",
    "        cell = desc[r, c]\n",
    "        if cell in (\"H\", \"G\"):\n",
    "            terminal_states.add(s)\n",
    "        if cell == \"S\":\n",
    "            start_state = s\n",
    "\n",
    "    P = {s: {a: [] for a in range(nA)} for s in range(nS)}\n",
    "    for s in range(nS):\n",
    "        for a in range(nA):\n",
    "            for prob, s_next, reward, done in p_raw[s][a]:\n",
    "                P[s][a].append((float(prob), int(s_next), float(reward), bool(done)))\n",
    "\n",
    "    return TabularMDP(\n",
    "        nS=nS,\n",
    "        nA=nA,\n",
    "        P=P,\n",
    "        state_to_pos=state_to_pos,\n",
    "        pos_to_state=pos_to_state,\n",
    "        terminal_states=terminal_states,\n",
    "        start_state=start_state,\n",
    "        grid_chars=desc,\n",
    "    )\n",
    "\n",
    "\n",
    "def render_frozenlake_map(mdp: TabularMDP, title: str = \"FrozenLake map\"):\n",
    "    code_map = {\"S\": 0, \"F\": 1, \"H\": 2, \"G\": 3}\n",
    "    arr = np.vectorize(code_map.get)(mdp.grid_chars)\n",
    "    cmap = ListedColormap([\"#A7D3F5\", \"#EAF6FF\", \"#F8B4B4\", \"#B7E4C7\"])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    ax.imshow(arr, cmap=cmap, vmin=0, vmax=3)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(range(arr.shape[1]))\n",
    "    ax.set_yticks(range(arr.shape[0]))\n",
    "    _draw_cell_grid(ax, arr.shape[0], arr.shape[1])\n",
    "\n",
    "    for r in range(arr.shape[0]):\n",
    "        for c in range(arr.shape[1]):\n",
    "            ax.text(c, r, mdp.grid_chars[r, c], ha=\"center\", va=\"center\", color=\"black\", fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def render_frozenlake_policy(mdp: TabularMDP, actions: np.ndarray, title: str = \"FrozenLake policy\"):\n",
    "    bg = np.zeros(mdp.grid_chars.shape)\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    ax.imshow(bg, cmap=ListedColormap([\"#f7f7f7\"]))\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(range(bg.shape[1]))\n",
    "    ax.set_yticks(range(bg.shape[0]))\n",
    "    _draw_cell_grid(ax, bg.shape[0], bg.shape[1])\n",
    "\n",
    "    for s, (r, c) in mdp.state_to_pos.items():\n",
    "        cell = mdp.grid_chars[r, c]\n",
    "        if cell in (\"H\", \"G\"):\n",
    "            txt = cell\n",
    "        elif cell == \"F\":\n",
    "            txt = \" \"\n",
    "        else:\n",
    "            txt = GYM_ACTION_SYMBOLS[int(actions[s])]\n",
    "        ax.text(c, r, txt, ha=\"center\", va=\"center\", color=\"black\", fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_frozenlake_policy_iteration_gif(\n",
    "    mdp: TabularMDP,\n",
    "    history: list,\n",
    "    gif_path: Path | None = None,\n",
    "    title_prefix: str = \"FrozenLake PI\",\n",
    "    save: bool = False,\n",
    "    fps: int = 2,\n",
    "    showcase: bool = True,\n",
    "):\n",
    "    if not history:\n",
    "        print(\"No history available; GIF was not created.\")\n",
    "        return\n",
    "\n",
    "    from matplotlib.animation import FuncAnimation\n",
    "    value_grids = [values_to_grid(mdp, h[\"V\"]) for h in history]\n",
    "\n",
    "    # policy per outer iteration\n",
    "    action_grids = []\n",
    "    for h in history:\n",
    "        a = np.argmax(h[\"policy\"], axis=1)\n",
    "        g = np.full(mdp.grid_chars.shape, -1, dtype=int)\n",
    "        for s, (r, c) in mdp.state_to_pos.items():\n",
    "            if mdp.grid_chars[r, c] not in (\"G\", \"H\"):\n",
    "                g[r, c] = a[s]\n",
    "        action_grids.append(g)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    im = ax.imshow(value_grids[0], cmap=\"viridis\")\n",
    "    txt = ax.set_title(f\"{title_prefix} | outer=1\")\n",
    "    ax.set_xticks(range(value_grids[0].shape[1]))\n",
    "    ax.set_yticks(range(value_grids[0].shape[0]))\n",
    "    _draw_cell_grid(ax, value_grids[0].shape[0], value_grids[0].shape[1])\n",
    "\n",
    "    q = None\n",
    "\n",
    "    def update(frame_idx):\n",
    "        nonlocal q\n",
    "        im.set_data(value_grids[frame_idx])\n",
    "\n",
    "        if q is not None:\n",
    "            q.remove()\n",
    "\n",
    "        X, Y, U, V = [], [], [], []\n",
    "        actions = action_grids[frame_idx]\n",
    "        for r in range(actions.shape[0]):\n",
    "            for c in range(actions.shape[1]):\n",
    "                if mdp.grid_chars[r, c] in (\"#\", \"G\", \"H\"):\n",
    "                    continue\n",
    "                a = actions[r, c]\n",
    "                if a < 0:\n",
    "                    continue\n",
    "                u, v = GYM_ACTION_VECTORS[a]\n",
    "                X.append(c); Y.append(r); U.append(u); V.append(v)\n",
    "\n",
    "        if X:\n",
    "            q = ax.quiver(X, Y, U, V, angles=\"xy\", scale_units=\"xy\", scale=1, color=\"white\", width=0.008)\n",
    "\n",
    "        outer = history[frame_idx][\"outer\"]\n",
    "        txt.set_text(f\"{title_prefix} | outer={outer}\")\n",
    "        return (im, txt)\n",
    "\n",
    "    ani = FuncAnimation(fig, update, frames=len(history), interval=550, blit=False)\n",
    "    _finalize_gif_animation(ani, fig, save=save, gif_path=gif_path, fps=fps, showcase=showcase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f2204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_vi_pi_on_frozenlake(map_name: str = \"4x4\", is_slippery: bool = False, gamma: float = 0.99, theta: float = 1e-10):\n",
    "    env = gym.make(\"FrozenLake-v1\", map_name=map_name, is_slippery=is_slippery)\n",
    "    mdp = mdp_from_frozenlake_env(env)\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    vi = value_iteration(mdp, gamma=gamma, theta=theta, return_history=True)\n",
    "    t_vi_ms = (time.perf_counter() - t0) * 1e3\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    pi = policy_iteration(mdp, gamma=gamma, eval_theta=theta, return_history=True)\n",
    "    t_pi_ms = (time.perf_counter() - t0) * 1e3\n",
    "\n",
    "    vi_actions = vi[\"greedy_actions\"]\n",
    "    pi_actions = np.argmax(pi[\"policy\"], axis=1)\n",
    "\n",
    "    agreement = compare_policies(vi_actions, pi_actions, mdp.terminal_states)\n",
    "    value_linf = float(np.max(np.abs(vi[\"V\"] - pi[\"V\"])))\n",
    "\n",
    "    return {\n",
    "        \"env\": env,\n",
    "        \"mdp\": mdp,\n",
    "        \"vi\": vi,\n",
    "        \"pi\": pi,\n",
    "        \"t_vi_ms\": t_vi_ms,\n",
    "        \"t_pi_ms\": t_pi_ms,\n",
    "        \"agreement\": agreement,\n",
    "        \"value_linf\": value_linf,\n",
    "    }\n",
    "\n",
    "\n",
    "if gym is None:\n",
    "    print(\"Skip: Gymnasium not available.\")\n",
    "else:\n",
    "    fl4 = run_vi_pi_on_frozenlake(map_name=\"4x4\", is_slippery=False, gamma=0.99, theta=1e-10)\n",
    "\n",
    "    mdp = fl4[\"mdp\"]\n",
    "    vi = fl4[\"vi\"]\n",
    "    pi = fl4[\"pi\"]\n",
    "\n",
    "    print(\"FrozenLake 4x4 | deterministic\")\n",
    "    print(f\"  nS={mdp.nS}, VI_sweeps={vi['sweeps']}, PI_outer={pi['outer_loops']}, PI_eval_total={sum(pi['eval_sweeps'])}\")\n",
    "    print(f\"  VI_ms={fl4['t_vi_ms']:.2f}, PI_ms={fl4['t_pi_ms']:.2f}\")\n",
    "    print(f\"  L_inf(VI-PI)={fl4['value_linf']:.3e}, policy_agreement={fl4['agreement']*100:.1f}%\")\n",
    "\n",
    "    render_frozenlake_map(mdp, title=\"FrozenLake 4x4 map (deterministic)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Embedded output asset(s) -->\n",
    "![cell-output](assets/web_outputs/session_02_mdp_dynamic_programming_cell082_out01.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00d60b1",
   "metadata": {},
   "source": [
    "The API demo above shows the runtime interaction contract (`reset`, `step`, spaces, and termination flags).\\n\n",
    "\\n\n",
    "Bridge to planning: now we switch from interaction to model-based DP by reading `env.unwrapped.P` and converting FrozenLake into our `TabularMDP` structure via `mdp_from_frozenlake_env(...)`. From that point onward, the same Value Iteration and Policy Iteration code used for custom GridWorld runs without algorithm changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec18e31",
   "metadata": {},
   "source": [
    "### FrozenLake 4x4 outputs: evolution and actual Gym rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c172f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if gym is None:\n",
    "    print(\"Skip: Gymnasium not available.\")\n",
    "elif imageio is None:\n",
    "    print(\"Skip: imageio is not available for rollout GIF export.\")\n",
    "else:\n",
    "    out_dir = Path(\"assets/web_outputs\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    mdp = fl4[\"mdp\"]\n",
    "    vi = fl4[\"vi\"]\n",
    "    pi = fl4[\"pi\"]\n",
    "\n",
    "    # 1) Evolution GIFs (VI vs PI)\n",
    "    gif_vi_evo = out_dir / \"session2_fl4_vi_value_evolution.gif\"\n",
    "    gif_pi_evo = out_dir / \"session2_fl4_pi_evolution.gif\"\n",
    "\n",
    "    save_vi_value_gif(\n",
    "        mdp,\n",
    "        vi[\"history\"],\n",
    "        gif_path=gif_vi_evo,\n",
    "        title_prefix=\"FrozenLake VI value evolution (4x4 det)\",\n",
    "        save=True,\n",
    "        showcase=False,\n",
    "    )\n",
    "    save_frozenlake_policy_iteration_gif(\n",
    "        mdp,\n",
    "        pi[\"history\"],\n",
    "        gif_path=gif_pi_evo,\n",
    "        title_prefix=\"FrozenLake PI evolution (4x4 det)\",\n",
    "        save=True,\n",
    "        showcase=False,\n",
    "    )\n",
    "\n",
    "    print(\"Evolution GIFs: VI vs PI\")\n",
    "    _display_media_row([gif_vi_evo, gif_pi_evo], width_px=430)\n",
    "\n",
    "    def save_frozenlake_gym_rollout_gif(\n",
    "        map_name: str,\n",
    "        is_slippery: bool,\n",
    "        actions: np.ndarray,\n",
    "        gif_path: Path,\n",
    "        seed: int = 7,\n",
    "        max_steps: int = 120,\n",
    "        duration: float = 0.22,\n",
    "    ):\n",
    "        env = gym.make(\"FrozenLake-v1\", map_name=map_name, is_slippery=is_slippery, render_mode=\"rgb_array\")\n",
    "        obs, _ = env.reset(seed=seed)\n",
    "        frames = [env.render()]\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            a = int(actions[int(obs)])\n",
    "            obs, _, terminated, truncated, _ = env.step(a)\n",
    "            frames.append(env.render())\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        gif_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        imageio.mimsave(gif_path, frames, duration=duration)\n",
    "        print(f\"Saved GIF: {gif_path}\")\n",
    "\n",
    "    # 2) Actual Gym rollout GIFs (VI vs PI)\n",
    "    gif_vi_roll = out_dir / \"session2_fl4_vi_gym_rollout.gif\"\n",
    "    gif_pi_roll = out_dir / \"session2_fl4_pi_gym_rollout.gif\"\n",
    "\n",
    "    save_frozenlake_gym_rollout_gif(\n",
    "        map_name=\"4x4\",\n",
    "        is_slippery=False,\n",
    "        actions=vi[\"greedy_actions\"],\n",
    "        gif_path=gif_vi_roll,\n",
    "        seed=7,\n",
    "    )\n",
    "    save_frozenlake_gym_rollout_gif(\n",
    "        map_name=\"4x4\",\n",
    "        is_slippery=False,\n",
    "        actions=np.argmax(pi[\"policy\"], axis=1),\n",
    "        gif_path=gif_pi_roll,\n",
    "        seed=7,\n",
    "    )\n",
    "\n",
    "    print(\"Actual Gym rollout GIFs: VI vs PI\")\n",
    "    _display_media_row([gif_vi_roll, gif_pi_roll], width_px=430)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Embedded output asset(s) -->\n",
    "![cell-output](assets/web_outputs/session2_fl4_vi_value_evolution.gif)\n",
    "![cell-output](assets/web_outputs/session2_fl4_pi_evolution.gif)\n",
    "![cell-output](assets/web_outputs/session2_fl4_vi_gym_rollout.gif)\n",
    "![cell-output](assets/web_outputs/session2_fl4_pi_gym_rollout.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe7589f",
   "metadata": {},
   "source": [
    "The first GIF pair shows how values and greedy actions evolve during planning in the tabular abstraction. The second pair switches to actual Gym-rendered execution, so you can verify that planned policies translate into concrete environment behavior. This side-by-side view helps separate model-level convergence from real rollout dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e97dcb",
   "metadata": {},
   "source": [
    "**FrozenLake GIF outputs** are saved under `assets/web_outputs/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8f95a4",
   "metadata": {},
   "source": [
    "## Gambler's Problem (tabular MDP example)\n",
    "\n",
    "Before moving to homework, we add one more classic DP example from Sutton & Barto: **Gambler's Problem**.\n",
    "\n",
    "Setup:\n",
    "- state $s \\in \\{0,1,\\dots,100\\}$ is current capital,\n",
    "- terminal states: $0$ (loss) and $100$ (goal),\n",
    "- action $a$ is the stake, with $a \\in \\{1,\\dots,\\min(s,100-s)\\}$,\n",
    "- with probability $p_h$ the coin is heads and capital increases by $a$,\n",
    "- with probability $1-p_h$ capital decreases by $a$,\n",
    "- reward is $1$ only when transitioning into state $100$.\n",
    "\n",
    "This is a pure planning problem, solved here with Value Iteration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002d8b80",
   "metadata": {},
   "source": [
    "### Extension - Implement Value Iteration for Gambler's Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689ea125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gambler_value_iteration(\n",
    "    p_heads: float = 0.4,\n",
    "    theta: float = 1e-12,\n",
    "    gamma: float = 1.0,\n",
    "    target: int = 100,\n",
    "):\n",
    "    # Aligned to session_02_gamblers_problem.ipynb logic (including tie handling)\n",
    "    V = np.zeros(target + 1, dtype=float)\n",
    "    policy = np.zeros(target + 1, dtype=int)\n",
    "\n",
    "    history = []\n",
    "    deltas = []\n",
    "    sweeps = 0\n",
    "\n",
    "    delta = theta\n",
    "    while delta >= theta:\n",
    "        delta = 0.0\n",
    "\n",
    "        for s in range(1, target):\n",
    "            v_old = V[s]\n",
    "\n",
    "            best_action_value = 0.0\n",
    "            best_action = 0\n",
    "\n",
    "            for a in range(1, min(s, target - s) + 1):\n",
    "                s_win = s + a\n",
    "                s_lose = max(0, s - a)\n",
    "\n",
    "                reward_win = 1.0 if s_win == target else 0.0\n",
    "\n",
    "                action_value = 0.0\n",
    "                action_value += p_heads * (reward_win + gamma * V[s_win])\n",
    "                action_value += (1.0 - p_heads) * gamma * V[s_lose]\n",
    "\n",
    "                # Same tie handling as the reference notebook\n",
    "                if action_value - theta > best_action_value:\n",
    "                    best_action_value = action_value\n",
    "                    best_action = a\n",
    "\n",
    "            V[s] = best_action_value\n",
    "            policy[s] = best_action\n",
    "            delta = max(delta, abs(v_old - best_action_value))\n",
    "\n",
    "        history.append(V.copy())\n",
    "        deltas.append(delta)\n",
    "        sweeps += 1\n",
    "\n",
    "    return {\n",
    "        \"V\": V,\n",
    "        \"policy\": policy,\n",
    "        \"history\": history,\n",
    "        \"deltas\": deltas,\n",
    "        \"sweeps\": sweeps,\n",
    "        \"p_heads\": p_heads,\n",
    "        \"theta\": theta,\n",
    "        \"gamma\": gamma,\n",
    "    }\n",
    "\n",
    "\n",
    "gambler_res = gambler_value_iteration(p_heads=0.4, theta=1e-12, gamma=1.0)\n",
    "print(\n",
    "    f\"Converged in {gambler_res['sweeps']} sweeps | \"\n",
    "    f\"final delta={gambler_res['deltas'][-1]:.3e}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7c1c50",
   "metadata": {},
   "source": [
    "### Extension - Visualize value function and optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d05777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gambler_values_and_policy(result: dict, title_prefix: str = \"Gambler\"):\n",
    "    V = result[\"V\"]\n",
    "    policy = result[\"policy\"]\n",
    "    target = len(V) - 1\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(np.arange(1, target), V[1:target], linewidth=2)\n",
    "    plt.xlabel(\"Capital\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.title(f\"{title_prefix}: Optimal value function\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(np.arange(1, target), policy[1:target])\n",
    "    plt.xlabel(\"Capital\")\n",
    "    plt.ylabel(\"Stake\")\n",
    "    plt.title(f\"{title_prefix}: Greedy stake policy\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_gambler_convergence(result: dict, title: str = \"Gambler Value Iteration convergence\"):\n",
    "    deltas = result[\"deltas\"]\n",
    "    plt.figure(figsize=(7, 3.5))\n",
    "    plt.plot(deltas)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel(\"Sweep\")\n",
    "    plt.ylabel(\"max update delta\")\n",
    "    plt.title(title)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_gambler_values_and_policy(gambler_res, title_prefix=\"Gambler (p_heads=0.4)\")\n",
    "plot_gambler_convergence(gambler_res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Embedded output asset(s) -->\n",
    "![cell-output](assets/web_outputs/session_02_mdp_dynamic_programming_cell092_out00.png)\n",
    "![cell-output](assets/web_outputs/session_02_mdp_dynamic_programming_cell092_out01.png)\n",
    "![cell-output](assets/web_outputs/session_02_mdp_dynamic_programming_cell092_out02.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3445671c",
   "metadata": {},
   "source": [
    "These Gambler plots reproduce the chapter-style behavior expected from the classic formulation. The value curve grows nonlinearly with capital because the chance of eventually hitting the goal is state dependent, and the optimal stake policy forms discrete regions rather than a smooth control law. This is a useful non-grid example of exact dynamic programming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a394707",
   "metadata": {},
   "source": [
    "### Extension - Value-evolution snapshots and GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8520f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gambler_snapshots(\n",
    "    result: dict,\n",
    "    k_list=None,\n",
    "    save: bool = False,\n",
    "    out_dir: Path | None = None,\n",
    "    file_tag: str = \"gambler\",\n",
    "):\n",
    "    hist = result[\"history\"]\n",
    "    if not hist:\n",
    "        return\n",
    "    if save and out_dir is None:\n",
    "        raise ValueError(\"out_dir must be provided when save=True\")\n",
    "\n",
    "    if k_list is None:\n",
    "        last = len(hist) - 1\n",
    "        mid = last // 2\n",
    "        k_list = sorted(set([0, mid, last]))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    for k in k_list:\n",
    "        ax.plot(np.arange(1, len(hist[k]) - 1), hist[k][1:-1], label=f\"sweep={k+1}\")\n",
    "    ax.set_xlabel(\"Capital\")\n",
    "    ax.set_ylabel(\"Value\")\n",
    "    ax.set_title(\"Gambler value evolution snapshots\")\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.legend()\n",
    "    _save_or_show_fig(\n",
    "        fig,\n",
    "        save=save,\n",
    "        path=(out_dir / f\"{file_tag}_value_snapshots.png\") if save else None,\n",
    "    )\n",
    "\n",
    "\n",
    "def save_gambler_value_gif(\n",
    "    result: dict,\n",
    "    gif_path: Path | None = None,\n",
    "    title_prefix: str = \"Gambler value evolution\",\n",
    "    save: bool = False,\n",
    "    fps: int = 8,\n",
    "):\n",
    "    hist = result[\"history\"]\n",
    "    if not hist:\n",
    "        print(\"No history available; GIF was not created.\")\n",
    "        return\n",
    "\n",
    "    x = np.arange(1, len(hist[0]) - 1)\n",
    "    fig, ax = plt.subplots(figsize=(9, 4))\n",
    "    line, = ax.plot([], [], linewidth=2)\n",
    "    ax.set_xlim(x[0], x[-1])\n",
    "    ax.set_ylim(0.0, max(float(np.max(v)) for v in hist) * 1.05)\n",
    "    ax.set_xlabel(\"Capital\")\n",
    "    ax.set_ylabel(\"Value\")\n",
    "    title = ax.set_title(f\"{title_prefix} | sweep=1\")\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "    def update(i):\n",
    "        y = hist[i][1:-1]\n",
    "        line.set_data(x, y)\n",
    "        title.set_text(f\"{title_prefix} | sweep={i+1}\")\n",
    "        return line, title\n",
    "\n",
    "    ani = FuncAnimation(fig, update, frames=len(hist), interval=160, blit=False)\n",
    "    _finalize_gif_animation(ani, fig, save=save, gif_path=gif_path, fps=fps)\n",
    "\n",
    "\n",
    "out_dir = Path(\"assets/web_outputs\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# plot_gambler_snapshots(gambler_res, save=True, out_dir=out_dir, file_tag=\"session2_gambler\")\n",
    "save_gambler_value_gif(\n",
    "    gambler_res,\n",
    "    out_dir / \"session2_gambler_value_evolution.gif\",\n",
    "    save=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Embedded output asset(s) -->\n",
    "![cell-output](assets/web_outputs/session2_gambler_value_evolution.gif)\n",
    "![cell-output](assets/web_outputs/session_02_mdp_dynamic_programming_cell095_out01.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d37f60",
   "metadata": {},
   "source": [
    "### Extension - Probability sensitivity experiments\n",
    "\n",
    "We compare $p_h \\in \\{0.25, 0.40, 0.55\\}$ and inspect how value and policy structure changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cf8b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_list = [0.25, 0.40, 0.55]\n",
    "results = {p: gambler_value_iteration(p_heads=p, theta=1e-12, gamma=1.0) for p in p_list}\n",
    "\n",
    "print(\"p_heads | sweeps | V(50) | suggested stake at s=50\")\n",
    "for p in p_list:\n",
    "    r = results[p]\n",
    "    print(f\"{p:6.2f} | {r['sweeps']:6d} | {r['V'][50]:6.3f} | {r['policy'][50]:24d}\")\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "for p in p_list:\n",
    "    r = results[p]\n",
    "    plt.plot(np.arange(1, 100), r[\"V\"][1:100], label=f\"p_heads={p:.2f}\")\n",
    "plt.xlabel(\"Capital\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Gambler value function sensitivity to coin bias\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "for p in p_list:\n",
    "    r = results[p]\n",
    "    plt.step(np.arange(1, 100), r[\"policy\"][1:100], where=\"mid\", label=f\"p_heads={p:.2f}\")\n",
    "plt.xlabel(\"Capital\")\n",
    "plt.ylabel(\"Stake\")\n",
    "plt.title(\"Gambler policy sensitivity to coin bias\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Embedded output asset(s) -->\n",
    "![cell-output](assets/web_outputs/session_02_mdp_dynamic_programming_cell097_out01.png)\n",
    "![cell-output](assets/web_outputs/session_02_mdp_dynamic_programming_cell097_out02.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d07e39",
   "metadata": {},
   "source": [
    "Changing coin bias directly modifies the transition model, so both value scale and optimal stakes should move in a structured way. As heads probability increases, aggressive stakes become attractive at more capitals; when it decreases, conservative behavior dominates. This sensitivity is the key teaching point of the Gambler example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df253dbf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
