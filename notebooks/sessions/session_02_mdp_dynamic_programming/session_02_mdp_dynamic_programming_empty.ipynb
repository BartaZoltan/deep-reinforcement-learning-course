{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Logo](https://github.com/BartaZoltan/deep-reinforcement-learning-course/blob/main/website/assets/logo.png?raw=1)\n\nMade by **Zoltán Barta**\n\n[<img src=\"https://colab.research.google.com/assets/colab-badge.svg\">](https://colab.research.google.com/github/BartaZoltan/deep-reinforcement-learning-course/blob/main/notebooks/sessions/session_02_mdp_dynamic_programming/session_02_mdp_dynamic_programming_empty.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3926574f",
      "metadata": {},
      "source": [
        "# 2. alkalom – Markov döntési folyamatok és dinamikus programozás\n",
        "\n",
        "Ebben a gyakorlati foglalkozásban megismerkedünk a **Markov döntési folyamatokkal (MDP)**, a dinamikus programozásban használt **értékfüggvényekkel és Bellman-egyenletekkel**, valamint két klasszikus algoritmussal: az **értékiterációval** és a **policy–iterációval**. A célunk, hogy a diákok kézzel is implementálják ezeket az eljárásokat és kísérletezzenek a hiperparaméterekkel (γ, θ), hogy érzékeljék a konvergencia sebességére és az eredő politikára gyakorolt hatásukat.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e075b69",
      "metadata": {},
      "source": [
        "## Markov döntési folyamat (MDP) definíció\n",
        "\n",
        "Egy **Markov döntési folyamatot** az alábbi összetevők határoznak meg:\n",
        "\n",
        "* **Állapotok halmaza**: \\(S\\). Az MDP-ben az állapotok ugyanúgy reprezentálhatók, mint a keresési problémákban.\n",
        "* **Akciók halmaza**: \\(A\\). Minden állapotból elérhető akciók halmaza.\n",
        "* **Kezdő állapot** és **terminális állapotok** (ha léteznek).\n",
        "* **Diszkontráta** \\(\\gamma\\). Ez 0 és 1 közé eső szám, amely meghatározza, mennyire értékeljük a jövőbeli jutalmakat.\n",
        "* **Átmeneti függvény** \\(T(s,a,s')\\), amely megadja annak valószínűségét, hogy az **s** állapotból **a** akció hatására az **s'** állapotba jutunk【448643135645475†L95-L115】.\n",
        "* **Jutalom függvény** \\(R(s,a,s')\\), ami általában kismértékű, \"életben maradást jutalmazó\" jutalmat ad minden lépésért, illetve nagy jutalmat a terminális állapot eléréséért【448643135645475†L109-L115】.\n",
        "\n",
        "Az MDP célja, hogy olyan **politika** \\(\\pi: S \\to A\\) függvényt találjunk, amely maximalizálja a hosszú távú jutalmat (az úgynevezett visszatérő értéket).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8e02e9d",
      "metadata": {},
      "source": [
        "## Értékfüggvények és Bellman-egyenlet\n",
        "\n",
        "Az MDP optimalitásának vizsgálatához bevezetjük az **állapotérték-függvényt** \\(V(s)\\), amely megadja az **s** állapotban várható visszatérő jutalmat, ha az optimális politikát követjük. A Bellman-optimalitási egyenlet szerint az optimális értékfüggvény az aktuális jutalom és a diszkontált jövőbeli érték összege:\n",
        "\n",
        "$$\\displaystyle V^{\\*}(s)=\\max_{a} \\Bigl[R(s,a)+\\gamma \\sum_{s'} T(s,a,s') V^{\\*}(s')\\Bigr].$$\n",
        "\n",
        "Ez a formula kifejezi, hogy a legjobb cselekvést úgy választjuk, hogy maximalizáljuk az azonnali jutalom és a jövőbeli (diszkontált) jutalmak összegét【586787408302835†L34-L49】.\n",
        "\n",
        "A későbbi algoritmusok ezen egyenlet numerikus megoldására épülnek.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5842341",
      "metadata": {},
      "source": [
        "## Policy értékelés és policy iteráció\n",
        "\n",
        "Egy adott \\(\\pi\\) politika **értékfüggvénye** \\(V^{\\pi}(s)\\) kiszámítható a Bellman várható érték egyenlettel:\n",
        "\n",
        "$$V^{\\pi}(s)=R(s,\\pi(s))+\\gamma \\sum_{s'} T(s,\\pi(s),s') V^{\\pi}(s').$$\n",
        "\n",
        "Ezt az egyenletet iteratív módon közelítjük: kezdetben \\(V_0(s)=0\\), majd folyamatosan frissítjük a \\(V\\) értékeket a fenti formula alapján, amíg a változás egy **θ** határ alá nem esik. Ez az eljárás a **policy értékelés** része.\n",
        "\n",
        "A **policy iteráció** két lépésből áll: először a jelenlegi politikához tartozó értékfüggvény meghatározása (**policy értékelés**), majd **policy javítás**, ahol minden állapotban azt az akciót választjuk, amely maximális értéket eredményez:\n",
        "\n",
        "$$\\pi'(s) = \\underset{a}{\\arg\\max}\\, \\Bigl[R(s,a) + \\gamma \\sum_{s'} T(s,a,s') V^{\\pi}(s')\\Bigr].$$\n",
        "\n",
        "A folyamat addig ismétlődik, amíg a politika már nem változik【586787408302835†L60-L74】.\n",
        "\n",
        "### Értékiteráció\n",
        "\n",
        "Az **értékiteráció** közvetlenül az optimális értékfüggvényre konvergál: minden iterációban az összes állapot értékét frissítjük a Bellman-optimalitási egyenlet alapján:\n",
        "\n",
        "$$V_{k+1}(s) \\leftarrow \\max_{a} \\sum_{s'} T(s,a,s') [ R(s,a,s') + \\gamma V_k(s') ].$$\n",
        "\n",
        "Amikor az értékfüggvény már nem változik (\\(V_{k+1}\\approx V_k\\)), az így kapott értékek kielégítik a Bellman-egyenletet【584237640575333†L107-L125】. A politika ezután a fenti \\(V\\) függvényből származtatható.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05191b83",
      "metadata": {},
      "source": [
        "## Értékiteráció és policy iteráció összehasonlítása\n",
        "\n",
        "Az értékiteráció minden iterációban az összes állapothoz tartozó értéket frissíti, majd a politika csak a végén származtatható. Ez több iterációt igényelhet, különösen, ha nagy az állapottér. A **policy iteráció** ezzel szemben váltakozva végzi az értékelést és javítást, ami gyakran kevesebb iterációt eredményez, bár az értékelő lépés során lineáris egyenletrendszert kell megoldanunk vagy iteratív becslést használnunk【103220485399117†L92-L121】. A gyakorlatban a választás a probléma méretétől és a rendelkezésre álló számítási kapacitástól függ.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6086d168",
      "metadata": {},
      "source": [
        "## Felépítés és feladatok\n",
        "\n",
        "Az alábbiakban részletesen végigmegyünk a dinamikus programozási megközelítések gyakorlati megvalósításán:\n",
        "\n",
        "1. **Gridworld környezet definiálása**: létrehozunk egy egyszerű rácsos világot, amely determinisztikus vagy stochasztikus átmenetekkel rendelkezhet.\n",
        "2. **Policy értékelés implementálása**: iteratív módszer a \\(V^\\pi(s)\\) kiszámítására.\n",
        "3. **Policy iteráció implementálása**: kombináljuk a politika értékelést és javítást.\n",
        "4. **Értékiteráció implementálása**: közvetlenül approximáljuk az optimális értékfüggvényt.\n",
        "5. **Kísérletek γ és θ paraméterekkel**: vizsgáljuk meg, hogyan hat a diszkontráta és a konvergencia-küszöb a sebességre és az eredményre.\n",
        "6. **FrozenLake-szerű környezet**: opcionálisan létrehozzuk a jeges tó problémát, ahol az átmenetek bizonytalanok (csúszós felület).\n",
        "7. **Kiegészítő feladatok**: változtassunk a jutalmakon, bővítsük a rács méretét, és értékeljük a két algoritmus hatékonyságát.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d0e2893",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from typing import Dict, Tuple, List\n",
        "\n",
        "# Általános hasznossági függvény a politika kiírására\n",
        "def print_policy(policy: Dict[Tuple[int, int], str], grid_size: Tuple[int, int]):\n",
        "    \"\"\"Politika szép kiírása rácsos környezethez.\n",
        "\n",
        "    Args:\n",
        "        policy: szótár, amely az (s) állapotokat betűk (U/D/L/R) alapján rendezi.\n",
        "        grid_size: a rács (sorok, oszlopok) mérete.\n",
        "    \"\"\"\n",
        "    rows, cols = grid_size\n",
        "    for r in range(rows):\n",
        "        line = ''\n",
        "        for c in range(cols):\n",
        "            if (r, c) in policy:\n",
        "                line += policy[(r, c)] + ' '\n",
        "            else:\n",
        "                line += 'T '  # Terminális vagy fal\n",
        "        print(line)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "340dfa5d",
      "metadata": {},
      "source": [
        "### Gridworld környezet implementációja\n",
        "\n",
        "A következő Python osztály egy **determinista** Gridworld környezetet valósít meg. A rács kockái között mozoghatunk fel (U), le (D), balra (L) vagy jobbra (R). A szegélyek falak, a terminális állapot elérése után a jutalom 0 és további átmenetek nem történnek. A standard beállításban minden nem terminális átmenet –1 jutalmat ad, a célállapot viszont 0 jutalmat. Ez hasonló a Sutton-könyv 4×4-es gridworld példájához, de paraméterezhető.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e782236",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Gridworld:\n",
        "    def __init__(self, rows: int, cols: int, terminals: List[Tuple[int, int]],\n",
        "                 slip_prob: float = 0.0, default_reward: float = -1.0):\n",
        "        \"\"\"Egyszerű gridworld rács.\n",
        "\n",
        "        Args:\n",
        "            rows: sorok száma.\n",
        "            cols: oszlopok száma.\n",
        "            terminals: terminális állapotok listája (sor, oszlop) formában.\n",
        "            slip_prob: csúszás valószínűsége (0: determinista, >0: stochasztikus átmenet).\n",
        "            default_reward: jutalom minden lépésnél.\n",
        "        \"\"\"\n",
        "        self.rows = rows\n",
        "        self.cols = cols\n",
        "        self.terminals = terminals\n",
        "        self.slip_prob = slip_prob\n",
        "        self.default_reward = default_reward\n",
        "        self.actions = ['U', 'D', 'L', 'R']\n",
        "\n",
        "    def in_bounds(self, state: Tuple[int, int]) -> bool:\n",
        "        r, c = state\n",
        "        return 0 <= r < self.rows and 0 <= c < self.cols\n",
        "\n",
        "    def step(self, state: Tuple[int, int], action: str) -> Tuple[Tuple[int, int], float]:\n",
        "        \"\"\"Egy lépés végrehajtása. Ha slip_prob > 0, akkor a kívánt akció mellett random\n",
        "        másik akció is előfordulhat. A terminális állapotban maradunk.\n",
        "        \"\"\"\n",
        "        if state in self.terminals:\n",
        "            return state, 0.0\n",
        "        actual_action = action\n",
        "        if self.slip_prob > 0 and random.random() < self.slip_prob:\n",
        "            actual_action = random.choice([a for a in self.actions if a != action])\n",
        "        r, c = state\n",
        "        if actual_action == 'U':\n",
        "            new_state = (max(r - 1, 0), c)\n",
        "        elif actual_action == 'D':\n",
        "            new_state = (min(r + 1, self.rows - 1), c)\n",
        "        elif actual_action == 'L':\n",
        "            new_state = (r, max(c - 1, 0))\n",
        "        elif actual_action == 'R':\n",
        "            new_state = (r, min(c + 1, self.cols - 1))\n",
        "        else:\n",
        "            new_state = state\n",
        "        reward = 0.0 if new_state in self.terminals else self.default_reward\n",
        "        return new_state, reward\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a7f8be3",
      "metadata": {},
      "source": [
        "### Policy értékelés implementációja\n",
        "\n",
        "A policy értékelés az aktuális politika értékfüggvényének közelítése. Paraméterezhető a diszkontráta (γ) és a konvergencia küszöb (θ). Az iteráció addig fut, amíg a változások mértéke minden állapotban kisebb, mint θ.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0db8a8d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def policy_evaluation(policy: Dict[Tuple[int, int], str], env: Gridworld, gamma: float = 1.0, theta: float = 1e-4) -> Dict[Tuple[int, int], float]:\n",
        "    \"\"\"Iteratív policy értékelés.\n",
        "    Args:\n",
        "        policy: dikt, amely az állapotokat akcióhoz rendeli.\n",
        "        env: Gridworld környezet.\n",
        "        gamma: diszkontráta.\n",
        "        theta: konvergencia küszöb.\n",
        "    Returns:\n",
        "        V: dict, minden állapot értéke.\n",
        "    \"\"\"\n",
        "    V = { (r, c): 0.0 for r in range(env.rows) for c in range(env.cols) }\n",
        "    for t in env.terminals:\n",
        "        V[t] = 0.0\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for r in range(env.rows):\n",
        "            for c in range(env.cols):\n",
        "                state = (r, c)\n",
        "                if state in env.terminals:\n",
        "                    continue\n",
        "                a = policy[state]\n",
        "                next_state, reward = env.step(state, a)\n",
        "                v_new = reward + gamma * V[next_state]\n",
        "                delta = max(delta, abs(v_new - V[state]))\n",
        "                V[state] = v_new\n",
        "        if delta < theta:\n",
        "            break\n",
        "    return V\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48818965",
      "metadata": {},
      "source": [
        "### Policy iteráció implementációja\n",
        "\n",
        "A következő függvény a determinisztikus policy iteráció algoritmust valósítja meg. Kezdetben minden állapotban véletlen akciót választunk, majd felváltva végzünk policy értékelést és javítást, amíg a politika nem stabil.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9327be1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def policy_iteration(env: Gridworld, gamma: float = 1.0, theta: float = 1e-4) -> Tuple[Dict[Tuple[int, int], float], Dict[Tuple[int, int], str], int]:\n",
        "    policy = {}\n",
        "    for r in range(env.rows):\n",
        "        for c in range(env.cols):\n",
        "            if (r, c) not in env.terminals:\n",
        "                policy[(r, c)] = random.choice(env.actions)\n",
        "    iteration = 0\n",
        "    while True:\n",
        "        iteration += 1\n",
        "        V = policy_evaluation(policy, env, gamma, theta)\n",
        "        stable = True\n",
        "        for r in range(env.rows):\n",
        "            for c in range(env.cols):\n",
        "                state = (r, c)\n",
        "                if state in env.terminals:\n",
        "                    continue\n",
        "                old_action = policy[state]\n",
        "                action_values = {}\n",
        "                for a in env.actions:\n",
        "                    next_state, reward = env.step(state, a)\n",
        "                    action_values[a] = reward + gamma * V[next_state]\n",
        "                best_action = max(action_values, key=action_values.get)\n",
        "                policy[state] = best_action\n",
        "                if best_action != old_action:\n",
        "                    stable = False\n",
        "        if stable:\n",
        "            break\n",
        "    return V, policy, iteration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1ac90ae",
      "metadata": {},
      "source": [
        "### Értékiteráció implementációja\n",
        "\n",
        "Az értékiteráció minden iterációban frissíti az összes állapot értékét a Bellman-optimalitási operátorral, majd származtatja a politikát. Megfigyelhetjük, hogy a konvergencia gyakran lassabb, de egyszerűbb implementációról van szó.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b170c32a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def value_iteration(env: Gridworld, gamma: float = 1.0, theta: float = 1e-4) -> Tuple[Dict[Tuple[int, int], float], Dict[Tuple[int, int], str], int]:\n",
        "    V = { (r, c): 0.0 for r in range(env.rows) for c in range(env.cols) }\n",
        "    for t in env.terminals:\n",
        "        V[t] = 0.0\n",
        "    iteration = 0\n",
        "    while True:\n",
        "        iteration += 1\n",
        "        delta = 0\n",
        "        for r in range(env.rows):\n",
        "            for c in range(env.cols):\n",
        "                state = (r, c)\n",
        "                if state in env.terminals:\n",
        "                    continue\n",
        "                action_values = []\n",
        "                for a in env.actions:\n",
        "                    next_state, reward = env.step(state, a)\n",
        "                    action_values.append(reward + gamma * V[next_state])\n",
        "                v_new = max(action_values)\n",
        "                delta = max(delta, abs(v_new - V[state]))\n",
        "                V[state] = v_new\n",
        "        if delta < theta:\n",
        "            break\n",
        "    policy = {}\n",
        "    for r in range(env.rows):\n",
        "        for c in range(env.cols):\n",
        "            state = (r, c)\n",
        "            if state in env.terminals:\n",
        "                continue\n",
        "            action_values = {}\n",
        "            for a in env.actions:\n",
        "                next_state, reward = env.step(state, a)\n",
        "                action_values[a] = reward + gamma * V[next_state]\n",
        "            policy[state] = max(action_values, key=action_values.get)\n",
        "    return V, policy, iteration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "886eeb8c",
      "metadata": {},
      "source": [
        "## Példa futtatások a Gridworld környezeten\n",
        "\n",
        "Most futtassuk le mindkét algoritmust egy 4×4-es rácson (terminális állapot a jobb alsó sarokban).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "febd51a8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Rács paraméterei\n",
        "rows, cols = 4, 4\n",
        "terminal_states = [(rows - 1, cols - 1)]  # cél a jobb alsó sarok\n",
        "env = Gridworld(rows, cols, terminal_states)\n",
        "\n",
        "# Policy iteráció futtatása\n",
        "V_pi, policy_pi, it_pi = policy_iteration(env, gamma=1.0, theta=1e-4)\n",
        "print(f'Policy iteráció konvergált {it_pi} iteráció után.')\n",
        "print('Kapott politika:')\n",
        "print_policy(policy_pi, (rows, cols))\n",
        "\n",
        "# Értékiteráció futtatása\n",
        "V_vi, policy_vi, it_vi = value_iteration(env, gamma=1.0, theta=1e-4)\n",
        "print(f'\n",
        "Értékiteráció konvergált {it_vi} iteráció után.')\n",
        "print('Kapott politika:')\n",
        "print_policy(policy_vi, (rows, cols))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03a9524a",
      "metadata": {},
      "source": [
        "## Kísérletek: a diszkontráta (γ) és a konvergencia-küszöb (θ) hatása\n",
        "\n",
        "A következő kódrész futtatja a policy iterációt és az értékiterációt különböző γ és θ paraméterek mellett, és visszatérési értékeik alapján összehasonlítja a konvergencia sebességét. Állítsuk be a paramétereket többféle módon, és nézzük meg, hány iteráció szükséges a konvergenciához.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96016fd1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def experiment_parameters(gammas: List[float], thetas: List[float]):\n",
        "    rows, cols = 4, 4\n",
        "    terminal_states = [(rows - 1, cols - 1)]\n",
        "    for gamma in gammas:\n",
        "        for theta in thetas:\n",
        "            env = Gridworld(rows, cols, terminal_states)\n",
        "            _, _, it_pi = policy_iteration(env, gamma=gamma, theta=theta)\n",
        "            _, _, it_vi = value_iteration(env, gamma=gamma, theta=theta)\n",
        "            print(f'γ={gamma}, θ={theta}: Policy iteráció {it_pi} iteráció, Értékiteráció {it_vi} iteráció')\n",
        "\n",
        "# Példa hívás\n",
        "gammas = [0.5, 0.8, 0.9, 1.0]\n",
        "thetas = [1e-2, 1e-3, 1e-4]\n",
        "experiment_parameters(gammas, thetas)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75baa747",
      "metadata": {},
      "source": [
        "## FrozenLake-szerű környezet (opcionális)\n",
        "\n",
        "A FrozenLake probléma egy jeges tó, ahol a mezők csúszósak. Mozgáskor csúszhatunk a szándékolt irányból egy másik irányba is. A cél az, hogy elérjük a célmezőt anélkül, hogy beleesnénk a lyukakba. A következő osztályban a `slip_prob` paraméterrel állíthatjuk be a csúszás valószínűségét, a terminális mezők pedig a lyukak és a cél.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d4126d0",
      "metadata": {},
      "source": [
        "### Feladatok a hallgatóknak\n",
        "\n",
        "1. **Jutalom módosítása:** Állítsák be a `default_reward` értékét −0.1-re, 0-ra vagy más értékre, és figyeljék meg, hogyan változik az optimális politika.\n",
        "2. **Nagyobb rács:** Bővítsék a rács méretét 5×5-re vagy 6×6-ra, és mérjék meg, hányszor több iterációra van szükség a konvergenciához.\n",
        "3. **Stochasztikus környezet:** Állítsák be a `slip_prob` paramétert 0.1 vagy 0.2 értékre, és figyeljék meg, hogyan változik a politika. Figyeljenek arra, hogy ilyenkor a `step` függvény többféle kimenetet adhat, ezért az `action_values` kiszámításakor a várható értékeket kell felhasználni.\n",
        "4. **FrozenLake-szerű környezet:** Hozzanak létre egy olyan rácsot, ahol bizonyos mezők lyukként (terminális állapotok negatív jutalommal) viselkednek, és a célállapot pozitív jutalmat ad. Implementálják az értékiterációt erre az MDP-re.\n",
        "5. **Paraméterérzékenység:** Változtassák a γ értékét 0,9-ről 0,5-re, majd 0,99-re, és magyarázzák el, hogyan módosul a politika.\n",
        "6. **Kreatív feladat:** Kísérletezzenek saját jutalomstruktúrákkal vagy akadályokkal (falak), és keressék meg az optimális politikát.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de165014",
      "metadata": {},
      "source": [
        "## Összefoglalás\n",
        "\n",
        "Ebben a foglalkozásban részletesen megismertük a Markov döntési folyamatokat, a Bellman-egyenleteket, valamint két fontos dinamikus programozási algoritmust: az értékiterációt és a policy iterációt. Implementáltuk őket egy egyszerű Gridworld környezetben, és kísérleteket végeztünk különböző diszkontrátákkal és konvergencia-küszöbökkel. A hallgatói feladatok célja, hogy ezeknek az algoritmusoknak a működését mélyebben megértsék és tovább bővítsék saját környezeteikben.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
