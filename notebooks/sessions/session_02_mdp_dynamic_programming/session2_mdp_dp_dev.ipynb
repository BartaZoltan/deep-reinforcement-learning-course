{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e768595b",
      "metadata": {},
      "source": [
        "![Logo](https://raw.githubusercontent.com/BartaZoltan/deep-reinforcement-learning-course/main/notebooks/shared_assets/logo.png)\n",
        "\n",
        "\n",
        "**Developers:** Domonkos Nagy, Balazs Nagy, Zoltan Barta  \n",
        "**Date:** 2026-02-23  \n",
        "**Version:** 2025-26/2\n",
        "\n",
        "[<img src=\"https://colab.research.google.com/assets/colab-badge.svg\">](https://colab.research.google.com/github/BartaZoltan/deep-reinforcement-learning-course/blob/main/notebooks/sessions/session_02_mdp_dynamic_programming/session2_mdp_dp_dev.ipynb)\n",
        "\n",
        "# Practice 2: MDP Dynamic Programming\n",
        "\n",
        "## Summary\n",
        "\n",
        "This notebook introduces **tabular Markov Decision Processes (MDPs)** and **Dynamic Programming** control methods in a practical, coding-first format.\n",
        "\n",
        "Content outline:\n",
        "- agent-environment interface and reusable GridWorld MDP design,\n",
        "- Value Iteration implementation and convergence analysis,\n",
        "- Policy Iteration implementation and behavior analysis,\n",
        "- larger-map stress tests for scalability and robustness,\n",
        "- transfer to Gymnasium FrozenLake,\n",
        "- optional extension: Gambler's Problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa0568b4",
      "metadata": {},
      "source": [
        "## Introduction\n",
        "\n",
        "This practical session develops tabular Dynamic Programming from first principles and then tests it across increasingly realistic environments. Following Sutton and Barto (Ch. 3-4), we start with a fully transparent custom GridWorld to formalize the agent-environment interface, state and action spaces, transition-reward dynamics, and the Markov property. On top of this model, we implement and analyze the two classical planning algorithms for finite MDPs: Value Iteration and Policy Iteration. The focus is not only on obtaining a final policy, but on understanding algorithmic behavior through convergence curves, value-function evolution, policy snapshots, sensitivity studies (e.g., $\\gamma$, stopping thresholds, stochasticity), and rollout-based validation. After baseline experiments on small maps, we stress-test both methods on larger layouts to study scalability, robustness, and computational trade-offs. We then transfer the same workflow to Gymnasium FrozenLake to connect custom tabular implementations with standard RL tooling and interface conventions (`reset`, `step`, `terminated`, `truncated`, and transition model access in toy-text environments). As an optional extension, we include Gambler’s Problem to broaden intuition beyond GridWorld and show how the same DP ideas apply to a different tabular structure. By the end of the notebook, you should be able to build a clean tabular MDP, implement both DP control methods correctly, interpret their dynamics with meaningful diagnostics, and move confidently between custom and Gymnasium-based environments.\n",
        "\n",
        "This notebook follows Chapter 3-4 of Sutton & Barto {cite}`sutton2018`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "350bcda8",
      "metadata": {},
      "source": [
        "## Markdov Decision Process, Agent-Environment interface, and GridWorld\n",
        "\n",
        "\n",
        "In reinforcement learning, the interaction loop is:\n",
        "- agent observes state $S_t$\n",
        "- agent takes action $A_t$\n",
        "- environment returns reward $R_{t+1}$ and next state $S_{t+1}$\n",
        "\n",
        "The **MDP assumption** is that the future depends on the current state-action pair, not the full history:\n",
        "$$\n",
        "P(S_{t+1}, R_{t+1} \\mid S_t, A_t)\n",
        "$$\n",
        "\n",
        "For planning with Dynamic Programming later, we need an explicit model:\n",
        "- state space $\\mathcal{S}$\n",
        "- action space $\\mathcal{A}$\n",
        "- transition-reward dynamics $p(s', r \\mid s, a)$\n",
        "- discount factor $\\gamma$\n",
        "\n",
        "So in this part we build a GridWorld that supports both:\n",
        "- a simulator-like API (`reset`, `step`) for rollouts,\n",
        "- and a tabular model (`P[s][a]`) for Value/Policy Iteration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "115110b4",
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "SEED = 42\n",
        "rng = np.random.default_rng(SEED)\n",
        "\n",
        "# Action encoding for consistency across all later parts\n",
        "UP, RIGHT, DOWN, LEFT = 0, 1, 2, 3\n",
        "ACTIONS = {\n",
        "    UP: (-1, 0),\n",
        "    RIGHT: (0, 1),\n",
        "    DOWN: (1, 0),\n",
        "    LEFT: (0, -1),\n",
        "}\n",
        "ACTION_SYMBOLS = {UP: \"↑\", RIGHT: \"→\", DOWN: \"↓\", LEFT: \"←\"}\n",
        "Transition = Tuple[float, int, float, bool]  # (prob, next_state, reward, done)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "788c3d1d",
      "metadata": {},
      "source": [
        "### Task 1 \n",
        "**Build a reusable GridWorld MDP (10 min)**\n",
        "\n",
        "We use a character map:\n",
        "- `S`: start\n",
        "- `.`: free cell\n",
        "- `#`: wall (blocked)\n",
        "- `G`: terminal goal\n",
        "- `H`: terminal hole\n",
        "\n",
        "Design choice (important for later DP):\n",
        "- terminal states are absorbing in the model (`P[s_terminal][a] -> s_terminal`),\n",
        "- walls are not states,\n",
        "- invalid moves (off-grid / into wall) keep the agent in place.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80b54202",
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class GridWorldConfig:\n",
        "    char_map: List[str]\n",
        "    step_reward: float = -1.0\n",
        "    goal_reward: float = 10.0\n",
        "    hole_reward: float = -10.0\n",
        "    slip_prob: float = 0.0  # 0.0 = deterministic dynamics\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TabularMDP:\n",
        "    nS: int\n",
        "    nA: int\n",
        "    P: Dict[int, Dict[int, List[Transition]]]\n",
        "    state_to_pos: Dict[int, Tuple[int, int]]\n",
        "    pos_to_state: Dict[Tuple[int, int], int]\n",
        "    terminal_states: set\n",
        "    start_state: int\n",
        "    grid_chars: np.ndarray\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab386673",
      "metadata": {},
      "source": [
        "### GridWorld implementation details\n",
        "\n",
        "This class exposes:\n",
        "- `reset()` and `step(action)` for interaction,\n",
        "- `as_mdp()` that returns a full tabular model for planning algorithms.\n",
        "\n",
        "This keeps one source of truth for dynamics and avoids mismatch bugs later.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f68cdbd",
      "metadata": {},
      "outputs": [],
      "source": [
        "class GridWorldEnv:\n",
        "    def __init__(self, cfg: GridWorldConfig):\n",
        "        self.cfg = cfg\n",
        "        self.grid = np.array([list(row) for row in cfg.char_map], dtype=\"<U1\")\n",
        "        self.H, self.W = self.grid.shape\n",
        "\n",
        "        self.pos_to_state: Dict[Tuple[int, int], int] = {}\n",
        "        self.state_to_pos: Dict[int, Tuple[int, int]] = {}\n",
        "\n",
        "        s_idx = 0\n",
        "        for r in range(self.H):\n",
        "            for c in range(self.W):\n",
        "                if self.grid[r, c] != \"#\":\n",
        "                    self.pos_to_state[(r, c)] = s_idx\n",
        "                    self.state_to_pos[s_idx] = (r, c)\n",
        "                    s_idx += 1\n",
        "\n",
        "        self.nS = s_idx\n",
        "        self.nA = 4\n",
        "\n",
        "        self.terminal_states = set()\n",
        "        self.start_state: Optional[int] = None\n",
        "\n",
        "        for (r, c), s in self.pos_to_state.items():\n",
        "            cell = self.grid[r, c]\n",
        "            if cell in (\"G\", \"H\"):\n",
        "                self.terminal_states.add(s)\n",
        "            if cell == \"S\":\n",
        "                if self.start_state is not None:\n",
        "                    raise ValueError(\"Map must contain exactly one start cell 'S'.\")\n",
        "                self.start_state = s\n",
        "\n",
        "        if self.start_state is None:\n",
        "            raise ValueError(\"Map must contain one start cell 'S'.\")\n",
        "\n",
        "        self._P = self._build_transition_model()\n",
        "        self._state = self.start_state\n",
        "\n",
        "    def _move(self, r: int, c: int, a: int) -> Tuple[int, int]:\n",
        "        dr, dc = ACTIONS[a]\n",
        "        nr, nc = r + dr, c + dc\n",
        "        if nr < 0 or nr >= self.H or nc < 0 or nc >= self.W or self.grid[nr, nc] == \"#\":\n",
        "            return r, c\n",
        "        return nr, nc\n",
        "\n",
        "    def _cell_reward_done(self, r: int, c: int) -> Tuple[float, bool]:\n",
        "        cell = self.grid[r, c]\n",
        "        if cell == \"G\":\n",
        "            return self.cfg.goal_reward, True\n",
        "        if cell == \"H\":\n",
        "            return self.cfg.hole_reward, True\n",
        "        return self.cfg.step_reward, False\n",
        "\n",
        "    def _build_transition_model(self) -> Dict[int, Dict[int, List[Transition]]]:\n",
        "        left_of = {UP: LEFT, RIGHT: UP, DOWN: RIGHT, LEFT: DOWN}\n",
        "        right_of = {UP: RIGHT, RIGHT: DOWN, DOWN: LEFT, LEFT: UP}\n",
        "\n",
        "        P: Dict[int, Dict[int, List[Transition]]] = {\n",
        "            s: {a: [] for a in range(self.nA)} for s in range(self.nS)\n",
        "        }\n",
        "\n",
        "        for s, (r, c) in self.state_to_pos.items():\n",
        "            if s in self.terminal_states:\n",
        "                for a in range(self.nA):\n",
        "                    P[s][a] = [(1.0, s, 0.0, True)]\n",
        "                continue\n",
        "\n",
        "            for a in range(self.nA):\n",
        "                outcomes = [(1.0 - self.cfg.slip_prob, a)]\n",
        "                if self.cfg.slip_prob > 0:\n",
        "                    outcomes.append((self.cfg.slip_prob / 2.0, left_of[a]))\n",
        "                    outcomes.append((self.cfg.slip_prob / 2.0, right_of[a]))\n",
        "\n",
        "                acc: Dict[Tuple[int, float, bool], float] = {}\n",
        "                for p, a_eff in outcomes:\n",
        "                    nr, nc = self._move(r, c, a_eff)\n",
        "                    s_next = self.pos_to_state[(nr, nc)]\n",
        "                    reward, done = self._cell_reward_done(nr, nc)\n",
        "                    key = (s_next, reward, done)\n",
        "                    acc[key] = acc.get(key, 0.0) + p\n",
        "\n",
        "                P[s][a] = [(p, s_next, reward, done) for (s_next, reward, done), p in acc.items()]\n",
        "\n",
        "        return P\n",
        "\n",
        "    def reset(self) -> int:\n",
        "        self._state = self.start_state\n",
        "        return self._state\n",
        "\n",
        "    def step(self, action: int) -> Tuple[int, float, bool, dict]:\n",
        "        if action not in ACTIONS:\n",
        "            raise ValueError(f\"Invalid action {action}. Must be in {list(ACTIONS.keys())}.\")\n",
        "\n",
        "        transitions = self._P[self._state][action]\n",
        "        probs = np.array([p for p, _, _, _ in transitions], dtype=float)\n",
        "        idx = int(rng.choice(len(transitions), p=probs))\n",
        "        _, s_next, reward, done = transitions[idx]\n",
        "\n",
        "        self._state = s_next\n",
        "        return s_next, reward, done, {}\n",
        "\n",
        "    def as_mdp(self) -> TabularMDP:\n",
        "        return TabularMDP(\n",
        "            nS=self.nS,\n",
        "            nA=self.nA,\n",
        "            P=self._P,\n",
        "            state_to_pos=self.state_to_pos,\n",
        "            pos_to_state=self.pos_to_state,\n",
        "            terminal_states=self.terminal_states,\n",
        "            start_state=self.start_state,\n",
        "            grid_chars=self.grid,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5cc6c86",
      "metadata": {},
      "source": [
        "### GridWorld diagnostics and sanity checks\n",
        "\n",
        "Before running any algorithms, we should verify the environment logic visually:\n",
        "- map rendering,\n",
        "- greedy/random rollout trace,\n",
        "- transition probability sanity check (`sum_p = 1`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a2f3e20",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from IPython.display import Image, display\n",
        "from matplotlib.collections import LineCollection\n",
        "\n",
        "\n",
        "def _draw_cell_grid(ax, H: int, W: int) -> None:\n",
        "    # Draw grid lines on cell borders (not through cell centers)\n",
        "    ax.set_xticks(np.arange(-0.5, W, 1), minor=True)\n",
        "    ax.set_yticks(np.arange(-0.5, H, 1), minor=True)\n",
        "    ax.grid(which=\"minor\", color=\"gray\", linewidth=0.8)\n",
        "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
        "    ax.set_xlim(-0.5, W - 0.5)\n",
        "    ax.set_ylim(H - 0.5, -0.5)\n",
        "\n",
        "\n",
        "def _map_code_array(grid_chars: np.ndarray) -> np.ndarray:\n",
        "    # Supports custom GridWorld and FrozenLake chars\n",
        "    code_map = {\"#\": 0, \".\": 1, \"F\": 1, \"S\": 2, \"G\": 3, \"H\": 4}\n",
        "    arr = np.full(grid_chars.shape, 1, dtype=int)\n",
        "    for r in range(grid_chars.shape[0]):\n",
        "        for c in range(grid_chars.shape[1]):\n",
        "            arr[r, c] = code_map.get(grid_chars[r, c], 1)\n",
        "    return arr\n",
        "\n",
        "\n",
        "def values_to_grid(mdp: TabularMDP, V: np.ndarray) -> np.ndarray:\n",
        "    H, W = mdp.grid_chars.shape\n",
        "    grid = np.full((H, W), np.nan, dtype=float)\n",
        "    for s, (r, c) in mdp.state_to_pos.items():\n",
        "        grid[r, c] = V[s]\n",
        "    return grid\n",
        "\n",
        "\n",
        "def _text_color_for_value(v: float, vmin: float, vmax: float, threshold: float = 0.55) -> str:\n",
        "    if not np.isfinite(v):\n",
        "        return \"black\"\n",
        "    if vmax <= vmin:\n",
        "        return \"black\"\n",
        "    norm = (v - vmin) / (vmax - vmin)\n",
        "    return \"white\" if norm < threshold else \"black\"\n",
        "\n",
        "\n",
        "def render_map(env: GridWorldEnv, title: str = \"GridWorld map\") -> None:\n",
        "    arr = _map_code_array(env.grid)\n",
        "    cmap = ListedColormap([\"black\", \"white\", \"#A7D3F5\", \"#B7E4C7\", \"#F8B4B4\"])\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(5, 5))\n",
        "    ax.imshow(arr, cmap=cmap, vmin=0, vmax=4)\n",
        "    ax.set_title(title)\n",
        "    ax.set_xticks(range(env.W))\n",
        "    ax.set_yticks(range(env.H))\n",
        "    _draw_cell_grid(ax, env.H, env.W)\n",
        "\n",
        "    for r in range(env.H):\n",
        "        for c in range(env.W):\n",
        "            txt = \" \" if env.grid[r, c] in (\".\", \"F\") else env.grid[r, c]\n",
        "            ax.text(c, r, txt, ha=\"center\", va=\"center\", color=\"black\", fontsize=12, fontweight=\"bold\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def render_policy_arrows(mdp: TabularMDP, greedy_actions: np.ndarray, title: str = \"Policy arrows\") -> None:\n",
        "    H, W = mdp.grid_chars.shape\n",
        "    bg = np.zeros((H, W), dtype=float)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(5, 5))\n",
        "    ax.imshow(bg, cmap=ListedColormap([\"#f7f7f7\"]))\n",
        "    ax.set_title(title)\n",
        "    ax.set_xticks(range(W))\n",
        "    ax.set_yticks(range(H))\n",
        "    _draw_cell_grid(ax, H, W)\n",
        "\n",
        "    for r in range(H):\n",
        "        for c in range(W):\n",
        "            ch = mdp.grid_chars[r, c]\n",
        "            if ch == \"#\":\n",
        "                txt = \"#\"\n",
        "            elif ch in (\"G\", \"H\"):\n",
        "                txt = ch\n",
        "            elif ch in (\".\", \"F\"):\n",
        "                txt = \" \"\n",
        "            else:\n",
        "                s = mdp.pos_to_state[(r, c)]\n",
        "                txt = ACTION_SYMBOLS[int(greedy_actions[s])]\n",
        "            ax.text(c, r, txt, ha=\"center\", va=\"center\", color=\"black\", fontsize=12, fontweight=\"bold\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def validate_transition_model(mdp: TabularMDP) -> None:\n",
        "    for s in range(mdp.nS):\n",
        "        for a in range(mdp.nA):\n",
        "            p_sum = sum(p for p, _, _, _ in mdp.P[s][a])\n",
        "            if not np.isclose(p_sum, 1.0):\n",
        "                raise AssertionError(f\"Transition probabilities do not sum to 1 at state={s}, action={a}: {p_sum}\")\n",
        "    print(\"Transition model sanity check passed (all action distributions sum to 1).\")\n",
        "\n",
        "\n",
        "def plot_visitation_heatmap(env: GridWorldEnv, counts: np.ndarray, title: str = \"Random policy visitation\"):\n",
        "    grid = np.full((env.H, env.W), np.nan, dtype=float)\n",
        "    for s, (r, c) in env.state_to_pos.items():\n",
        "        grid[r, c] = counts[s]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(5, 5))\n",
        "    im = ax.imshow(grid, cmap=\"magma\")\n",
        "    ax.set_title(title)\n",
        "    ax.set_xticks(range(env.W))\n",
        "    ax.set_yticks(range(env.H))\n",
        "    _draw_cell_grid(ax, env.H, env.W)\n",
        "\n",
        "    max_v = np.nanmax(grid) if np.any(~np.isnan(grid)) else 1.0\n",
        "    for r in range(env.H):\n",
        "        for c in range(env.W):\n",
        "            ch = env.grid[r, c]\n",
        "            if ch == \"#\":\n",
        "                ax.text(c, r, \"#\", ha=\"center\", va=\"center\", color=\"white\", fontweight=\"bold\")\n",
        "            elif not np.isnan(grid[r, c]):\n",
        "                color = \"black\" if grid[r, c] > 0.45 * max_v else \"white\"\n",
        "                ax.text(c, r, f\"{int(grid[r, c])}\", ha=\"center\", va=\"center\", color=color, fontsize=9)\n",
        "\n",
        "    fig.colorbar(im, ax=ax, shrink=0.8, label=\"visit count\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_value_heatmap(mdp: TabularMDP, V: np.ndarray, title: str = \"Value heatmap\"):\n",
        "    grid = values_to_grid(mdp, V)\n",
        "    H, W = grid.shape\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(5, 5))\n",
        "    im = ax.imshow(grid, cmap=\"viridis\")\n",
        "    ax.set_title(title)\n",
        "    ax.set_xticks(range(W))\n",
        "    ax.set_yticks(range(H))\n",
        "    _draw_cell_grid(ax, H, W)\n",
        "\n",
        "    finite_vals = grid[np.isfinite(grid)]\n",
        "    vmin = float(np.min(finite_vals)) if finite_vals.size else 0.0\n",
        "    vmax = float(np.max(finite_vals)) if finite_vals.size else 1.0\n",
        "\n",
        "    for r in range(H):\n",
        "        for c in range(W):\n",
        "            ch = mdp.grid_chars[r, c]\n",
        "            if ch == \"#\":\n",
        "                ax.text(c, r, \"#\", ha=\"center\", va=\"center\", color=\"white\", fontweight=\"bold\")\n",
        "            elif np.isfinite(grid[r, c]):\n",
        "                color = _text_color_for_value(grid[r, c], vmin, vmax, threshold=0.55)\n",
        "                ax.text(c, r, f\"{grid[r, c]:.1f}\", ha=\"center\", va=\"center\", color=color, fontsize=9)\n",
        "\n",
        "    fig.colorbar(im, ax=ax, shrink=0.8, label=\"V(s)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_vi_convergence(deltas: list, title: str = \"Value Iteration convergence\"):\n",
        "    fig, ax = plt.subplots(figsize=(6, 3.5))\n",
        "    ax.plot(deltas, lw=2)\n",
        "    ax.set_yscale(\"log\")\n",
        "    ax.set_xlabel(\"Sweep\")\n",
        "    ax.set_ylabel(\"max update delta\")\n",
        "    ax.set_title(title)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_vi_snapshots(mdp: TabularMDP, history: list, k_list=None, title_prefix: str = \"VI snapshot\"):\n",
        "    if not history:\n",
        "        return\n",
        "    if k_list is None:\n",
        "        last = len(history) - 1\n",
        "        mid = last // 2\n",
        "        k_list = sorted(set([0, mid, last]))\n",
        "\n",
        "    fig, axes = plt.subplots(1, len(k_list), figsize=(4 * len(k_list), 4))\n",
        "    if len(k_list) == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    all_grids = [values_to_grid(mdp, history[k]) for k in k_list]\n",
        "    all_vals = np.concatenate([g[np.isfinite(g)] for g in all_grids if np.any(np.isfinite(g))])\n",
        "    vmin = float(np.min(all_vals)) if all_vals.size else 0.0\n",
        "    vmax = float(np.max(all_vals)) if all_vals.size else 1.0\n",
        "\n",
        "    for ax, k in zip(axes, k_list):\n",
        "        grid = values_to_grid(mdp, history[k])\n",
        "        im = ax.imshow(grid, cmap=\"viridis\")\n",
        "        ax.set_title(f\"{title_prefix} k={k+1}\")\n",
        "        ax.set_xticks(range(grid.shape[1]))\n",
        "        ax.set_yticks(range(grid.shape[0]))\n",
        "        _draw_cell_grid(ax, grid.shape[0], grid.shape[1])\n",
        "\n",
        "        for r in range(grid.shape[0]):\n",
        "            for c in range(grid.shape[1]):\n",
        "                ch = mdp.grid_chars[r, c]\n",
        "                if ch == \"#\":\n",
        "                    ax.text(c, r, \"#\", ha=\"center\", va=\"center\", color=\"white\", fontweight=\"bold\")\n",
        "                elif np.isfinite(grid[r, c]):\n",
        "                    color = _text_color_for_value(grid[r, c], vmin, vmax, threshold=0.55)\n",
        "                    ax.text(c, r, f\"{grid[r, c]:.1f}\", ha=\"center\", va=\"center\", color=color, fontsize=8)\n",
        "\n",
        "    fig.colorbar(im, ax=axes, shrink=0.75)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def _draw_policy_on_axis(ax, mdp: TabularMDP, actions: np.ndarray, title: str):\n",
        "    arr = np.zeros(mdp.grid_chars.shape, dtype=float)\n",
        "    ax.imshow(arr, cmap=ListedColormap([\"#f7f7f7\"]))\n",
        "    ax.set_title(title, fontsize=11)\n",
        "    ax.set_xticks(range(arr.shape[1]))\n",
        "    ax.set_yticks(range(arr.shape[0]))\n",
        "    _draw_cell_grid(ax, arr.shape[0], arr.shape[1])\n",
        "\n",
        "    for r in range(arr.shape[0]):\n",
        "        for c in range(arr.shape[1]):\n",
        "            ch = mdp.grid_chars[r, c]\n",
        "            if ch == \"#\":\n",
        "                txt = \"#\"\n",
        "            elif ch in (\"G\", \"H\"):\n",
        "                txt = ch\n",
        "            elif ch in (\".\", \"F\"):\n",
        "                txt = \" \"\n",
        "            else:\n",
        "                s = mdp.pos_to_state[(r, c)]\n",
        "                txt = ACTION_SYMBOLS[int(actions[s])]\n",
        "            ax.text(c, r, txt, ha=\"center\", va=\"center\", color=\"black\", fontsize=11, fontweight=\"bold\")\n",
        "\n",
        "\n",
        "def _draw_value_on_axis(ax, mdp: TabularMDP, V: np.ndarray, title: str):\n",
        "    grid = values_to_grid(mdp, V)\n",
        "    im = ax.imshow(grid, cmap=\"viridis\")\n",
        "    ax.set_title(title, fontsize=11)\n",
        "    ax.set_xticks(range(grid.shape[1]))\n",
        "    ax.set_yticks(range(grid.shape[0]))\n",
        "    _draw_cell_grid(ax, grid.shape[0], grid.shape[1])\n",
        "\n",
        "    finite_vals = grid[np.isfinite(grid)]\n",
        "    vmin = float(np.min(finite_vals)) if finite_vals.size else 0.0\n",
        "    vmax = float(np.max(finite_vals)) if finite_vals.size else 1.0\n",
        "\n",
        "    for r in range(grid.shape[0]):\n",
        "        for c in range(grid.shape[1]):\n",
        "            ch = mdp.grid_chars[r, c]\n",
        "            if ch == \"#\":\n",
        "                ax.text(c, r, \"#\", ha=\"center\", va=\"center\", color=\"white\", fontweight=\"bold\")\n",
        "            elif np.isfinite(grid[r, c]):\n",
        "                color = _text_color_for_value(grid[r, c], vmin, vmax, threshold=0.55)\n",
        "                ax.text(c, r, f\"{grid[r, c]:.1f}\", ha=\"center\", va=\"center\", color=color, fontsize=8)\n",
        "\n",
        "    return im\n",
        "\n",
        "\n",
        "def save_vi_value_gif(\n",
        "    mdp: TabularMDP,\n",
        "    history: list,\n",
        "    gif_path: Path,\n",
        "    title_prefix: str = \"VI value evolution\",\n",
        "    show_after_save: bool = True,\n",
        "):\n",
        "    if not history:\n",
        "        print(\"No history available; GIF was not created.\")\n",
        "        return\n",
        "\n",
        "    from matplotlib.animation import FuncAnimation, PillowWriter\n",
        "\n",
        "    gif_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    grids = [values_to_grid(mdp, V) for V in history]\n",
        "    H, W = grids[0].shape\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(5, 5))\n",
        "    im = ax.imshow(grids[0], cmap=\"viridis\")\n",
        "    txt = ax.set_title(f\"{title_prefix} | sweep=1\")\n",
        "    ax.set_xticks(range(W))\n",
        "    ax.set_yticks(range(H))\n",
        "    _draw_cell_grid(ax, H, W)\n",
        "\n",
        "    def update(frame_idx):\n",
        "        im.set_data(grids[frame_idx])\n",
        "        txt.set_text(f\"{title_prefix} | sweep={frame_idx + 1}\")\n",
        "        return im, txt\n",
        "\n",
        "    ani = FuncAnimation(fig, update, frames=len(grids), interval=180, blit=False)\n",
        "    ani.save(gif_path, writer=PillowWriter(fps=6))\n",
        "    plt.close(fig)\n",
        "    print(f\"Saved GIF: {gif_path}\")\n",
        "\n",
        "    if show_after_save:\n",
        "        display(Image(filename=str(gif_path)))\n",
        "\n",
        "\n",
        "def save_vi_convergence_png(\n",
        "    deltas: list,\n",
        "    png_path: Path,\n",
        "    title: str = \"VI convergence\",\n",
        "    show_after_save: bool = True,\n",
        "):\n",
        "    if not deltas:\n",
        "        print(\"No deltas available; PNG was not created.\")\n",
        "        return\n",
        "\n",
        "    png_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6, 3.5))\n",
        "    ax.plot(deltas, lw=2)\n",
        "    ax.set_yscale(\"log\")\n",
        "    ax.set_xlabel(\"Sweep\")\n",
        "    ax.set_ylabel(\"max update delta\")\n",
        "    ax.set_title(title)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(png_path, dpi=160, bbox_inches=\"tight\")\n",
        "    plt.close(fig)\n",
        "    print(f\"Saved PNG: {png_path}\")\n",
        "\n",
        "    if show_after_save:\n",
        "        display(Image(filename=str(png_path)))\n",
        "\n",
        "\n",
        "def save_trajectory_gif(\n",
        "    env,\n",
        "    states: List[int],\n",
        "    gif_path: Path,\n",
        "    title: str = \"Trajectory evolution\",\n",
        "    fps: int = 6,\n",
        "    interval_ms: int = 170,\n",
        "    trail_len: int = 18,\n",
        "    show_after_save: bool = True,\n",
        "):\n",
        "    from matplotlib.animation import FuncAnimation, PillowWriter\n",
        "\n",
        "    gif_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    pts = np.array([env.state_to_pos[s] for s in states], dtype=float)\n",
        "    ys, xs = pts[:, 0], pts[:, 1]\n",
        "\n",
        "    arr = _map_code_array(env.grid)\n",
        "    cmap = ListedColormap([\"black\", \"white\", \"#A7D3F5\", \"#B7E4C7\", \"#F8B4B4\"])\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(5, 5))\n",
        "    ax.imshow(arr, cmap=cmap, vmin=0, vmax=4)\n",
        "    ax.set_title(title)\n",
        "    ax.set_xticks(range(env.W))\n",
        "    ax.set_yticks(range(env.H))\n",
        "    _draw_cell_grid(ax, env.H, env.W)\n",
        "\n",
        "    for r in range(env.H):\n",
        "        for c in range(env.W):\n",
        "            txt = \" \" if env.grid[r, c] in (\".\", \"F\") else env.grid[r, c]\n",
        "            ax.text(c, r, txt, ha=\"center\", va=\"center\", color=\"black\", fontsize=10, fontweight=\"bold\")\n",
        "\n",
        "    lc = LineCollection([], linewidths=2.5, capstyle=\"round\")\n",
        "    ax.add_collection(lc)\n",
        "\n",
        "    ax.scatter(xs[0], ys[0], color=\"orange\", s=90, zorder=6, label=\"start\")\n",
        "    head = ax.scatter([], [], color=\"red\", s=75, zorder=7, label=\"current\")\n",
        "    ax.legend(loc=\"upper right\")\n",
        "\n",
        "    base_rgb = np.array([31/255, 119/255, 180/255])\n",
        "\n",
        "    def update(i):\n",
        "        if i == 0:\n",
        "            lc.set_segments([])\n",
        "            head.set_offsets([[xs[0], ys[0]]])\n",
        "            return lc, head\n",
        "\n",
        "        start_idx = max(0, i - trail_len)\n",
        "        segs, cols = [], []\n",
        "        window = i - start_idx\n",
        "\n",
        "        for j in range(start_idx, i):\n",
        "            segs.append([(xs[j], ys[j]), (xs[j + 1], ys[j + 1])])\n",
        "            age = i - j\n",
        "            alpha = max(0.08, 1.0 - (age - 1) / max(1, window))\n",
        "            cols.append((base_rgb[0], base_rgb[1], base_rgb[2], alpha))\n",
        "\n",
        "        lc.set_segments(segs)\n",
        "        lc.set_color(cols)\n",
        "        head.set_offsets([[xs[i], ys[i]]])\n",
        "        return lc, head\n",
        "\n",
        "    ani = FuncAnimation(fig, update, frames=len(states), interval=interval_ms, blit=False)\n",
        "    ani.save(gif_path, writer=PillowWriter(fps=fps))\n",
        "    plt.close(fig)\n",
        "    print(f\"Saved GIF: {gif_path}\")\n",
        "\n",
        "    if show_after_save:\n",
        "        display(Image(filename=str(gif_path)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96303201",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example map intended for later Value/Policy Iteration experiments\n",
        "char_map = [\n",
        "    \"S...\",\n",
        "    \".#..\",\n",
        "    \"..H.\",\n",
        "    \"...G\",\n",
        "]\n",
        "\n",
        "cfg = GridWorldConfig(\n",
        "    char_map=char_map,\n",
        "    step_reward=-1.0,\n",
        "    goal_reward=10.0,\n",
        "    hole_reward=-10.0,\n",
        "    slip_prob=0.10,\n",
        ")\n",
        "\n",
        "env = GridWorldEnv(cfg)\n",
        "mdp = env.as_mdp()\n",
        "\n",
        "render_map(env, \"GridWorld\")\n",
        "validate_transition_model(mdp)\n",
        "print(f\"nS={mdp.nS}, nA={mdp.nA}, start={mdp.start_state}, terminal_states={sorted(mdp.terminal_states)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f5a5a6e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick rollout demo with a random policy to validate reset/step interface\n",
        "state = env.reset()\n",
        "trajectory = [state]\n",
        "rewards = []\n",
        "\n",
        "for t in range(25):\n",
        "    action = int(rng.integers(0, env.nA))\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    trajectory.append(next_state)\n",
        "    rewards.append(reward)\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "print(\"Rollout length:\", len(trajectory) - 1)\n",
        "print(\"Total reward:\", float(np.sum(rewards)))\n",
        "print(\"Visited states:\", trajectory)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "469a9a62",
      "metadata": {},
      "source": [
        "### Why this implementation is suitable for later DP\n",
        "\n",
        "- **Single source of dynamics truth**: simulator and tabular model are derived from the same transition logic.\n",
        "- **Tabular structure ready**: `P[s][a] -> [(prob, s_next, reward, done), ...]` exactly what Value/Policy Iteration needs.\n",
        "- **Configurable stochasticity**: `slip_prob` lets us move from deterministic GridWorld to FrozenLake-like behavior.\n",
        "- **Debuggable**: map rendering + rollout + transition checks catch modeling errors early.\n",
        "\n",
        "\n",
        "---CUT---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22bd1b4a",
      "metadata": {},
      "source": [
        "### Experiments - Random walk and diagnostics\n",
        "\n",
        "These experiments deepen intuition before dynamic programming algorithms.\n",
        "\n",
        "1. Random-walk baseline performance.\n",
        "2. Monte Carlo visitation heatmap.\n",
        "3. Transition probability sanity probes.\n",
        "4. Reward sensitivity sweep.\n",
        "5. Single-episode trajectory visualization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "540ce43d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_random_episode(env: GridWorldEnv, max_steps: int = 200):\n",
        "    state = env.reset()\n",
        "    states = [state]\n",
        "    actions = []\n",
        "    rewards = []\n",
        "\n",
        "    done = False\n",
        "    for _ in range(max_steps):\n",
        "        a = int(rng.integers(0, env.nA))\n",
        "        s_next, r, done, _ = env.step(a)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "        states.append(s_next)\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return {\n",
        "        \"states\": states,\n",
        "        \"actions\": actions,\n",
        "        \"rewards\": rewards,\n",
        "        \"done\": done,\n",
        "        \"return\": float(np.sum(rewards)),\n",
        "        \"length\": len(actions),\n",
        "        \"terminal_state\": states[-1],\n",
        "    }\n",
        "\n",
        "\n",
        "def evaluate_random_policy(env: GridWorldEnv, n_episodes: int = 2000, max_steps: int = 200):\n",
        "    success = 0\n",
        "    holes = 0\n",
        "    timeouts = 0\n",
        "    returns = []\n",
        "    lengths = []\n",
        "\n",
        "    for _ in range(n_episodes):\n",
        "        ep = run_random_episode(env, max_steps=max_steps)\n",
        "        returns.append(ep[\"return\"])\n",
        "        lengths.append(ep[\"length\"])\n",
        "\n",
        "        if ep[\"done\"]:\n",
        "            r, c = env.state_to_pos[ep[\"terminal_state\"]]\n",
        "            cell = env.grid[r, c]\n",
        "            if cell == \"G\":\n",
        "                success += 1\n",
        "            elif cell == \"H\":\n",
        "                holes += 1\n",
        "        else:\n",
        "            timeouts += 1\n",
        "\n",
        "    return {\n",
        "        \"episodes\": n_episodes,\n",
        "        \"success_rate\": success / n_episodes,\n",
        "        \"hole_rate\": holes / n_episodes,\n",
        "        \"timeout_rate\": timeouts / n_episodes,\n",
        "        \"mean_return\": float(np.mean(returns)),\n",
        "        \"std_return\": float(np.std(returns)),\n",
        "        \"mean_length\": float(np.mean(lengths)),\n",
        "    }\n",
        "\n",
        "\n",
        "def print_baseline_report(title: str, report: dict):\n",
        "    print(title)\n",
        "    print(f\"  episodes:     {report['episodes']}\")\n",
        "    print(f\"  success_rate: {report['success_rate']*100:6.2f}%\")\n",
        "    print(f\"  hole_rate:    {report['hole_rate']*100:6.2f}%\")\n",
        "    print(f\"  timeout_rate: {report['timeout_rate']*100:6.2f}%\")\n",
        "    print(f\"  mean_return:  {report['mean_return']:7.3f} ± {report['std_return']:.3f}\")\n",
        "    print(f\"  mean_length:  {report['mean_length']:7.2f}\")\n",
        "\n",
        "\n",
        "cfg_det = GridWorldConfig(char_map=char_map, step_reward=-1.0, goal_reward=10.0, hole_reward=-10.0, slip_prob=0.0)\n",
        "cfg_slip = GridWorldConfig(char_map=char_map, step_reward=-1.0, goal_reward=10.0, hole_reward=-10.0, slip_prob=0.20)\n",
        "\n",
        "env_det = GridWorldEnv(cfg_det)\n",
        "env_slip = GridWorldEnv(cfg_slip)\n",
        "\n",
        "rep_det = evaluate_random_policy(env_det, n_episodes=1500, max_steps=120)\n",
        "rep_slip = evaluate_random_policy(env_slip, n_episodes=1500, max_steps=120)\n",
        "\n",
        "print_baseline_report(\"Random policy baseline | deterministic\", rep_det)\n",
        "print()\n",
        "print_baseline_report(\"Random policy baseline | slippery (slip_prob=0.20)\", rep_slip)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "363abc14",
      "metadata": {},
      "source": [
        "**Monte Carlo visitation heatmap**\n",
        "\n",
        "State visitation frequencies show which areas are naturally explored under random behavior.\n",
        "This is useful context before policy optimization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b213f9ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "def visitation_counts_random(env: GridWorldEnv, n_episodes: int = 2000, max_steps: int = 200) -> np.ndarray:\n",
        "    counts = np.zeros(env.nS, dtype=float)\n",
        "\n",
        "    for _ in range(n_episodes):\n",
        "        s = env.reset()\n",
        "        counts[s] += 1\n",
        "        for _ in range(max_steps):\n",
        "            a = int(rng.integers(0, env.nA))\n",
        "            s_next, _, done, _ = env.step(a)\n",
        "            counts[s_next] += 1\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "    return counts\n",
        "\n",
        "\n",
        "counts_det = visitation_counts_random(env_det, n_episodes=2000, max_steps=120)\n",
        "counts_slip = visitation_counts_random(env_slip, n_episodes=2000, max_steps=120)\n",
        "\n",
        "plot_visitation_heatmap(env_det, counts_det, \"Visitation heatmap | deterministic\")\n",
        "plot_visitation_heatmap(env_slip, counts_slip, \"Visitation heatmap | slippery\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5055ca2",
      "metadata": {},
      "source": [
        "**Transition sanity probes**\n",
        "\n",
        "For selected states/actions, inspect the full $p(s', r, done \\mid s, a)$ entries.\n",
        "This validates edge handling, wall collisions, and slip dynamics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d946549a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_transition_probe(env: GridWorldEnv, state_pos: Tuple[int, int], action: int):\n",
        "    s = env.pos_to_state[state_pos]\n",
        "    print(f\"Probe at state={s}, pos={state_pos}, action={action} ({ACTION_SYMBOLS[action]})\")\n",
        "    rows = sorted(env.as_mdp().P[s][action], key=lambda x: (-x[0], x[1]))\n",
        "    for p, s_next, r, done in rows:\n",
        "        print(f\"  p={p:.3f} -> s'={s_next:2d}, pos'={env.state_to_pos[s_next]}, reward={r:5.1f}, done={done}\")\n",
        "    print()\n",
        "\n",
        "\n",
        "print(\"Deterministic probes\")\n",
        "print_transition_probe(env_det, (0, 0), RIGHT)\n",
        "print_transition_probe(env_det, (0, 0), UP)\n",
        "print_transition_probe(env_det, (1, 0), RIGHT)  # attempts to move into wall at (1,1)\n",
        "\n",
        "print(\"Slippery probes\")\n",
        "print_transition_probe(env_slip, (0, 0), RIGHT)\n",
        "print_transition_probe(env_slip, (1, 0), RIGHT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eece9fd3",
      "metadata": {},
      "source": [
        "**Reward sensitivity sweep**\n",
        "\n",
        "Without changing dynamics, vary `step_reward` and evaluate random-policy return.\n",
        "Even before optimization, reward scale influences expected return statistics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "601dd70e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def reward_sensitivity_experiment(step_rewards=(-0.1, -0.5, -1.0, -2.0), slip_prob=0.10):\n",
        "    rows = []\n",
        "    for sr in step_rewards:\n",
        "        cfg_tmp = GridWorldConfig(\n",
        "            char_map=char_map,\n",
        "            step_reward=float(sr),\n",
        "            goal_reward=10.0,\n",
        "            hole_reward=-10.0,\n",
        "            slip_prob=slip_prob,\n",
        "        )\n",
        "        env_tmp = GridWorldEnv(cfg_tmp)\n",
        "        rep = evaluate_random_policy(env_tmp, n_episodes=1200, max_steps=120)\n",
        "        rows.append((sr, rep[\"mean_return\"], rep[\"success_rate\"], rep[\"hole_rate\"]))\n",
        "\n",
        "    print(\"step_reward | mean_return | success_rate | hole_rate\")\n",
        "    for sr, mr, suc, hol in rows:\n",
        "        print(f\"{sr:10.2f} | {mr:11.3f} | {suc*100:10.2f}% | {hol*100:8.2f}%\")\n",
        "\n",
        "    x = [r[0] for r in rows]\n",
        "    y = [r[1] for r in rows]\n",
        "    plt.figure(figsize=(6, 3.5))\n",
        "    plt.plot(x, y, marker=\"o\")\n",
        "    plt.xlabel(\"step_reward\")\n",
        "    plt.ylabel(\"mean random-policy return\")\n",
        "    plt.title(\"Reward sensitivity under random policy\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "reward_sensitivity_experiment(step_rewards=(-0.1, -0.5, -1.0, -2.0), slip_prob=0.10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6cb1837",
      "metadata": {},
      "source": [
        "**Trajectory visualization**\n",
        "\n",
        "Plot a single random episode path on top of the grid for debugging and teaching.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb9dcb25",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from pathlib import Path\n",
        "from matplotlib.animation import FuncAnimation, PillowWriter\n",
        "from matplotlib.collections import LineCollection\n",
        "from IPython.display import Image, display\n",
        "\n",
        "def save_random_walk_gif_fading(\n",
        "    env: GridWorldEnv,\n",
        "    states: List[int],\n",
        "    gif_path: str = \"random_walk_fading.gif\",\n",
        "    title: str = \"Random walk trajectory (fading trail)\",\n",
        "    fps: int = 6,\n",
        "    trail_len: int = 18,\n",
        "    show_after_save: bool = True,   # <- new\n",
        "):\n",
        "    code_map = {\"#\": 0, \".\": 1, \"S\": 2, \"G\": 3, \"H\": 4}\n",
        "    arr = np.vectorize(code_map.get)(env.grid)\n",
        "    cmap = ListedColormap([\"black\", \"white\", \"#A7D3F5\", \"#B7E4C7\", \"#F8B4B4\"])\n",
        "\n",
        "    pts = np.array([env.state_to_pos[s] for s in states], dtype=float)\n",
        "    ys, xs = pts[:, 0], pts[:, 1]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(5, 5))\n",
        "    ax.imshow(arr, cmap=cmap, vmin=0, vmax=4)\n",
        "    ax.set_title(title)\n",
        "    ax.set_xticks(range(env.W))\n",
        "    ax.set_yticks(range(env.H))\n",
        "    _draw_cell_grid(ax, env.H, env.W)\n",
        "\n",
        "    for r in range(env.H):\n",
        "        for c in range(env.W):\n",
        "            txt = \" \" if env.grid[r, c] == \".\" else env.grid[r, c]\n",
        "            ax.text(c, r, txt, ha=\"center\", va=\"center\", color=\"black\", fontsize=10, fontweight=\"bold\")\n",
        "\n",
        "    lc = LineCollection([], linewidths=2.5, capstyle=\"round\")\n",
        "    ax.add_collection(lc)\n",
        "\n",
        "    head = ax.scatter([], [], color=\"red\", s=80, zorder=6)\n",
        "    ax.scatter(xs[0], ys[0], color=\"orange\", s=90, zorder=6, label=\"start\")\n",
        "    ax.legend(loc=\"upper right\")\n",
        "\n",
        "    base_rgb = np.array([31/255, 119/255, 180/255])\n",
        "\n",
        "    def update(i):\n",
        "        if i == 0:\n",
        "            lc.set_segments([])\n",
        "            head.set_offsets([[xs[0], ys[0]]])\n",
        "            return lc, head\n",
        "\n",
        "        start_idx = max(0, i - trail_len)\n",
        "        segs, cols = [], []\n",
        "        window = i - start_idx\n",
        "\n",
        "        for j in range(start_idx, i):\n",
        "            segs.append([(xs[j], ys[j]), (xs[j + 1], ys[j + 1])])\n",
        "            age = i - j\n",
        "            alpha = max(0.08, 1.0 - (age - 1) / max(1, window))\n",
        "            cols.append((base_rgb[0], base_rgb[1], base_rgb[2], alpha))\n",
        "\n",
        "        lc.set_segments(segs)\n",
        "        lc.set_color(cols)\n",
        "        head.set_offsets([[xs[i], ys[i]]])\n",
        "        return lc, head\n",
        "\n",
        "    ani = FuncAnimation(fig, update, frames=len(states), interval=170, blit=False)\n",
        "\n",
        "    out = Path(gif_path)\n",
        "    out.parent.mkdir(parents=True, exist_ok=True)\n",
        "    ani.save(out, writer=PillowWriter(fps=fps))\n",
        "    plt.close(fig)\n",
        "    print(f\"Saved GIF: {out}\")\n",
        "\n",
        "    if show_after_save:\n",
        "        display(Image(filename=str(out)))\n",
        "\n",
        "\n",
        "# Example\n",
        "ep = run_random_episode(env_slip, max_steps=60)\n",
        "print(f\"Trajectory length={ep['length']}, return={ep['return']:.2f}, done={ep['done']}\")\n",
        "save_random_walk_gif_fading(\n",
        "    env_slip,\n",
        "    ep[\"states\"],\n",
        "    gif_path=\"notebooks/sessions/session_02_mdp_dynamic_programming/assets/web_outputs/random_walk_slippery_fading.gif\",\n",
        "    title=\"Random walk trajectory (slippery, fading trail)\",\n",
        "    fps=6,\n",
        "    trail_len=18,\n",
        "    show_after_save=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "102ca564",
      "metadata": {},
      "source": [
        "## Value Iteration\n",
        "\n",
        "### Short theory recap (Sutton & Barto, Ch. 4)\n",
        "\n",
        "Value Iteration repeatedly applies the Bellman optimality operator:\n",
        "$$\n",
        "V_{k+1}(s) = \\max_a \\sum_{s',r} p(s',r\\mid s,a) \\left[r + \\gamma V_k(s')\\right]\n",
        "$$\n",
        "\n",
        "Intuition:\n",
        "- each sweep improves the approximation of the optimal value function,\n",
        "- the `max` operation already performs policy improvement implicitly,\n",
        "- once $V$ converges, an optimal greedy policy can be extracted from it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "736278fe",
      "metadata": {},
      "source": [
        "### Task 2\n",
        "**Implement Value Iteration (10-20 min)**\n",
        "\n",
        "Goal:\n",
        "- implement a Bellman optimality backup,\n",
        "- run the full Value Iteration loop (`delta < theta`),\n",
        "- extract the greedy policy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52d9787b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def bellman_optimality_backup(mdp: TabularMDP, V: np.ndarray, s: int, gamma: float) -> float:\n",
        "    if s in mdp.terminal_states:\n",
        "        return 0.0\n",
        "\n",
        "    q_vals = np.zeros(mdp.nA, dtype=float)\n",
        "    for a in range(mdp.nA):\n",
        "        for p, s_next, r, done in mdp.P[s][a]:\n",
        "            q_vals[a] += p * (r + gamma * (0.0 if done else V[s_next]))\n",
        "    return float(np.max(q_vals))\n",
        "\n",
        "\n",
        "def greedy_actions_from_values(mdp: TabularMDP, V: np.ndarray, gamma: float) -> np.ndarray:\n",
        "    actions = np.zeros(mdp.nS, dtype=int)\n",
        "    for s in range(mdp.nS):\n",
        "        if s in mdp.terminal_states:\n",
        "            actions[s] = 0\n",
        "            continue\n",
        "\n",
        "        q_vals = np.zeros(mdp.nA, dtype=float)\n",
        "        for a in range(mdp.nA):\n",
        "            for p, s_next, r, done in mdp.P[s][a]:\n",
        "                q_vals[a] += p * (r + gamma * (0.0 if done else V[s_next]))\n",
        "        actions[s] = int(np.argmax(q_vals))\n",
        "    return actions\n",
        "\n",
        "\n",
        "def value_iteration(\n",
        "    mdp: TabularMDP,\n",
        "    gamma: float = 0.95,\n",
        "    theta: float = 1e-10,\n",
        "    max_sweeps: int = 10_000,\n",
        "    return_history: bool = False,\n",
        "):\n",
        "    V = np.zeros(mdp.nS, dtype=float)\n",
        "    deltas = []\n",
        "    history = []\n",
        "\n",
        "    for sweep in range(1, max_sweeps + 1):\n",
        "        V_old = V.copy()\n",
        "        delta = 0.0\n",
        "\n",
        "        for s in range(mdp.nS):\n",
        "            v_new = bellman_optimality_backup(mdp, V_old, s, gamma)\n",
        "            V[s] = v_new\n",
        "            delta = max(delta, abs(V[s] - V_old[s]))\n",
        "\n",
        "        deltas.append(delta)\n",
        "        if return_history:\n",
        "            history.append(V.copy())\n",
        "\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "    greedy_actions = greedy_actions_from_values(mdp, V, gamma)\n",
        "    result = {\n",
        "        \"V\": V,\n",
        "        \"greedy_actions\": greedy_actions,\n",
        "        \"sweeps\": sweep,\n",
        "        \"deltas\": deltas,\n",
        "    }\n",
        "    if return_history:\n",
        "        result[\"history\"] = history\n",
        "    return result\n",
        "\n",
        "\n",
        "# Futtatás a két Part-1 környezeten\n",
        "res_det = value_iteration(env_det.as_mdp(), gamma=0.95, theta=1e-10, return_history=True)\n",
        "res_slip = value_iteration(env_slip.as_mdp(), gamma=0.95, theta=1e-10, return_history=True)\n",
        "\n",
        "print(f\"Deterministic: sweeps={res_det['sweeps']}, final_delta={res_det['deltas'][-1]:.3e}\")\n",
        "print(f\"Slippery:      sweeps={res_slip['sweeps']}, final_delta={res_slip['deltas'][-1]:.3e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1762f87",
      "metadata": {},
      "source": [
        "### Value Iteration visual diagnostics\n",
        "\n",
        "We visualize:\n",
        "- value-function heatmaps,\n",
        "- greedy-policy arrow maps,\n",
        "- convergence curves (`delta` per sweep),\n",
        "- intermediate Value Iteration snapshots.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bcb2282",
      "metadata": {},
      "outputs": [],
      "source": [
        "mdp_det = env_det.as_mdp()\n",
        "mdp_slip = env_slip.as_mdp()\n",
        "\n",
        "plot_value_heatmap(mdp_det, res_det[\"V\"], title=\"VI value heatmap | deterministic\")\n",
        "render_policy_arrows(mdp_det, res_det[\"greedy_actions\"], title=\"VI greedy policy | deterministic\")\n",
        "plot_vi_convergence(res_det[\"deltas\"], title=\"VI convergence | deterministic\")\n",
        "plot_vi_snapshots(mdp_det, res_det[\"history\"], title_prefix=\"Det VI\")\n",
        "\n",
        "plot_value_heatmap(mdp_slip, res_slip[\"V\"], title=\"VI value heatmap | slippery\")\n",
        "render_policy_arrows(mdp_slip, res_slip[\"greedy_actions\"], title=\"VI greedy policy | slippery\")\n",
        "plot_vi_convergence(res_slip[\"deltas\"], title=\"VI convergence | slippery\")\n",
        "plot_vi_snapshots(mdp_slip, res_slip[\"history\"], title_prefix=\"Slip VI\")\n",
        "\n",
        "\n",
        "try:\n",
        "    out_dir = Path(\"notebooks/sessions/session_02_mdp_dynamic_programming/assets/web_outputs\")\n",
        "\n",
        "    # GIFs only for gridworld-like spatial evolution\n",
        "    save_vi_value_gif(\n",
        "        mdp_det,\n",
        "        res_det[\"history\"],\n",
        "        out_dir / \"session2_vi_value_det.gif\",\n",
        "        title_prefix=\"VI value evolution (deterministic)\",\n",
        "        show_after_save=True,\n",
        "    )\n",
        "    save_vi_value_gif(\n",
        "        mdp_slip,\n",
        "        res_slip[\"history\"],\n",
        "        out_dir / \"session2_vi_value_slip.gif\",\n",
        "        title_prefix=\"VI value evolution (slippery)\",\n",
        "        show_after_save=True,\n",
        "    )\n",
        "\n",
        "    # Line plots as static PNG\n",
        "    save_vi_convergence_png(\n",
        "        res_det[\"deltas\"],\n",
        "        out_dir / \"session2_vi_conv_det.png\",\n",
        "        title=\"VI convergence (deterministic)\",\n",
        "        show_after_save=True,\n",
        "    )\n",
        "    save_vi_convergence_png(\n",
        "        res_slip[\"deltas\"],\n",
        "        out_dir / \"session2_vi_conv_slip.png\",\n",
        "        title=\"VI convergence (slippery)\",\n",
        "        show_after_save=True,\n",
        "    )\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Asset export skipped: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1d9138f",
      "metadata": {},
      "source": [
        "### Value Iteration experiment A: `gamma` sensitivity\n",
        "\n",
        "What we analyze:\n",
        "- number of convergence sweeps,\n",
        "- value at the start state,\n",
        "- changes in the greedy policy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c840360",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_policies(actions_a: np.ndarray, actions_b: np.ndarray, terminal_states: set) -> float:\n",
        "    mask = np.ones_like(actions_a, dtype=bool)\n",
        "    for s in terminal_states:\n",
        "        mask[s] = False\n",
        "    return float(np.mean(actions_a[mask] == actions_b[mask]))\n",
        "\n",
        "\n",
        "gammas = [0.70, 0.85, 0.95, 0.99]\n",
        "rows = []\n",
        "policies = {}\n",
        "\n",
        "for g in gammas:\n",
        "    out = value_iteration(mdp_det, gamma=g, theta=1e-10, return_history=False)\n",
        "    s0 = mdp_det.start_state\n",
        "    rows.append((g, out[\"sweeps\"], out[\"V\"][s0]))\n",
        "    policies[g] = out[\"greedy_actions\"]\n",
        "\n",
        "print(\"gamma | sweeps | V(start)\")\n",
        "for g, sw, vs in rows:\n",
        "    print(f\"{g:4.2f} | {sw:6d} | {vs:8.3f}\")\n",
        "\n",
        "print(\"\\nPolicy agreement vs gamma=0.95\")\n",
        "base = policies[0.95]\n",
        "for g in gammas:\n",
        "    agree = compare_policies(base, policies[g], mdp_det.terminal_states)\n",
        "    print(f\"gamma={g:4.2f} -> agreement={agree*100:6.2f}%\")\n",
        "\n",
        "plt.figure(figsize=(6, 3.5))\n",
        "plt.plot([r[0] for r in rows], [r[2] for r in rows], marker=\"o\")\n",
        "plt.xlabel(\"gamma\")\n",
        "plt.ylabel(\"V(start)\")\n",
        "plt.title(\"Gamma sensitivity (deterministic GridWorld)\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba9771ac",
      "metadata": {},
      "source": [
        "### Value Iteration experiment B: `theta` (stopping threshold) sensitivity\n",
        "\n",
        "What we analyze:\n",
        "- number of sweeps and runtime,\n",
        "- solution accuracy compared to a strict reference solution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a46bb5e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "thetas = [1e-4, 1e-6, 1e-8, 1e-10]\n",
        "ref = value_iteration(mdp_det, gamma=0.95, theta=1e-12, return_history=False)\n",
        "V_ref = ref[\"V\"]\n",
        "\n",
        "results = []\n",
        "for th in thetas:\n",
        "    t0 = time.perf_counter()\n",
        "    out = value_iteration(mdp_det, gamma=0.95, theta=th, return_history=False)\n",
        "    dt_ms = (time.perf_counter() - t0) * 1e3\n",
        "    linf = float(np.max(np.abs(out[\"V\"] - V_ref)))\n",
        "    results.append((th, out[\"sweeps\"], dt_ms, linf))\n",
        "\n",
        "print(\"theta | sweeps | runtime_ms | L_inf_to_ref\")\n",
        "for th, sw, dt, linf in results:\n",
        "    print(f\"{th:>5.0e} | {sw:6d} | {dt:10.3f} | {linf:11.3e}\")\n",
        "\n",
        "plt.figure(figsize=(6, 3.5))\n",
        "plt.plot([r[0] for r in results], [r[1] for r in results], marker=\"o\")\n",
        "plt.xscale(\"log\")\n",
        "plt.xlabel(\"theta\")\n",
        "plt.ylabel(\"sweeps\")\n",
        "plt.title(\"Stopping threshold vs sweep count\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ded83e3",
      "metadata": {},
      "source": [
        "### Value Iteration example: greedy-policy rollouts\n",
        "\n",
        "We execute episodes with the greedy policy returned by Value Iteration to inspect behavior, not only value tables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddf713f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def rollout_with_greedy_actions(env: GridWorldEnv, greedy_actions: np.ndarray, max_steps: int = 100):\n",
        "    s = env.reset()\n",
        "    states = [s]\n",
        "    rewards = []\n",
        "    done = False\n",
        "\n",
        "    for _ in range(max_steps):\n",
        "        a = int(greedy_actions[s])\n",
        "        s_next, r, done, _ = env.step(a)\n",
        "        states.append(s_next)\n",
        "        rewards.append(r)\n",
        "        s = s_next\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return {\n",
        "        \"states\": states,\n",
        "        \"return\": float(np.sum(rewards)),\n",
        "        \"length\": len(rewards),\n",
        "        \"done\": done,\n",
        "        \"terminal_state\": states[-1],\n",
        "    }\n",
        "\n",
        "\n",
        "def evaluate_greedy_policy(env: GridWorldEnv, greedy_actions: np.ndarray, n_episodes: int = 200):\n",
        "    returns = []\n",
        "    success = 0\n",
        "\n",
        "    for _ in range(n_episodes):\n",
        "        ep = rollout_with_greedy_actions(env, greedy_actions, max_steps=120)\n",
        "        returns.append(ep[\"return\"])\n",
        "        if ep[\"done\"]:\n",
        "            r, c = env.state_to_pos[ep[\"terminal_state\"]]\n",
        "            if env.grid[r, c] == \"G\":\n",
        "                success += 1\n",
        "\n",
        "    return float(np.mean(returns)), success / n_episodes\n",
        "\n",
        "\n",
        "mean_ret_det, succ_det = evaluate_greedy_policy(env_det, res_det[\"greedy_actions\"], n_episodes=200)\n",
        "mean_ret_slip, succ_slip = evaluate_greedy_policy(env_slip, res_slip[\"greedy_actions\"], n_episodes=200)\n",
        "\n",
        "print(f\"Deterministic greedy policy: mean_return={mean_ret_det:.3f}, success_rate={succ_det*100:.1f}%\")\n",
        "print(f\"Slippery greedy policy:      mean_return={mean_ret_slip:.3f}, success_rate={succ_slip*100:.1f}%\")\n",
        "\n",
        "ep_det = rollout_with_greedy_actions(env_det, res_det[\"greedy_actions\"], max_steps=60)\n",
        "ep_slip = rollout_with_greedy_actions(env_slip, res_slip[\"greedy_actions\"], max_steps=60)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "    out_dir = Path(\"notebooks/sessions/session_02_mdp_dynamic_programming/assets/web_outputs\")\n",
        "    gif_det = out_dir / \"session2_vi_greedy_traj_det.gif\"\n",
        "    gif_slip = out_dir / \"session2_vi_greedy_traj_slip.gif\"\n",
        "\n",
        "    save_trajectory_gif(\n",
        "        env_det,\n",
        "        ep_det[\"states\"],\n",
        "        gif_det,\n",
        "        title=\"Greedy rollout evolution (deterministic)\",\n",
        "        show_after_save=True,\n",
        "    )\n",
        "    save_trajectory_gif(\n",
        "        env_slip,\n",
        "        ep_slip[\"states\"],\n",
        "        gif_slip,\n",
        "        title=\"Greedy rollout evolution (slippery)\",\n",
        "        show_after_save=True,\n",
        "    )\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"PI trajectory GIF export skipped: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2939476",
      "metadata": {},
      "source": [
        "## Policy Evaluation + Policy Iteration\n",
        "\n",
        "### Short theory recap (Sutton & Barto, Ch. 4)\n",
        "\n",
        "Policy Iteration alternates between two operators:\n",
        "\n",
        "1. **Policy Evaluation**: compute $V^{\\pi}$ for the current policy $\\pi$.\n",
        "2. **Policy Improvement**: update policy greedily with respect to the evaluated value function.\n",
        "\n",
        "Policy evaluation uses the Bellman expectation equation:\n",
        "$$\n",
        "V^{\\pi}(s)=\\sum_a \\pi(a\\mid s)\\sum_{s',r}p(s',r\\mid s,a)\\left[r+\\gamma V^{\\pi}(s')\\right]\n",
        "$$\n",
        "\n",
        "Policy improvement is greedy:\n",
        "$$\n",
        "\\pi_{new}(s)\\in\\argmax_a \\sum_{s',r}p(s',r\\mid s,a)\\left[r+\\gamma V^{\\pi}(s')\\right]\n",
        "$$\n",
        "\n",
        "With finite tabular MDPs, repeated evaluation + improvement converges to an optimal policy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e093ca0",
      "metadata": {},
      "source": [
        "### Task 3\n",
        "**Implement Policy Iteration (10-20 min)**\n",
        "\n",
        "Goal:\n",
        "- implement iterative policy evaluation,\n",
        "- implement greedy policy improvement,\n",
        "- run full policy iteration until the policy is stable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b77f6133",
      "metadata": {},
      "outputs": [],
      "source": [
        "def policy_evaluation(\n",
        "    mdp: TabularMDP,\n",
        "    policy: np.ndarray,\n",
        "    gamma: float = 0.95,\n",
        "    theta: float = 1e-10,\n",
        "    max_sweeps: int = 50_000,\n",
        "):\n",
        "    V = np.zeros(mdp.nS, dtype=float)\n",
        "    deltas = []\n",
        "\n",
        "    for sweep in range(1, max_sweeps + 1):\n",
        "        V_old = V.copy()\n",
        "        delta = 0.0\n",
        "\n",
        "        for s in range(mdp.nS):\n",
        "            if s in mdp.terminal_states:\n",
        "                V[s] = 0.0\n",
        "                continue\n",
        "\n",
        "            v_new = 0.0\n",
        "            for a in range(mdp.nA):\n",
        "                pi_sa = policy[s, a]\n",
        "                if pi_sa == 0.0:\n",
        "                    continue\n",
        "                for p, s_next, r, done in mdp.P[s][a]:\n",
        "                    v_new += pi_sa * p * (r + gamma * (0.0 if done else V_old[s_next]))\n",
        "\n",
        "            V[s] = v_new\n",
        "            delta = max(delta, abs(V[s] - V_old[s]))\n",
        "\n",
        "        deltas.append(delta)\n",
        "        if delta < theta:\n",
        "            return V, sweep, deltas\n",
        "\n",
        "    return V, max_sweeps, deltas\n",
        "\n",
        "\n",
        "def greedy_policy_from_values(mdp: TabularMDP, V: np.ndarray, gamma: float = 0.95) -> np.ndarray:\n",
        "    policy = np.zeros((mdp.nS, mdp.nA), dtype=float)\n",
        "\n",
        "    for s in range(mdp.nS):\n",
        "        if s in mdp.terminal_states:\n",
        "            policy[s, :] = 1.0 / mdp.nA\n",
        "            continue\n",
        "\n",
        "        q_vals = np.zeros(mdp.nA, dtype=float)\n",
        "        for a in range(mdp.nA):\n",
        "            for p, s_next, r, done in mdp.P[s][a]:\n",
        "                q_vals[a] += p * (r + gamma * (0.0 if done else V[s_next]))\n",
        "\n",
        "        best_a = int(np.argmax(q_vals))\n",
        "        policy[s, best_a] = 1.0\n",
        "\n",
        "    return policy\n",
        "\n",
        "\n",
        "def policy_iteration(\n",
        "    mdp: TabularMDP,\n",
        "    gamma: float = 0.95,\n",
        "    eval_theta: float = 1e-10,\n",
        "    max_outer_loops: int = 1_000,\n",
        "    return_history: bool = False,\n",
        "):\n",
        "    policy = np.ones((mdp.nS, mdp.nA), dtype=float) / mdp.nA\n",
        "\n",
        "    history = []\n",
        "    eval_sweeps_list = []\n",
        "    eval_delta_curves = []\n",
        "\n",
        "    for outer in range(1, max_outer_loops + 1):\n",
        "        V, eval_sweeps, eval_deltas = policy_evaluation(\n",
        "            mdp,\n",
        "            policy,\n",
        "            gamma=gamma,\n",
        "            theta=eval_theta,\n",
        "        )\n",
        "\n",
        "        eval_sweeps_list.append(eval_sweeps)\n",
        "        eval_delta_curves.append(eval_deltas)\n",
        "\n",
        "        improved_policy = greedy_policy_from_values(mdp, V, gamma=gamma)\n",
        "\n",
        "        stable = np.array_equal(np.argmax(policy, axis=1), np.argmax(improved_policy, axis=1))\n",
        "\n",
        "        if return_history:\n",
        "            history.append(\n",
        "                {\n",
        "                    \"outer\": outer,\n",
        "                    \"V\": V.copy(),\n",
        "                    \"policy\": improved_policy.copy(),\n",
        "                    \"eval_sweeps\": eval_sweeps,\n",
        "                    \"eval_deltas\": eval_deltas,\n",
        "                    \"stable\": stable,\n",
        "                }\n",
        "            )\n",
        "\n",
        "        policy = improved_policy\n",
        "        if stable:\n",
        "            break\n",
        "\n",
        "    result = {\n",
        "        \"policy\": policy,\n",
        "        \"V\": V,\n",
        "        \"outer_loops\": outer,\n",
        "        \"eval_sweeps\": eval_sweeps_list,\n",
        "        \"eval_delta_curves\": eval_delta_curves,\n",
        "    }\n",
        "    if return_history:\n",
        "        result[\"history\"] = history\n",
        "    return result\n",
        "\n",
        "\n",
        "pi_det = policy_iteration(mdp_det, gamma=0.95, eval_theta=1e-10, return_history=True)\n",
        "pi_slip = policy_iteration(mdp_slip, gamma=0.95, eval_theta=1e-10, return_history=True)\n",
        "\n",
        "print(f\"Policy Iteration (deterministic): outer_loops={pi_det['outer_loops']}, eval_sweeps_total={sum(pi_det['eval_sweeps'])}\")\n",
        "print(f\"Policy Iteration (slippery):      outer_loops={pi_slip['outer_loops']}, eval_sweeps_total={sum(pi_slip['eval_sweeps'])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "654ec3d3",
      "metadata": {},
      "source": [
        "### Policy Iteration visual diagnostics\n",
        "\n",
        "We visualize:\n",
        "- final policy and value maps,\n",
        "- policy-iteration progress across outer loops,\n",
        "- policy-evaluation sweep counts per outer iteration,\n",
        "- policy-evaluation convergence curves.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcd826a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_policy_iteration_summary(mdp: TabularMDP, result: dict, title_prefix: str = \"PI\"):\n",
        "    plot_value_heatmap(mdp, result[\"V\"], title=f\"{title_prefix} value heatmap\")\n",
        "    final_actions = np.argmax(result[\"policy\"], axis=1)\n",
        "    render_policy_arrows(mdp, final_actions, title=f\"{title_prefix} greedy policy\")\n",
        "\n",
        "    plt.figure(figsize=(6, 3.5))\n",
        "    plt.plot(result[\"eval_sweeps\"], marker=\"o\")\n",
        "    plt.xlabel(\"Outer loop\")\n",
        "    plt.ylabel(\"Policy-evaluation sweeps\")\n",
        "    plt.title(f\"{title_prefix} evaluation sweeps per outer loop\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(6, 3.5))\n",
        "    for i, curve in enumerate(result[\"eval_delta_curves\"], start=1):\n",
        "        plt.plot(curve, label=f\"outer={i}\")\n",
        "    plt.yscale(\"log\")\n",
        "    plt.xlabel(\"Evaluation sweep\")\n",
        "    plt.ylabel(\"max update delta\")\n",
        "    plt.title(f\"{title_prefix} policy-evaluation convergence\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    if len(result[\"eval_delta_curves\"]) <= 8:\n",
        "        plt.legend(loc=\"best\", fontsize=8)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_policy_iteration_summary(mdp_det, pi_det, title_prefix=\"PI | deterministic\")\n",
        "plot_policy_iteration_summary(mdp_slip, pi_slip, title_prefix=\"PI | slippery\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36e58357",
      "metadata": {},
      "source": [
        "### Policy Iteration snapshots and GIF\n",
        "\n",
        "We capture intermediate policies and values across outer loops to show how policy improvement progresses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19c3ee35",
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "\n",
        "def plot_policy_iteration_snapshots(mdp: TabularMDP, history: list, title_prefix: str = \"PI snapshot\"):\n",
        "    if not history:\n",
        "        print(\"No policy-iteration history available.\")\n",
        "        return\n",
        "\n",
        "    idxs = sorted(set([0, len(history)//2, len(history)-1]))\n",
        "    for idx in idxs:\n",
        "        h = history[idx]\n",
        "        outer = h[\"outer\"]\n",
        "        V = h[\"V\"]\n",
        "        actions = np.argmax(h[\"policy\"], axis=1)\n",
        "        plot_value_heatmap(mdp, V, title=f\"{title_prefix} value | outer={outer}\")\n",
        "        render_policy_arrows(mdp, actions, title=f\"{title_prefix} policy | outer={outer}\")\n",
        "\n",
        "\n",
        "def save_policy_iteration_gif(\n",
        "    mdp: TabularMDP,\n",
        "    history: list,\n",
        "    gif_path: Path,\n",
        "    title_prefix: str = \"Policy Iteration\",\n",
        "    show_after_save: bool = True,   # <- new\n",
        "):\n",
        "    if not history:\n",
        "        print(\"No history available; GIF was not created.\")\n",
        "        return\n",
        "\n",
        "    from matplotlib.animation import FuncAnimation, PillowWriter\n",
        "\n",
        "    gif_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    value_grids = [values_to_grid(mdp, h[\"V\"]) for h in history]\n",
        "    action_grids = []\n",
        "    for h in history:\n",
        "        a = np.argmax(h[\"policy\"], axis=1)\n",
        "        g = np.full(mdp.grid_chars.shape, -1, dtype=int)\n",
        "        for s, (r, c) in mdp.state_to_pos.items():\n",
        "            if mdp.grid_chars[r, c] not in (\"G\", \"H\"):\n",
        "                g[r, c] = a[s]\n",
        "        action_grids.append(g)\n",
        "\n",
        "    vec = {\n",
        "        UP: (0.0, -0.35),\n",
        "        RIGHT: (0.35, 0.0),\n",
        "        DOWN: (0.0, 0.35),\n",
        "        LEFT: (-0.35, 0.0),\n",
        "    }\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(5, 5))\n",
        "    im = ax.imshow(value_grids[0], cmap=\"viridis\")\n",
        "    txt = ax.set_title(f\"{title_prefix} | outer=1\")\n",
        "    ax.set_xticks(range(value_grids[0].shape[1]))\n",
        "    ax.set_yticks(range(value_grids[0].shape[0]))\n",
        "    _draw_cell_grid(ax, value_grids[0].shape[0], value_grids[0].shape[1])  # optional nice alignment\n",
        "\n",
        "    q = None\n",
        "\n",
        "    def update(frame_idx):\n",
        "        nonlocal q\n",
        "        grid = value_grids[frame_idx]\n",
        "        actions = action_grids[frame_idx]\n",
        "        im.set_data(grid)\n",
        "\n",
        "        if q is not None:\n",
        "            q.remove()\n",
        "\n",
        "        X, Y, U, V = [], [], [], []\n",
        "        for r in range(actions.shape[0]):\n",
        "            for c in range(actions.shape[1]):\n",
        "                if mdp.grid_chars[r, c] in (\"#\", \"G\", \"H\"):\n",
        "                    continue\n",
        "                a = actions[r, c]\n",
        "                if a < 0:\n",
        "                    continue\n",
        "                u, v = vec[a]\n",
        "                X.append(c); Y.append(r); U.append(u); V.append(v)\n",
        "\n",
        "        if X:\n",
        "            q = ax.quiver(X, Y, U, V, angles=\"xy\", scale_units=\"xy\", scale=1, color=\"white\", width=0.008)\n",
        "\n",
        "        outer = history[frame_idx][\"outer\"]\n",
        "        txt.set_text(f\"{title_prefix} | outer={outer}\")\n",
        "        return (im, txt)\n",
        "\n",
        "    ani = FuncAnimation(fig, update, frames=len(history), interval=550, blit=False)\n",
        "    ani.save(gif_path, writer=PillowWriter(fps=2))\n",
        "    plt.close(fig)\n",
        "    print(f\"Saved GIF: {gif_path}\")\n",
        "\n",
        "    if show_after_save:\n",
        "        display(Image(filename=str(gif_path)))\n",
        "\n",
        "\n",
        "plot_policy_iteration_snapshots(mdp_det, pi_det[\"history\"], title_prefix=\"PI deterministic\")\n",
        "plot_policy_iteration_snapshots(mdp_slip, pi_slip[\"history\"], title_prefix=\"PI slippery\")\n",
        "\n",
        "try:\n",
        "    out_dir = Path(\"notebooks/sessions/session_02_mdp_dynamic_programming/assets/web_outputs\")\n",
        "    save_policy_iteration_gif(\n",
        "        mdp_det, pi_det[\"history\"], out_dir / \"session2_pi_evolution_det.gif\",\n",
        "        title_prefix=\"Policy Iteration deterministic\", show_after_save=True\n",
        "    )\n",
        "    save_policy_iteration_gif(\n",
        "        mdp_slip, pi_slip[\"history\"], out_dir / \"session2_pi_evolution_slip.gif\",\n",
        "        title_prefix=\"Policy Iteration slippery\", show_after_save=True\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"PI GIF export skipped: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7ac7ac7",
      "metadata": {},
      "source": [
        "### Policy Iteration experiment A: `gamma` sensitivity\n",
        "\n",
        "What we analyze:\n",
        "- number of outer loops,\n",
        "- total policy-evaluation sweeps,\n",
        "- value at the start state,\n",
        "- policy agreement relative to $\\gamma=0.95$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b688ed92",
      "metadata": {},
      "outputs": [],
      "source": [
        "gammas_pi = [0.70, 0.85, 0.95, 0.99]\n",
        "rows_pi = []\n",
        "policies_pi = {}\n",
        "\n",
        "for g in gammas_pi:\n",
        "    out = policy_iteration(mdp_det, gamma=g, eval_theta=1e-10, return_history=False)\n",
        "    start_v = out[\"V\"][mdp_det.start_state]\n",
        "    rows_pi.append((g, out[\"outer_loops\"], int(sum(out[\"eval_sweeps\"])), start_v))\n",
        "    policies_pi[g] = np.argmax(out[\"policy\"], axis=1)\n",
        "\n",
        "print(\"gamma | outer_loops | eval_sweeps_total | V(start)\")\n",
        "for g, ol, es, vs in rows_pi:\n",
        "    print(f\"{g:4.2f} | {ol:11d} | {es:17d} | {vs:8.3f}\")\n",
        "\n",
        "print(\"\\nPolicy agreement vs gamma=0.95\")\n",
        "base = policies_pi[0.95]\n",
        "for g in gammas_pi:\n",
        "    agree = compare_policies(base, policies_pi[g], mdp_det.terminal_states)\n",
        "    print(f\"gamma={g:4.2f} -> agreement={agree*100:6.2f}%\")\n",
        "\n",
        "plt.figure(figsize=(6, 3.5))\n",
        "plt.plot([r[0] for r in rows_pi], [r[3] for r in rows_pi], marker=\"o\")\n",
        "plt.xlabel(\"gamma\")\n",
        "plt.ylabel(\"V(start)\")\n",
        "plt.title(\"Policy Iteration gamma sensitivity\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24153e2d",
      "metadata": {},
      "source": [
        "### Policy Iteration experiment B: evaluation-threshold sensitivity\n",
        "\n",
        "What we analyze:\n",
        "- how `eval_theta` affects computational effort,\n",
        "- whether the final greedy policy changes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7de2cb75",
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_thetas = [1e-4, 1e-6, 1e-8, 1e-10]\n",
        "rows_theta = []\n",
        "pol_by_theta = {}\n",
        "\n",
        "for th in eval_thetas:\n",
        "    out = policy_iteration(mdp_det, gamma=0.95, eval_theta=th, return_history=False)\n",
        "    actions = np.argmax(out[\"policy\"], axis=1)\n",
        "    rows_theta.append((th, out[\"outer_loops\"], int(sum(out[\"eval_sweeps\"]))))\n",
        "    pol_by_theta[th] = actions\n",
        "\n",
        "print(\"eval_theta | outer_loops | eval_sweeps_total\")\n",
        "for th, ol, es in rows_theta:\n",
        "    print(f\"{th:>9.0e} | {ol:11d} | {es:17d}\")\n",
        "\n",
        "base = pol_by_theta[1e-10]\n",
        "print(\"\\nPolicy agreement vs eval_theta=1e-10\")\n",
        "for th in eval_thetas:\n",
        "    agree = compare_policies(base, pol_by_theta[th], mdp_det.terminal_states)\n",
        "    print(f\"eval_theta={th:>9.0e} -> agreement={agree*100:6.2f}%\")\n",
        "\n",
        "plt.figure(figsize=(6, 3.5))\n",
        "plt.plot([r[0] for r in rows_theta], [r[2] for r in rows_theta], marker=\"o\")\n",
        "plt.xscale(\"log\")\n",
        "plt.xlabel(\"eval_theta\")\n",
        "plt.ylabel(\"total evaluation sweeps\")\n",
        "plt.title(\"Policy Iteration threshold sensitivity\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c3b9343",
      "metadata": {},
      "source": [
        "### Policy Iteration example: greedy-policy rollouts\n",
        "\n",
        "We run episodes with the final PI policy in deterministic and slippery environments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20f9a9ca",
      "metadata": {},
      "outputs": [],
      "source": [
        "pi_actions_det = np.argmax(pi_det[\"policy\"], axis=1)\n",
        "pi_actions_slip = np.argmax(pi_slip[\"policy\"], axis=1)\n",
        "\n",
        "mean_ret_det_pi, succ_det_pi = evaluate_greedy_policy(env_det, pi_actions_det, n_episodes=200)\n",
        "mean_ret_slip_pi, succ_slip_pi = evaluate_greedy_policy(env_slip, pi_actions_slip, n_episodes=200)\n",
        "\n",
        "print(f\"PI deterministic policy: mean_return={mean_ret_det_pi:.3f}, success_rate={succ_det_pi*100:.1f}%\")\n",
        "print(f\"PI slippery policy:      mean_return={mean_ret_slip_pi:.3f}, success_rate={succ_slip_pi*100:.1f}%\")\n",
        "\n",
        "ep_det_pi = rollout_with_greedy_actions(env_det, pi_actions_det, max_steps=60)\n",
        "ep_slip_pi = rollout_with_greedy_actions(env_slip, pi_actions_slip, max_steps=60)\n",
        "\n",
        "# plot_trajectory(env_det, ep_det_pi[\"states\"], title=\"Greedy rollout (PI) | deterministic\")\n",
        "# plot_trajectory(env_slip, ep_slip_pi[\"states\"], title=\"Greedy rollout (PI) | slippery\")\n",
        "\n",
        "try:\n",
        "    out_dir = Path(\"notebooks/sessions/session_02_mdp_dynamic_programming/assets/web_outputs\")\n",
        "    gif_det = out_dir / \"session2_pi_greedy_traj_det.gif\"\n",
        "    gif_slip = out_dir / \"session2_pi_greedy_traj_slip.gif\"\n",
        "\n",
        "    # Assuming your save_trajectory_gif has show_after_save=True support\n",
        "    save_trajectory_gif(\n",
        "        env_det,\n",
        "        ep_det_pi[\"states\"],\n",
        "        gif_det,\n",
        "        title=\"Greedy rollout evolution (PI deterministic)\",\n",
        "        show_after_save=True,\n",
        "    )\n",
        "    save_trajectory_gif(\n",
        "        env_slip,\n",
        "        ep_slip_pi[\"states\"],\n",
        "        gif_slip,\n",
        "        title=\"Greedy rollout evolution (PI slippery)\",\n",
        "        show_after_save=True,\n",
        "    )\n",
        "\n",
        "    # Fallback explicit display (works even if save_trajectory_gif ignores show_after_save)\n",
        "    display(Image(filename=str(gif_det)))\n",
        "    display(Image(filename=str(gif_slip)))\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"PI trajectory GIF export skipped: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9d93028",
      "metadata": {},
      "source": [
        "## Large-map experiments\n",
        "\n",
        "Before switching to Gym/FrozenLake, we stress-test Value Iteration and Policy Iteration on larger custom GridWorld maps.\n",
        "\n",
        "Focus:\n",
        "- scalability with increasing state count,\n",
        "- policy quality and agreement,\n",
        "- runtime/sweep trade-offs,\n",
        "- more visual intuition on complex layouts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e57d8c8",
      "metadata": {},
      "source": [
        "### Task 4 (10-20 min) - Scale to larger maps and benchmark VI vs PI\n",
        "\n",
        "We create multiple larger maps and run both algorithms under the same hyperparameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c9de730",
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "large_maps = {\n",
        "    \"maze_8x8\": [\n",
        "        \"S..#....\",\n",
        "        \".##.#H#.\",\n",
        "        \"...#.#..\",\n",
        "        \".#...##.\",\n",
        "        \".#.H....\",\n",
        "        \".###.#..\",\n",
        "        \"...#..#.\",\n",
        "        \".H....G.\",\n",
        "    ],\n",
        "    \"corridor_10x10\": [\n",
        "        \"S...#.....\",\n",
        "        \".##.#.###.\",\n",
        "        \".#..#...#.\",\n",
        "        \".#.###.#..\",\n",
        "        \".H....#.#.\",\n",
        "        \"###.#.#.#.\",\n",
        "        \"...#...#..\",\n",
        "        \".#.###.##.\",\n",
        "        \".#...H...#\",\n",
        "        \"...##...G.\",\n",
        "    ],\n",
        "    \"rooms_12x12\": [\n",
        "        \"S...#......G\",\n",
        "        \".##.#..##H#H\",\n",
        "        \"...#.#...#..\",\n",
        "        \".#.#.###.#..\",\n",
        "        \".#...#......\",\n",
        "        \".###.#H###..\",\n",
        "        \"...#...#....\",\n",
        "        \".#.###.#.##.\",\n",
        "        \".#...H.#...#\",\n",
        "        \".###H#.###.#\",\n",
        "        \"...#...#....\",\n",
        "        \".H...#...#..\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "def make_large_env(char_map, slip_prob=0.10, step_reward=-1.0, goal_reward=12.0, hole_reward=-12.0):\n",
        "    cfg = GridWorldConfig(\n",
        "        char_map=char_map,\n",
        "        step_reward=step_reward,\n",
        "        goal_reward=goal_reward,\n",
        "        hole_reward=hole_reward,\n",
        "        slip_prob=slip_prob,\n",
        "    )\n",
        "    return GridWorldEnv(cfg)\n",
        "\n",
        "\n",
        "def benchmark_vi_pi_on_env(env: GridWorldEnv, gamma: float = 0.95, theta: float = 1e-10):\n",
        "    mdp = env.as_mdp()\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    vi = value_iteration(mdp, gamma=gamma, theta=theta, return_history=True)\n",
        "    t_vi_ms = (time.perf_counter() - t0) * 1e3\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    pi = policy_iteration(mdp, gamma=gamma, eval_theta=theta, return_history=True)\n",
        "    t_pi_ms = (time.perf_counter() - t0) * 1e3\n",
        "\n",
        "    vi_actions = vi[\"greedy_actions\"]\n",
        "    pi_actions = np.argmax(pi[\"policy\"], axis=1)\n",
        "\n",
        "    agreement = compare_policies(vi_actions, pi_actions, mdp.terminal_states)\n",
        "    value_linf = float(np.max(np.abs(vi[\"V\"] - pi[\"V\"])))\n",
        "\n",
        "    vi_ret, vi_succ = evaluate_greedy_policy(env, vi_actions, n_episodes=200)\n",
        "    pi_ret, pi_succ = evaluate_greedy_policy(env, pi_actions, n_episodes=200)\n",
        "\n",
        "    return {\n",
        "        \"mdp\": mdp,\n",
        "        \"vi\": vi,\n",
        "        \"pi\": pi,\n",
        "        \"t_vi_ms\": t_vi_ms,\n",
        "        \"t_pi_ms\": t_pi_ms,\n",
        "        \"value_linf\": value_linf,\n",
        "        \"policy_agreement\": agreement,\n",
        "        \"vi_mean_return\": vi_ret,\n",
        "        \"vi_success_rate\": vi_succ,\n",
        "        \"pi_mean_return\": pi_ret,\n",
        "        \"pi_success_rate\": pi_succ,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "137f010c",
      "metadata": {},
      "source": [
        "**Run VI vs PI on all larger maps**\n",
        "\n",
        "Metrics:\n",
        "- states/actions,\n",
        "- VI and PI runtime,\n",
        "- VI sweeps,\n",
        "- PI outer loops and total evaluation sweeps,\n",
        "- value-function $L_\\infty$ difference,\n",
        "- greedy-policy agreement,\n",
        "- rollout return and success rate.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "940f087d",
      "metadata": {},
      "outputs": [],
      "source": [
        "large_results = {}\n",
        "\n",
        "for name, char_map in large_maps.items():\n",
        "    env = make_large_env(char_map, slip_prob=0.0,step_reward=-0.1) # Negative -1 would go to hole because its less loss than reaching goal, so we use smaller step penalty to encourage reaching goal\n",
        "    \n",
        "    res = benchmark_vi_pi_on_env(env, gamma=0.95, theta=1e-10)\n",
        "    large_results[name] = {\"env\": env, **res}\n",
        "\n",
        "print(\n",
        "    \"map | nS | VI_ms | PI_ms | VI_sweeps | PI_outer | PI_eval_total | L_inf(VI-PI) | policy_agree | VI_succ | PI_succ\"\n",
        ")\n",
        "for name, data in large_results.items():\n",
        "    mdp = data[\"mdp\"]\n",
        "    vi = data[\"vi\"]\n",
        "    pi = data[\"pi\"]\n",
        "    print(\n",
        "        f\"{name:12s} | {mdp.nS:3d} | {data['t_vi_ms']:6.1f} | {data['t_pi_ms']:6.1f} | \"\n",
        "        f\"{vi['sweeps']:9d} | {pi['outer_loops']:8d} | {sum(pi['eval_sweeps']):13d} | \"\n",
        "        f\"{data['value_linf']:11.2e} | {data['policy_agreement']*100:11.1f}% | \"\n",
        "        f\"{data['vi_success_rate']*100:6.1f}% | {data['pi_success_rate']*100:6.1f}%\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fed5a679",
      "metadata": {},
      "source": [
        "**Experiment L3 - Visual deep dive on selected maps**\n",
        "We inspect two maps in detail:\n",
        "- value heatmaps,\n",
        "- greedy policies,\n",
        "- convergence diagnostics,\n",
        "- policy-iteration snapshots.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25fd359a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def showcase_pi_collage(name: str, data: dict):\n",
        "    mdp = data[\"mdp\"]\n",
        "    pi = data[\"pi\"]\n",
        "    pi_actions = np.argmax(pi[\"policy\"], axis=1)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    fig.suptitle(f\"Policy Iteration Showcase | {name}\", fontsize=16, y=1.02)\n",
        "\n",
        "    map_arr = _map_code_array(mdp.grid_chars)\n",
        "    map_cmap = ListedColormap([\"black\", \"white\", \"#A7D3F5\", \"#B7E4C7\", \"#F8B4B4\"])\n",
        "\n",
        "    axes[0].imshow(map_arr, cmap=map_cmap, vmin=0, vmax=4)\n",
        "    axes[0].set_title(\"Map layout\", fontsize=11)\n",
        "    axes[0].set_xticks(range(map_arr.shape[1]))\n",
        "    axes[0].set_yticks(range(map_arr.shape[0]))\n",
        "    _draw_cell_grid(axes[0], map_arr.shape[0], map_arr.shape[1])\n",
        "    for r in range(map_arr.shape[0]):\n",
        "        for c in range(map_arr.shape[1]):\n",
        "            txt = \" \" if mdp.grid_chars[r, c] in (\".\", \"F\") else mdp.grid_chars[r, c]\n",
        "            axes[0].text(c, r, txt, ha=\"center\", va=\"center\", color=\"black\", fontsize=10, fontweight=\"bold\")\n",
        "\n",
        "    im_pi = _draw_value_on_axis(axes[1], mdp, pi[\"V\"], \"PI final values\")\n",
        "    _draw_policy_on_axis(axes[2], mdp, pi_actions, \"PI greedy policy\")\n",
        "\n",
        "    fig.colorbar(im_pi, ax=axes[1], fraction=0.046, pad=0.04)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "selected_maps = [\"maze_8x8\", \"corridor_10x10\", \"rooms_12x12\"]\n",
        "\n",
        "for name in selected_maps:\n",
        "    data = large_results[name]\n",
        "    env = data[\"env\"]\n",
        "    pi = data[\"pi\"]\n",
        "\n",
        "    print(f\"\\n=== {name} | Policy Iteration ===\")\n",
        "    showcase_pi_collage(name, data)\n",
        "\n",
        "    out_dir = Path(\"notebooks/sessions/session_02_mdp_dynamic_programming/assets/web_outputs\")\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    ep_pi = rollout_with_greedy_actions(env, np.argmax(pi[\"policy\"], axis=1), max_steps=220)\n",
        "    gif_pi = out_dir / f\"session2_showcase_{name}_pi_traj.gif\"\n",
        "\n",
        "    save_trajectory_gif(\n",
        "        env,\n",
        "        ep_pi[\"states\"],\n",
        "        gif_pi,\n",
        "        title=f\"PI greedy rollout ({name})\",\n",
        "        trail_len=22,\n",
        "        show_after_save=False,\n",
        "    )\n",
        "\n",
        "    print(f\"Saved: {gif_pi}\")\n",
        "    display(Image(filename=str(gif_pi)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "631bc681",
      "metadata": {},
      "outputs": [],
      "source": [
        "def _ensure_large_showcase_assets(name: str, data: dict, out_dir: Path):\n",
        "    \"\"\"Generate all large-map GIF assets for one map.\"\"\"\n",
        "    env = data[\"env\"]\n",
        "    mdp = data[\"mdp\"]\n",
        "    vi = data[\"vi\"]\n",
        "    pi = data[\"pi\"]\n",
        "\n",
        "    p_vi_value = out_dir / f\"session2_large_{name}_vi_value.gif\"\n",
        "    p_pi_evo = out_dir / f\"session2_large_{name}_pi_evolution.gif\"\n",
        "    p_vi_traj = out_dir / f\"session2_large_{name}_vi_traj.gif\"\n",
        "    p_pi_traj = out_dir / f\"session2_large_{name}_pi_traj.gif\"\n",
        "\n",
        "    save_vi_value_gif(mdp, vi[\"history\"], p_vi_value, title_prefix=f\"VI value evolution ({name})\", show_after_save=False)\n",
        "    save_policy_iteration_gif(mdp, pi[\"history\"], p_pi_evo, title_prefix=f\"PI evolution ({name})\", show_after_save=False)\n",
        "\n",
        "    ep_vi = rollout_with_greedy_actions(env, vi[\"greedy_actions\"], max_steps=220)\n",
        "    ep_pi = rollout_with_greedy_actions(env, np.argmax(pi[\"policy\"], axis=1), max_steps=220)\n",
        "\n",
        "    save_trajectory_gif(env, ep_vi[\"states\"], p_vi_traj, title=f\"VI greedy rollout ({name})\", trail_len=22, show_after_save=False)\n",
        "    save_trajectory_gif(env, ep_pi[\"states\"], p_pi_traj, title=f\"PI greedy rollout ({name})\", trail_len=22, show_after_save=False)\n",
        "\n",
        "    return {\n",
        "        \"vi_value\": p_vi_value,\n",
        "        \"pi_evolution\": p_pi_evo,\n",
        "        \"vi_traj\": p_vi_traj,\n",
        "        \"pi_traj\": p_pi_traj,\n",
        "    }\n",
        "\n",
        "\n",
        "def show_large_map_merged_showcase(selected_maps, large_results):\n",
        "    out_dir = Path(\"notebooks/sessions/session_02_mdp_dynamic_programming/assets/web_outputs\")\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for name in selected_maps:\n",
        "        data = large_results[name]\n",
        "        print(f\"\\n=== {name} ===\")\n",
        "\n",
        "        # Reuse compact static collage helper\n",
        "        showcase_pi_collage(name, data)\n",
        "\n",
        "        paths = _ensure_large_showcase_assets(name, data, out_dir)\n",
        "\n",
        "        print(\"Evolution GIFs\")\n",
        "        display(Image(filename=str(paths[\"vi_value\"])))\n",
        "        display(Image(filename=str(paths[\"pi_evolution\"])))\n",
        "\n",
        "        print(\"Trajectory showcase GIFs\")\n",
        "        display(Image(filename=str(paths[\"vi_traj\"])))\n",
        "        display(Image(filename=str(paths[\"pi_traj\"])))\n",
        "\n",
        "    print(f\"\\nSaved large-map GIFs into: {out_dir}\")\n",
        "\n",
        "\n",
        "selected_maps = [\"maze_8x8\", \"corridor_10x10\", \"rooms_12x12\"]\n",
        "show_large_map_merged_showcase(selected_maps, large_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7142d63",
      "metadata": {},
      "source": [
        "### Experiment L5 - Robustness stress-test (stochasticity sweep)\n",
        "\n",
        "We vary `slip_prob` on one larger map and compare policy quality for VI/PI.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8d72428",
      "metadata": {},
      "outputs": [],
      "source": [
        "stress_map_name = \"rooms_12x12\"\n",
        "stress_map = large_maps[stress_map_name]\n",
        "slips = [0.00, 0.05, 0.10, 0.20, 0.30]\n",
        "\n",
        "rows = []\n",
        "for sp in slips:\n",
        "    env = make_large_env(stress_map, slip_prob=sp)\n",
        "    data = benchmark_vi_pi_on_env(env, gamma=0.95, theta=1e-10)\n",
        "    rows.append((sp, data[\"vi_success_rate\"], data[\"pi_success_rate\"], data[\"policy_agreement\"], data[\"t_vi_ms\"], data[\"t_pi_ms\"]))\n",
        "\n",
        "print(\"slip_prob | VI_success | PI_success | policy_agreement | VI_ms | PI_ms\")\n",
        "for sp, vi_s, pi_s, ag, tvi, tpi in rows:\n",
        "    print(f\"{sp:8.2f} | {vi_s*100:9.1f}% | {pi_s*100:9.1f}% | {ag*100:15.1f}% | {tvi:5.1f} | {tpi:5.1f}\")\n",
        "\n",
        "plt.figure(figsize=(6, 3.5))\n",
        "plt.plot([r[0] for r in rows], [r[1] for r in rows], marker=\"o\", label=\"VI success\")\n",
        "plt.plot([r[0] for r in rows], [r[2] for r in rows], marker=\"s\", label=\"PI success\")\n",
        "plt.xlabel(\"slip_prob\")\n",
        "plt.ylabel(\"success rate\")\n",
        "plt.title(f\"Robustness to stochasticity | {stress_map_name}\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0260b2d3",
      "metadata": {},
      "source": [
        "## Part 4 - FrozenLake validation and Gymnasium interface\n",
        "\n",
        "This section moves the same DP ideas from custom GridWorld into a standard Gymnasium environment: **FrozenLake-v1**.\n",
        "\n",
        "Goal:\n",
        "- show how Gymnasium environments are structured,\n",
        "- convert FrozenLake into tabular MDP form,\n",
        "- run Value Iteration and Policy Iteration,\n",
        "- compare deterministic and slippery dynamics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46176a11",
      "metadata": {},
      "source": [
        "### Gymnasium quick tips (interface essentials)\n",
        "\n",
        "Core API pattern:\n",
        "- `env = gym.make(...)`\n",
        "- `obs, info = env.reset(seed=...)`\n",
        "- `obs, reward, terminated, truncated, info = env.step(action)`\n",
        "\n",
        "Important fields:\n",
        "- `obs`: current observation (for FrozenLake: discrete state id)\n",
        "- `reward`: scalar reward from the transition\n",
        "- `terminated`: true terminal condition of the task\n",
        "- `truncated`: episode stopped by external limit (e.g., time limit)\n",
        "- `info`: optional debug metadata\n",
        "\n",
        "For tabular planning in toy-text environments, `env.unwrapped.P` exposes transition dynamics in the form:\n",
        "- `P[s][a] = [(prob, s_next, reward, done), ...]`\n",
        "\n",
        "FrozenLake action indexing in Gymnasium:\n",
        "- `0: Left`, `1: Down`, `2: Right`, `3: Up`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c600db09",
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    import gymnasium as gym\n",
        "except Exception:\n",
        "    gym = None\n",
        "\n",
        "if gym is None:\n",
        "    print(\"Gymnasium is not installed. Install with: pip install gymnasium[toy-text]\")\n",
        "else:\n",
        "    print(\"Gymnasium import successful.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f410d7d7",
      "metadata": {},
      "source": [
        "### Task 5 (10-20 min) - Transfer to FrozenLake with Gymnasium\n",
        "\n",
        "We build an adapter from Gym's transition dictionary to `TabularMDP`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fc1c69e",
      "metadata": {},
      "outputs": [],
      "source": [
        "GYM_ACTION_SYMBOLS = {0: \"←\", 1: \"↓\", 2: \"→\", 3: \"↑\"}\n",
        "GYM_ACTION_VECTORS = {\n",
        "    0: (-0.35, 0.0),   # LEFT\n",
        "    1: (0.0, 0.35),    # DOWN\n",
        "    2: (0.35, 0.0),    # RIGHT\n",
        "    3: (0.0, -0.35),   # UP\n",
        "}\n",
        "\n",
        "\n",
        "def frozenlake_desc_to_grid(desc) -> np.ndarray:\n",
        "    # desc is byte array in Gymnasium toy-text envs\n",
        "    arr = np.array(\n",
        "        [[ch.decode(\"utf-8\") if isinstance(ch, (bytes, bytearray)) else str(ch) for ch in row] for row in desc],\n",
        "        dtype=\"<U1\",\n",
        "    )\n",
        "    return arr\n",
        "\n",
        "\n",
        "def mdp_from_frozenlake_env(env) -> TabularMDP:\n",
        "    p_raw = env.unwrapped.P\n",
        "    nS = env.observation_space.n\n",
        "    nA = env.action_space.n\n",
        "\n",
        "    desc = frozenlake_desc_to_grid(env.unwrapped.desc)\n",
        "    H, W = desc.shape\n",
        "\n",
        "    state_to_pos = {s: (s // W, s % W) for s in range(nS)}\n",
        "    pos_to_state = {(s // W, s % W): s for s in range(nS)}\n",
        "\n",
        "    terminal_states = set()\n",
        "    start_state = 0\n",
        "\n",
        "    for s, (r, c) in state_to_pos.items():\n",
        "        cell = desc[r, c]\n",
        "        if cell in (\"H\", \"G\"):\n",
        "            terminal_states.add(s)\n",
        "        if cell == \"S\":\n",
        "            start_state = s\n",
        "\n",
        "    P = {s: {a: [] for a in range(nA)} for s in range(nS)}\n",
        "    for s in range(nS):\n",
        "        for a in range(nA):\n",
        "            for prob, s_next, reward, done in p_raw[s][a]:\n",
        "                P[s][a].append((float(prob), int(s_next), float(reward), bool(done)))\n",
        "\n",
        "    return TabularMDP(\n",
        "        nS=nS,\n",
        "        nA=nA,\n",
        "        P=P,\n",
        "        state_to_pos=state_to_pos,\n",
        "        pos_to_state=pos_to_state,\n",
        "        terminal_states=terminal_states,\n",
        "        start_state=start_state,\n",
        "        grid_chars=desc,\n",
        "    )\n",
        "\n",
        "\n",
        "def render_frozenlake_map(mdp: TabularMDP, title: str = \"FrozenLake map\"):\n",
        "    code_map = {\"S\": 0, \"F\": 1, \"H\": 2, \"G\": 3}\n",
        "    arr = np.vectorize(code_map.get)(mdp.grid_chars)\n",
        "    cmap = ListedColormap([\"#A7D3F5\", \"#EAF6FF\", \"#F8B4B4\", \"#B7E4C7\"])\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(5, 5))\n",
        "    ax.imshow(arr, cmap=cmap, vmin=0, vmax=3)\n",
        "    ax.set_title(title)\n",
        "    ax.set_xticks(range(arr.shape[1]))\n",
        "    ax.set_yticks(range(arr.shape[0]))\n",
        "    _draw_cell_grid(ax, arr.shape[0], arr.shape[1])\n",
        "\n",
        "    for r in range(arr.shape[0]):\n",
        "        for c in range(arr.shape[1]):\n",
        "            ax.text(c, r, mdp.grid_chars[r, c], ha=\"center\", va=\"center\", color=\"black\", fontsize=12, fontweight=\"bold\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def render_frozenlake_policy(mdp: TabularMDP, actions: np.ndarray, title: str = \"FrozenLake policy\"):\n",
        "    bg = np.zeros(mdp.grid_chars.shape)\n",
        "    fig, ax = plt.subplots(figsize=(5, 5))\n",
        "    ax.imshow(bg, cmap=ListedColormap([\"#f7f7f7\"]))\n",
        "    ax.set_title(title)\n",
        "    ax.set_xticks(range(bg.shape[1]))\n",
        "    ax.set_yticks(range(bg.shape[0]))\n",
        "    _draw_cell_grid(ax, bg.shape[0], bg.shape[1])\n",
        "\n",
        "    for s, (r, c) in mdp.state_to_pos.items():\n",
        "        cell = mdp.grid_chars[r, c]\n",
        "        if cell in (\"H\", \"G\"):\n",
        "            txt = cell\n",
        "        elif cell == \"F\":\n",
        "            txt = \" \"\n",
        "        else:\n",
        "            txt = GYM_ACTION_SYMBOLS[int(actions[s])]\n",
        "        ax.text(c, r, txt, ha=\"center\", va=\"center\", color=\"black\", fontsize=12, fontweight=\"bold\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def save_frozenlake_policy_iteration_gif(\n",
        "    mdp: TabularMDP,\n",
        "    history: list,\n",
        "    gif_path: Path,\n",
        "    title_prefix: str = \"FrozenLake PI\",\n",
        "    show_after_save: bool = True,\n",
        "):\n",
        "    if not history:\n",
        "        print(\"No history available; GIF was not created.\")\n",
        "        return\n",
        "\n",
        "    from matplotlib.animation import FuncAnimation, PillowWriter\n",
        "\n",
        "    gif_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    value_grids = [values_to_grid(mdp, h[\"V\"]) for h in history]\n",
        "\n",
        "    # policy per outer iteration\n",
        "    action_grids = []\n",
        "    for h in history:\n",
        "        a = np.argmax(h[\"policy\"], axis=1)\n",
        "        g = np.full(mdp.grid_chars.shape, -1, dtype=int)\n",
        "        for s, (r, c) in mdp.state_to_pos.items():\n",
        "            if mdp.grid_chars[r, c] not in (\"G\", \"H\"):\n",
        "                g[r, c] = a[s]\n",
        "        action_grids.append(g)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(5, 5))\n",
        "    im = ax.imshow(value_grids[0], cmap=\"viridis\")\n",
        "    txt = ax.set_title(f\"{title_prefix} | outer=1\")\n",
        "    ax.set_xticks(range(value_grids[0].shape[1]))\n",
        "    ax.set_yticks(range(value_grids[0].shape[0]))\n",
        "    _draw_cell_grid(ax, value_grids[0].shape[0], value_grids[0].shape[1])\n",
        "\n",
        "    q = None\n",
        "\n",
        "    def update(frame_idx):\n",
        "        nonlocal q\n",
        "        im.set_data(value_grids[frame_idx])\n",
        "\n",
        "        if q is not None:\n",
        "            q.remove()\n",
        "\n",
        "        X, Y, U, V = [], [], [], []\n",
        "        actions = action_grids[frame_idx]\n",
        "        for r in range(actions.shape[0]):\n",
        "            for c in range(actions.shape[1]):\n",
        "                if mdp.grid_chars[r, c] in (\"#\", \"G\", \"H\"):\n",
        "                    continue\n",
        "                a = actions[r, c]\n",
        "                if a < 0:\n",
        "                    continue\n",
        "                u, v = GYM_ACTION_VECTORS[a]\n",
        "                X.append(c); Y.append(r); U.append(u); V.append(v)\n",
        "\n",
        "        if X:\n",
        "            q = ax.quiver(X, Y, U, V, angles=\"xy\", scale_units=\"xy\", scale=1, color=\"white\", width=0.008)\n",
        "\n",
        "        outer = history[frame_idx][\"outer\"]\n",
        "        txt.set_text(f\"{title_prefix} | outer={outer}\")\n",
        "        return (im, txt)\n",
        "\n",
        "    ani = FuncAnimation(fig, update, frames=len(history), interval=550, blit=False)\n",
        "    ani.save(gif_path, writer=PillowWriter(fps=2))\n",
        "    plt.close(fig)\n",
        "    print(f\"Saved GIF: {gif_path}\")\n",
        "\n",
        "    if show_after_save:\n",
        "        display(Image(filename=str(gif_path)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48a651c4",
      "metadata": {},
      "source": [
        "### FrozenLake experiment A: deterministic vs slippery (4x4)\n",
        "\n",
        "We compare:\n",
        "- `is_slippery=False` (deterministic transitions)\n",
        "- `is_slippery=True` (stochastic transitions)\n",
        "\n",
        "for both Value Iteration and Policy Iteration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0f2204d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_vi_pi_on_frozenlake(map_name: str = \"4x4\", is_slippery: bool = True, gamma: float = 0.99, theta: float = 1e-10):\n",
        "    env = gym.make(\"FrozenLake-v1\", map_name=map_name, is_slippery=is_slippery)\n",
        "    mdp = mdp_from_frozenlake_env(env)\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    vi = value_iteration(mdp, gamma=gamma, theta=theta, return_history=True)\n",
        "    t_vi_ms = (time.perf_counter() - t0) * 1e3\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    pi = policy_iteration(mdp, gamma=gamma, eval_theta=theta, return_history=True)\n",
        "    t_pi_ms = (time.perf_counter() - t0) * 1e3\n",
        "\n",
        "    vi_actions = vi[\"greedy_actions\"]\n",
        "    pi_actions = np.argmax(pi[\"policy\"], axis=1)\n",
        "\n",
        "    agreement = compare_policies(vi_actions, pi_actions, mdp.terminal_states)\n",
        "    value_linf = float(np.max(np.abs(vi[\"V\"] - pi[\"V\"])))\n",
        "\n",
        "    # Quick PI consistency check: final policy should be greedy w.r.t. returned V\n",
        "    pi_greedy_from_V = np.argmax(greedy_policy_from_values(mdp, pi[\"V\"], gamma=gamma), axis=1)\n",
        "    pi_consistent = np.array_equal(pi_actions, pi_greedy_from_V)\n",
        "\n",
        "    return {\n",
        "        \"env\": env,\n",
        "        \"mdp\": mdp,\n",
        "        \"vi\": vi,\n",
        "        \"pi\": pi,\n",
        "        \"t_vi_ms\": t_vi_ms,\n",
        "        \"t_pi_ms\": t_pi_ms,\n",
        "        \"agreement\": agreement,\n",
        "        \"value_linf\": value_linf,\n",
        "        \"pi_consistent\": pi_consistent,\n",
        "    }\n",
        "\n",
        "\n",
        "def evaluate_policy_in_gym(env, actions: np.ndarray, n_episodes: int = 500, max_steps: int = 200, seed: int = 123):\n",
        "    returns = []\n",
        "    success = 0\n",
        "\n",
        "    for ep in range(n_episodes):\n",
        "        obs, _ = env.reset(seed=seed + ep)\n",
        "        total_r = 0.0\n",
        "\n",
        "        for _ in range(max_steps):\n",
        "            a = int(actions[obs])\n",
        "            obs, reward, terminated, truncated, _ = env.step(a)\n",
        "            total_r += reward\n",
        "            if terminated or truncated:\n",
        "                if reward > 0:\n",
        "                    success += 1\n",
        "                break\n",
        "\n",
        "        returns.append(total_r)\n",
        "\n",
        "    return float(np.mean(returns)), success / n_episodes\n",
        "\n",
        "\n",
        "if gym is None:\n",
        "    print(\"Skip: Gymnasium not available.\")\n",
        "else:\n",
        "    fl_det = run_vi_pi_on_frozenlake(map_name=\"4x4\", is_slippery=False)\n",
        "    fl_slip = run_vi_pi_on_frozenlake(map_name=\"4x4\", is_slippery=True)\n",
        "\n",
        "    for name, data in [(\"4x4 deterministic\", fl_det), (\"4x4 slippery\", fl_slip)]:\n",
        "        mdp = data[\"mdp\"]\n",
        "        vi = data[\"vi\"]\n",
        "        pi = data[\"pi\"]\n",
        "\n",
        "        print(f\"\\n{name}\")\n",
        "        print(f\"  nS={mdp.nS}, VI_sweeps={vi['sweeps']}, PI_outer={pi['outer_loops']}, PI_eval_total={sum(pi['eval_sweeps'])}\")\n",
        "        print(f\"  VI_ms={data['t_vi_ms']:.2f}, PI_ms={data['t_pi_ms']:.2f}\")\n",
        "        print(f\"  L_inf(VI-PI)={data['value_linf']:.3e}, policy_agreement={data['agreement']*100:.1f}%\")\n",
        "        print(f\"  PI greedy-consistency check: {data['pi_consistent']}\")\n",
        "\n",
        "        vi_ret, vi_succ = evaluate_policy_in_gym(data[\"env\"], vi[\"greedy_actions\"], n_episodes=500)\n",
        "        pi_ret, pi_succ = evaluate_policy_in_gym(data[\"env\"], np.argmax(pi[\"policy\"], axis=1), n_episodes=500)\n",
        "        print(f\"  VI eval: mean_return={vi_ret:.3f}, success_rate={vi_succ*100:.1f}%\")\n",
        "        print(f\"  PI eval: mean_return={pi_ret:.3f}, success_rate={pi_succ*100:.1f}%\")\n",
        "\n",
        "        render_frozenlake_map(mdp, title=f\"FrozenLake map | {name}\")\n",
        "        plot_value_heatmap(mdp, vi[\"V\"], title=f\"VI value heatmap | {name}\")\n",
        "        render_frozenlake_policy(mdp, vi[\"greedy_actions\"], title=f\"VI greedy policy | {name}\")\n",
        "        render_frozenlake_policy(mdp, np.argmax(pi[\"policy\"], axis=1), title=f\"PI greedy policy | {name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eec18e31",
      "metadata": {},
      "source": [
        "### FrozenLake experiment B: 8x8 slippery\n",
        "\n",
        "This gives a larger Gym benchmark with the same algorithms.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c172f5d",
      "metadata": {},
      "outputs": [],
      "source": [
        "if gym is None:\n",
        "    print(\"Skip: Gymnasium not available.\")\n",
        "else:\n",
        "    fl8 = run_vi_pi_on_frozenlake(map_name=\"8x8\", is_slippery=True, gamma=0.99, theta=1e-10)\n",
        "\n",
        "    mdp = fl8[\"mdp\"]\n",
        "    vi = fl8[\"vi\"]\n",
        "    pi = fl8[\"pi\"]\n",
        "\n",
        "    print(\"8x8 slippery\")\n",
        "    print(f\"  nS={mdp.nS}, VI_sweeps={vi['sweeps']}, PI_outer={pi['outer_loops']}, PI_eval_total={sum(pi['eval_sweeps'])}\")\n",
        "    print(f\"  VI_ms={fl8['t_vi_ms']:.2f}, PI_ms={fl8['t_pi_ms']:.2f}\")\n",
        "    print(f\"  L_inf(VI-PI)={fl8['value_linf']:.3e}, policy_agreement={fl8['agreement']*100:.1f}%\")\n",
        "\n",
        "    vi_ret, vi_succ = evaluate_policy_in_gym(fl8[\"env\"], vi[\"greedy_actions\"], n_episodes=300)\n",
        "    pi_ret, pi_succ = evaluate_policy_in_gym(fl8[\"env\"], np.argmax(pi[\"policy\"], axis=1), n_episodes=300)\n",
        "    print(f\"  VI eval: mean_return={vi_ret:.3f}, success_rate={vi_succ*100:.1f}%\")\n",
        "    print(f\"  PI eval: mean_return={pi_ret:.3f}, success_rate={pi_succ*100:.1f}%\")\n",
        "\n",
        "    plot_value_heatmap(mdp, vi[\"V\"], title=\"VI value heatmap | FrozenLake 8x8 slippery\")\n",
        "    render_frozenlake_policy(mdp, vi[\"greedy_actions\"], title=\"VI greedy policy | FrozenLake 8x8 slippery\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2e97dcb",
      "metadata": {},
      "source": [
        "### FrozenLake GIFs (time-evolving views)\n",
        "\n",
        "We export:\n",
        "- Value Iteration value evolution GIF,\n",
        "- Policy Iteration evolution GIF,\n",
        "- greedy rollout trajectory GIF.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab4cee7c",
      "metadata": {},
      "outputs": [],
      "source": [
        "if gym is None:\n",
        "    print(\"Skip: Gymnasium not available.\")\n",
        "else:\n",
        "    out_dir = Path(\"notebooks/sessions/session_02_mdp_dynamic_programming/assets/web_outputs\")\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    frozenlake_cases = {\n",
        "        \"fl4_det\": fl_det,\n",
        "        \"fl4_slip\": fl_slip,\n",
        "    }\n",
        "\n",
        "    for tag, data in frozenlake_cases.items():\n",
        "        mdp = data[\"mdp\"]\n",
        "        vi = data[\"vi\"]\n",
        "        pi = data[\"pi\"]\n",
        "\n",
        "        gif_vi_value = out_dir / f\"session2_{tag}_vi_value.gif\"\n",
        "        gif_pi_evo = out_dir / f\"session2_{tag}_pi_evolution.gif\"\n",
        "        gif_vi_traj = out_dir / f\"session2_{tag}_vi_traj.gif\"\n",
        "        gif_pi_traj = out_dir / f\"session2_{tag}_pi_traj.gif\"\n",
        "\n",
        "        save_vi_value_gif(\n",
        "            mdp,\n",
        "            vi[\"history\"],\n",
        "            gif_vi_value,\n",
        "            title_prefix=f\"VI value evolution ({tag})\",\n",
        "            show_after_save=True,\n",
        "        )\n",
        "        save_frozenlake_policy_iteration_gif(\n",
        "            mdp,\n",
        "            pi[\"history\"],\n",
        "            gif_pi_evo,\n",
        "            title_prefix=f\"FrozenLake PI evolution ({tag})\",\n",
        "            show_after_save=True,\n",
        "        )\n",
        "\n",
        "        # VI greedy trajectory from Gym rollouts projected to state ids\n",
        "        env = data[\"env\"]\n",
        "        obs, _ = env.reset(seed=7)\n",
        "        states_vi = [int(obs)]\n",
        "        for _ in range(120):\n",
        "            a = int(vi[\"greedy_actions\"][obs])\n",
        "            obs, _, terminated, truncated, _ = env.step(a)\n",
        "            states_vi.append(int(obs))\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "\n",
        "        # PI greedy trajectory from Gym rollouts projected to state ids\n",
        "        obs, _ = env.reset(seed=11)\n",
        "        pi_actions = np.argmax(pi[\"policy\"], axis=1)\n",
        "        states_pi = [int(obs)]\n",
        "        for _ in range(120):\n",
        "            a = int(pi_actions[obs])\n",
        "            obs, _, terminated, truncated, _ = env.step(a)\n",
        "            states_pi.append(int(obs))\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "\n",
        "        # Lightweight adapter for trajectory renderer\n",
        "        class _Adapter:\n",
        "            pass\n",
        "\n",
        "        adapter = _Adapter()\n",
        "        adapter.grid = np.where(mdp.grid_chars == \"F\", \".\", mdp.grid_chars)\n",
        "        adapter.H, adapter.W = adapter.grid.shape\n",
        "        adapter.state_to_pos = mdp.state_to_pos\n",
        "\n",
        "        save_trajectory_gif(\n",
        "            adapter,\n",
        "            states_vi,\n",
        "            gif_vi_traj,\n",
        "            title=f\"VI greedy trajectory ({tag})\",\n",
        "            trail_len=20,\n",
        "            show_after_save=True,\n",
        "        )\n",
        "        save_trajectory_gif(\n",
        "            adapter,\n",
        "            states_pi,\n",
        "            gif_pi_traj,\n",
        "            title=f\"PI greedy trajectory ({tag})\",\n",
        "            trail_len=20,\n",
        "            show_after_save=True,\n",
        "        )\n",
        "\n",
        "    print(f\"FrozenLake GIFs saved in: {out_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f6657a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from IPython.display import display, Image as IPyImage\n",
        "import imageio.v2 as imageio\n",
        "from pathlib import Path\n",
        "\n",
        "# 1) Build env + MDP\n",
        "env_train = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True)\n",
        "mdp_fl = mdp_from_frozenlake_env(env_train)\n",
        "\n",
        "# 2) Train with YOUR Policy Iteration\n",
        "pi_res = policy_iteration(mdp_fl, gamma=0.99, eval_theta=1e-10, return_history=True)\n",
        "pi_actions = np.argmax(pi_res[\"policy\"], axis=1)  # learned greedy policy\n",
        "\n",
        "print(f\"PI outer loops: {pi_res['outer_loops']}, eval sweeps total: {sum(pi_res['eval_sweeps'])}\")\n",
        "\n",
        "# 3) Render rollout with learned policy\n",
        "env_render = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=True, render_mode=\"rgb_array\")\n",
        "obs, info = env_render.reset(seed=7)\n",
        "\n",
        "frames = [env_render.render()]\n",
        "for _ in range(100):\n",
        "    a = int(pi_actions[obs])              # <- learned policy action\n",
        "    obs, reward, terminated, truncated, info = env_render.step(a)\n",
        "    frames.append(env_render.render())\n",
        "    if terminated or truncated:\n",
        "        break\n",
        "\n",
        "print(\"Episode finished. Final reward:\", reward)\n",
        "\n",
        "# Show last frame\n",
        "display(Image.fromarray(frames[-1]))\n",
        "\n",
        "# 4) Save + show GIF\n",
        "out_dir = Path(\"notebooks/sessions/session_02_mdp_dynamic_programming/assets/web_outputs\")\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "gif_path = out_dir / \"frozenlake_pi_policy_rollout.gif\"\n",
        "imageio.mimsave(gif_path, frames, duration=0.22)\n",
        "display(IPyImage(filename=str(gif_path)))\n",
        "print(\"Saved:\", gif_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbcacbe3",
      "metadata": {},
      "source": [
        "**FrozenLake GIF outputs** are saved under `notebooks/sessions/session_02_mdp_dynamic_programming/assets/web_outputs/`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a8f95a4",
      "metadata": {},
      "source": [
        "## Gambler's Problem (tabular MDP example)\n",
        "\n",
        "Before moving to homework, we add one more classic DP example from Sutton & Barto: **Gambler's Problem**.\n",
        "\n",
        "Setup:\n",
        "- state $s \\in \\{0,1,\\dots,100\\}$ is current capital,\n",
        "- terminal states: $0$ (loss) and $100$ (goal),\n",
        "- action $a$ is the stake, with $a \\in \\{1,\\dots,\\min(s,100-s)\\}$,\n",
        "- with probability $p_h$ the coin is heads and capital increases by $a$,\n",
        "- with probability $1-p_h$ capital decreases by $a$,\n",
        "- reward is $1$ only when transitioning into state $100$.\n",
        "\n",
        "This is a pure planning problem, solved here with Value Iteration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "002d8b80",
      "metadata": {},
      "source": [
        "### Extension - Implement Value Iteration for Gambler's Problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "689ea125",
      "metadata": {},
      "outputs": [],
      "source": [
        "def gambler_value_iteration(\n",
        "    p_heads: float = 0.5,\n",
        "    theta: float = 1e-12,\n",
        "    gamma: float = 1.0,\n",
        "    target: int = 100,\n",
        "):\n",
        "    V = np.zeros(target + 1, dtype=float)\n",
        "    policy = np.zeros(target + 1, dtype=int)\n",
        "    history = []\n",
        "    deltas = []\n",
        "\n",
        "    sweeps = 0\n",
        "    while True:\n",
        "        delta = 0.0\n",
        "        V_old = V.copy()\n",
        "\n",
        "        for s in range(1, target):\n",
        "            max_stake = min(s, target - s)\n",
        "            action_values = np.zeros(max_stake + 1, dtype=float)\n",
        "\n",
        "            for a in range(1, max_stake + 1):\n",
        "                s_win = s + a\n",
        "                s_lose = s - a\n",
        "                reward_win = 1.0 if s_win == target else 0.0\n",
        "\n",
        "                action_values[a] = (\n",
        "                    p_heads * (reward_win + gamma * V_old[s_win])\n",
        "                    + (1.0 - p_heads) * gamma * V_old[s_lose]\n",
        "                )\n",
        "\n",
        "            best_a = int(np.argmax(action_values))\n",
        "            best_v = float(action_values[best_a])\n",
        "\n",
        "            V[s] = best_v\n",
        "            policy[s] = best_a\n",
        "            delta = max(delta, abs(V[s] - V_old[s]))\n",
        "\n",
        "        history.append(V.copy())\n",
        "        deltas.append(delta)\n",
        "        sweeps += 1\n",
        "\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "    return {\n",
        "        \"V\": V,\n",
        "        \"policy\": policy,\n",
        "        \"history\": history,\n",
        "        \"deltas\": deltas,\n",
        "        \"sweeps\": sweeps,\n",
        "        \"p_heads\": p_heads,\n",
        "        \"theta\": theta,\n",
        "        \"gamma\": gamma,\n",
        "    }\n",
        "\n",
        "\n",
        "gambler_res = gambler_value_iteration(p_heads=0.5, theta=1e-12, gamma=1.0)\n",
        "print(\n",
        "    f\"Converged in {gambler_res['sweeps']} sweeps | \"\n",
        "    f\"final delta={gambler_res['deltas'][-1]:.3e}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b7c1c50",
      "metadata": {},
      "source": [
        "### Extension - Visualize value function and optimal policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80d05777",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_gambler_values_and_policy(result: dict, title_prefix: str = \"Gambler\"):\n",
        "    V = result[\"V\"]\n",
        "    policy = result[\"policy\"]\n",
        "    target = len(V) - 1\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(np.arange(1, target), V[1:target], linewidth=2)\n",
        "    plt.xlabel(\"Capital\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.title(f\"{title_prefix}: Optimal value function\")\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.bar(np.arange(1, target), policy[1:target])\n",
        "    plt.xlabel(\"Capital\")\n",
        "    plt.ylabel(\"Stake\")\n",
        "    plt.title(f\"{title_prefix}: Greedy stake policy\")\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_gambler_convergence(result: dict, title: str = \"Gambler Value Iteration convergence\"):\n",
        "    deltas = result[\"deltas\"]\n",
        "    plt.figure(figsize=(7, 3.5))\n",
        "    plt.plot(deltas)\n",
        "    plt.yscale(\"log\")\n",
        "    plt.xlabel(\"Sweep\")\n",
        "    plt.ylabel(\"max update delta\")\n",
        "    plt.title(title)\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_gambler_values_and_policy(gambler_res, title_prefix=\"Gambler (p_heads=0.4)\")\n",
        "plot_gambler_convergence(gambler_res)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a394707",
      "metadata": {},
      "source": [
        "### Extension - Value-evolution snapshots and GIF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f8520f0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_gambler_snapshots(result: dict, k_list=None):\n",
        "    hist = result[\"history\"]\n",
        "    if not hist:\n",
        "        return\n",
        "\n",
        "    if k_list is None:\n",
        "        last = len(hist) - 1\n",
        "        mid = last // 2\n",
        "        k_list = sorted(set([0, mid, last]))\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    for k in k_list:\n",
        "        plt.plot(np.arange(1, len(hist[k]) - 1), hist[k][1:-1], label=f\"sweep={k+1}\")\n",
        "    plt.xlabel(\"Capital\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.title(\"Gambler value evolution snapshots\")\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def save_gambler_value_gif(result: dict, gif_path: Path, title_prefix: str = \"Gambler value evolution\"):\n",
        "    hist = result[\"history\"]\n",
        "    if not hist:\n",
        "        print(\"No history available; GIF was not created.\")\n",
        "        return\n",
        "\n",
        "    from matplotlib.animation import FuncAnimation, PillowWriter\n",
        "\n",
        "    gif_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    x = np.arange(1, len(hist[0]) - 1)\n",
        "    fig, ax = plt.subplots(figsize=(9, 4))\n",
        "    line, = ax.plot([], [], linewidth=2)\n",
        "    ax.set_xlim(x[0], x[-1])\n",
        "    ax.set_ylim(0.0, max(float(np.max(v)) for v in hist) * 1.05)\n",
        "    ax.set_xlabel(\"Capital\")\n",
        "    ax.set_ylabel(\"Value\")\n",
        "    title = ax.set_title(f\"{title_prefix} | sweep=1\")\n",
        "    ax.grid(alpha=0.3)\n",
        "\n",
        "    def update(i):\n",
        "        y = hist[i][1:-1]\n",
        "        line.set_data(x, y)\n",
        "        title.set_text(f\"{title_prefix} | sweep={i+1}\")\n",
        "        return (line, title)\n",
        "\n",
        "    ani = FuncAnimation(fig, update, frames=len(hist), interval=160, blit=False)\n",
        "    ani.save(gif_path, writer=PillowWriter(fps=8))\n",
        "    plt.close(fig)\n",
        "    print(f\"Saved GIF: {gif_path}\")\n",
        "\n",
        "\n",
        "plot_gambler_snapshots(gambler_res)\n",
        "\n",
        "out_dir = Path(\"notebooks/sessions/session_02_mdp_dynamic_programming/assets/web_outputs\")\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "save_gambler_value_gif(gambler_res, out_dir / \"session2_gambler_value_evolution.gif\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1d37f60",
      "metadata": {},
      "source": [
        "### Extension - Probability sensitivity experiments\n",
        "\n",
        "We compare $p_h \\in \\{0.25, 0.40, 0.55\\}$ and inspect how value and policy structure changes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1cf8b18",
      "metadata": {},
      "outputs": [],
      "source": [
        "p_list = [0.25, 0.40, 0.55]\n",
        "results = {p: gambler_value_iteration(p_heads=p, theta=1e-12, gamma=1.0) for p in p_list}\n",
        "\n",
        "print(\"p_heads | sweeps | V(50) | suggested stake at s=50\")\n",
        "for p in p_list:\n",
        "    r = results[p]\n",
        "    print(f\"{p:6.2f} | {r['sweeps']:6d} | {r['V'][50]:6.3f} | {r['policy'][50]:24d}\")\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "for p in p_list:\n",
        "    r = results[p]\n",
        "    plt.plot(np.arange(1, 100), r[\"V\"][1:100], label=f\"p_heads={p:.2f}\")\n",
        "plt.xlabel(\"Capital\")\n",
        "plt.ylabel(\"Value\")\n",
        "plt.title(\"Gambler value function sensitivity to coin bias\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "for p in p_list:\n",
        "    r = results[p]\n",
        "    plt.step(np.arange(1, 100), r[\"policy\"][1:100], where=\"mid\", label=f\"p_heads={p:.2f}\")\n",
        "plt.xlabel(\"Capital\")\n",
        "plt.ylabel(\"Stake\")\n",
        "plt.title(\"Gambler policy sensitivity to coin bias\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7cd5cca",
      "metadata": {},
      "source": [
        "**Gambler GIF output** is saved under `notebooks/sessions/session_02_mdp_dynamic_programming/assets/web_outputs/`.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "drl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
