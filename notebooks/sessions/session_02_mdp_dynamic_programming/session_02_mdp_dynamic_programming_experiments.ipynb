{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Logo](https://raw.githubusercontent.com/BartaZoltan/deep-reinforcement-learning-course/main/notebooks/shared_assets/logo.png)\n\nMade by **Zoltán Barta**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cb58cfd",
      "metadata": {},
      "source": [
        "# 2. alkalom – Dinamikus programozási kísérletek\n",
        "\n",
        "Ez a notebook kifejezetten az előző foglalkozáson implementált algoritmusokhoz kapcsolódó kísérletekhez készült. A cél a paraméterek érzékenységének vizsgálata és a különböző környezetek összehasonlítása. Az alapvető definíciókat az előző notebook tartalmazza, itt a hangsúly a gyakorlati vizsgálatokon van.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6503a6d0",
      "metadata": {},
      "source": [
        "## Szükséges eszközök beimportálása\n",
        "\n",
        "Töltsük be a szükséges függvényeket és definiáljuk a környezetet. Célszerű az előző notebookból importálható módon egy `gridworld_utils.py` modulba szervezni a kódot. Itt azonban közvetlenül bemásoljuk a függvényeket.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fa93108",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from typing import Dict, Tuple, List\n",
        "\n",
        "# A Gridworld, policy_evaluation, policy_iteration és value_iteration függvények legyenek elérhetők\n",
        "# Ha külön modulban tárolnánk őket, importálhatók lennének. Itt ismét lemásoljuk őket.\n",
        "class Gridworld:\n",
        "    def __init__(self, rows: int, cols: int, terminals: List[Tuple[int, int]],\n",
        "                 slip_prob: float = 0.0, default_reward: float = -1.0):\n",
        "        self.rows = rows\n",
        "        self.cols = cols\n",
        "        self.terminals = terminals\n",
        "        self.slip_prob = slip_prob\n",
        "        self.default_reward = default_reward\n",
        "        self.actions = ['U', 'D', 'L', 'R']\n",
        "\n",
        "    def in_bounds(self, state):\n",
        "        r, c = state\n",
        "        return 0 <= r < self.rows and 0 <= c < self.cols\n",
        "\n",
        "    def step(self, state, action):\n",
        "        if state in self.terminals:\n",
        "            return state, 0.0\n",
        "        actual_action = action\n",
        "        if self.slip_prob > 0 and random.random() < self.slip_prob:\n",
        "            actual_action = random.choice([a for a in self.actions if a != action])\n",
        "        r, c = state\n",
        "        if actual_action == 'U':\n",
        "            new_state = (max(r - 1, 0), c)\n",
        "        elif actual_action == 'D':\n",
        "            new_state = (min(r + 1, self.rows - 1), c)\n",
        "        elif actual_action == 'L':\n",
        "            new_state = (r, max(c - 1, 0))\n",
        "        elif actual_action == 'R':\n",
        "            new_state = (r, min(c + 1, self.cols - 1))\n",
        "        else:\n",
        "            new_state = state\n",
        "        reward = 0.0 if new_state in self.terminals else self.default_reward\n",
        "        return new_state, reward\n",
        "\n",
        "def policy_evaluation(policy, env, gamma=1.0, theta=1e-4):\n",
        "    V = { (r, c): 0.0 for r in range(env.rows) for c in range(env.cols) }\n",
        "    for t in env.terminals:\n",
        "        V[t] = 0.0\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for r in range(env.rows):\n",
        "            for c in range(env.cols):\n",
        "                state = (r, c)\n",
        "                if state in env.terminals:\n",
        "                    continue\n",
        "                a = policy[state]\n",
        "                next_state, reward = env.step(state, a)\n",
        "                v_new = reward + gamma * V[next_state]\n",
        "                delta = max(delta, abs(v_new - V[state]))\n",
        "                V[state] = v_new\n",
        "        if delta < theta:\n",
        "            break\n",
        "    return V\n",
        "\n",
        "def policy_iteration(env, gamma=1.0, theta=1e-4):\n",
        "    policy = {}\n",
        "    for r in range(env.rows):\n",
        "        for c in range(env.cols):\n",
        "            if (r, c) not in env.terminals:\n",
        "                policy[(r, c)] = random.choice(env.actions)\n",
        "    iteration = 0\n",
        "    while True:\n",
        "        iteration += 1\n",
        "        V = policy_evaluation(policy, env, gamma, theta)\n",
        "        stable = True\n",
        "        for r in range(env.rows):\n",
        "            for c in range(env.cols):\n",
        "                state = (r, c)\n",
        "                if state in env.terminals:\n",
        "                    continue\n",
        "                old_action = policy[state]\n",
        "                action_values = {}\n",
        "                for a in env.actions:\n",
        "                    next_state, reward = env.step(state, a)\n",
        "                    action_values[a] = reward + gamma * V[next_state]\n",
        "                best_action = max(action_values, key=action_values.get)\n",
        "                policy[state] = best_action\n",
        "                if best_action != old_action:\n",
        "                    stable = False\n",
        "        if stable:\n",
        "            break\n",
        "    return V, policy, iteration\n",
        "\n",
        "def value_iteration(env, gamma=1.0, theta=1e-4):\n",
        "    V = { (r, c): 0.0 for r in range(env.rows) for c in range(env.cols) }\n",
        "    for t in env.terminals:\n",
        "        V[t] = 0.0\n",
        "    iteration = 0\n",
        "    while True:\n",
        "        iteration += 1\n",
        "        delta = 0\n",
        "        for r in range(env.rows):\n",
        "            for c in range(env.cols):\n",
        "                state = (r, c)\n",
        "                if state in env.terminals:\n",
        "                    continue\n",
        "                action_values = []\n",
        "                for a in env.actions:\n",
        "                    next_state, reward = env.step(state, a)\n",
        "                    action_values.append(reward + gamma * V[next_state])\n",
        "                v_new = max(action_values)\n",
        "                delta = max(delta, abs(v_new - V[state]))\n",
        "                V[state] = v_new\n",
        "        if delta < theta:\n",
        "            break\n",
        "    policy = {}\n",
        "    for r in range(env.rows):\n",
        "        for c in range(env.cols):\n",
        "            state = (r, c)\n",
        "            if state in env.terminals:\n",
        "                continue\n",
        "            action_values = {}\n",
        "            for a in env.actions:\n",
        "                next_state, reward = env.step(state, a)\n",
        "                action_values[a] = reward + gamma * V[next_state]\n",
        "            policy[state] = max(action_values, key=action_values.get)\n",
        "    return V, policy, iteration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c98ce2d",
      "metadata": {},
      "source": [
        "## 1. γ és θ paraméterek részletes vizsgálata\n",
        "\n",
        "Ebben a szakaszban részletes hőtérképeket vagy grafikonokat készítünk a konvergencia sebességéről a diszkontráta és a konvergencia-küszöb függvényében. A bemeneti paraméterek listáit tetszés szerint bővíthetjük.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c91c7c9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import itertools\n",
        "\n",
        "def parameter_sweep(gammas, thetas, rows=4, cols=4):\n",
        "    data = []\n",
        "    terminal_states = [(rows - 1, cols - 1)]\n",
        "    for gamma, theta in itertools.product(gammas, thetas):\n",
        "        env = Gridworld(rows, cols, terminal_states)\n",
        "        _, _, it_pi = policy_iteration(env, gamma=gamma, theta=theta)\n",
        "        _, _, it_vi = value_iteration(env, gamma=gamma, theta=theta)\n",
        "        data.append({'gamma': gamma, 'theta': theta, 'policy_iter': it_pi, 'value_iter': it_vi})\n",
        "    df = pd.DataFrame(data)\n",
        "    return df\n",
        "\n",
        "# Példa paraméterkészlet\n",
        "gammas = [0.5, 0.7, 0.9, 0.99]\n",
        "thetas = [1e-1, 1e-2, 1e-3, 1e-4]\n",
        "\n",
        "df = parameter_sweep(gammas, thetas)\n",
        "df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa164358",
      "metadata": {},
      "source": [
        "Az így kapott DataFrame-ben jól látható, hogy a diszkontráta növelése általában több iterációt igényel. A konvergencia-küszöb csökkentése (például 1e-4 helyett 1e-1) jelentősen csökkenti az iterációk számát, de pontatlanabb értékfüggvényt eredményez. Használjunk `df.pivot_table` segítségével hőtérképet vagy grafikont is az adatok vizualizálására.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ab9bb68",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Hőtérkép készítése a value iteration iterációszámára\n",
        "pivot_vi = df.pivot_table(values='value_iter', index='gamma', columns='theta')\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(pivot_vi, annot=True, fmt='d', cmap='YlGnBu')\n",
        "plt.title('Értékiteráció iterációszám')\n",
        "plt.ylabel('gamma')\n",
        "plt.xlabel('theta')\n",
        "plt.show()\n",
        "\n",
        "# Hőtérkép készítése a policy iteration iterációszámára\n",
        "pivot_pi = df.pivot_table(values='policy_iter', index='gamma', columns='theta')\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(pivot_pi, annot=True, fmt='d', cmap='YlOrBr')\n",
        "plt.title('Policy iteráció iterációszám')\n",
        "plt.ylabel('gamma')\n",
        "plt.xlabel('theta')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea24bbd1",
      "metadata": {},
      "source": [
        "## 2. Nagyobb rácsok\n",
        "\n",
        "Készítsünk 5×5 és 6×6 méretű rácsokat, és vizsgáljuk meg, hogyan nő az iterációk száma. A fenti függvényekkel könnyen általánosíthatjuk a vizsgálatot, de a nagyobb állapottér miatt az algoritmusok lassulni fognak.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2607383a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Nagyobb rács vizsgálata 5x5 és 6x6 méretben\n",
        "sizes = [(5, 5), (6, 6)]\n",
        "for size in sizes:\n",
        "    rows, cols = size\n",
        "    terminal_states = [(rows - 1, cols - 1)]\n",
        "    env = Gridworld(rows, cols, terminal_states)\n",
        "    _, _, it_pi = policy_iteration(env, gamma=0.9, theta=1e-3)\n",
        "    _, _, it_vi = value_iteration(env, gamma=0.9, theta=1e-3)\n",
        "    print(f'{rows}x{cols} rács: Policy iteráció {it_pi} iteráció, Értékiteráció {it_vi} iteráció')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c72474a3",
      "metadata": {},
      "source": [
        "## 3. Stochasztikus környezet és csúszás\n",
        "\n",
        "A `slip_prob` paraméter beállításával sztochasztikus átmeneteket hozhatunk létre. Ilyenkor a step függvény különböző kimeneteket adhat, ezért az értékek kiszámításakor a várható értéket kell figyelembe venni. Egyszerűsített tanulmányozásához a slip valószínűséget 0,1 vagy 0,2 értékre állítjuk.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24c95e6d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def value_iteration_stochastic(env: Gridworld, gamma=1.0, theta=1e-4):\n",
        "    V = { (r, c): 0.0 for r in range(env.rows) for c in range(env.cols) }\n",
        "    for t in env.terminals:\n",
        "        V[t] = 0.0\n",
        "    iteration = 0\n",
        "    while True:\n",
        "        iteration += 1\n",
        "        delta = 0\n",
        "        for r in range(env.rows):\n",
        "            for c in range(env.cols):\n",
        "                state = (r, c)\n",
        "                if state in env.terminals:\n",
        "                    continue\n",
        "                action_values = []\n",
        "                for a in env.actions:\n",
        "                    total = 0.0\n",
        "                    # slip_prob szerint súlyozott várható érték\n",
        "                    for possible_action in env.actions:\n",
        "                        prob = env.slip_prob / (len(env.actions) - 1) if possible_action != a else 1 - env.slip_prob\n",
        "                        next_state, reward = env.step(state, possible_action)\n",
        "                        total += prob * (reward + gamma * V[next_state])\n",
        "                    action_values.append(total)\n",
        "                v_new = max(action_values)\n",
        "                delta = max(delta, abs(v_new - V[state]))\n",
        "                V[state] = v_new\n",
        "        if delta < theta:\n",
        "            break\n",
        "    # Politika származtatása\n",
        "    policy = {}\n",
        "    for r in range(env.rows):\n",
        "        for c in range(env.cols):\n",
        "            state = (r, c)\n",
        "            if state in env.terminals:\n",
        "                continue\n",
        "            action_values = {}\n",
        "            for a in env.actions:\n",
        "                total = 0.0\n",
        "                for possible_action in env.actions:\n",
        "                    prob = env.slip_prob / (len(env.actions) - 1) if possible_action != a else 1 - env.slip_prob\n",
        "                    next_state, reward = env.step(state, possible_action)\n",
        "                    total += prob * (reward + gamma * V[next_state])\n",
        "                action_values[a] = total\n",
        "            policy[state] = max(action_values, key=action_values.get)\n",
        "    return V, policy, iteration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c913310c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Példa: sztochasztikus 4x4 környezet slip_prob=0.2\n",
        "env_stochastic = Gridworld(4, 4, [(3, 3)], slip_prob=0.2)\n",
        "V_stoch, policy_stoch, it_stoch = value_iteration_stochastic(env_stochastic, gamma=0.9, theta=1e-3)\n",
        "print(f'Sztochasztikus értékiteráció {it_stoch} iteráció alatt konvergált.')\n",
        "print('Optimális politika sztochasztikus környezetben:')\n",
        "print('\n",
        "'.join([' '.join([policy_stoch.get((r, c), 'T') for c in range(4)]) for r in range(4)]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b143163",
      "metadata": {},
      "source": [
        "## 4. FrozenLake-szerű környezet létrehozása\n",
        "\n",
        "**Feladat:** Implementálják a fenti Gridworld osztály speciális példányát, ahol néhány állapot lyukként (negatív jutalom, azonnali terminál) viselkedik, és a célállapot pozitív jutalmat ad. Ezt követően futtassák rajta a value iteration vagy policy iteration algoritmust.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67a175c7",
      "metadata": {},
      "source": [
        "## 5. Összefoglaló\n",
        "\n",
        "Ezek a kísérletek segítenek abban, hogy mélyebben megértsük, hogyan hatnak a hiperparaméterek a dinamikus programozási algoritmusokra és hogyan viselkednek ezek különböző környezetekben. Vizsgáljuk meg a változásokat vizuálisan is, és beszéljük meg a tanulságokat.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
