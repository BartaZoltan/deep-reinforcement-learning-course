{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "71323ea5-f77e-42cd-8b5e-c1ff89e2c6b2",
      "metadata": {},
      "source": [
        "![Logo](https://raw.githubusercontent.com/BartaZoltan/deep-reinforcement-learning-course/main/notebooks/shared_assets/logo.png)\n\n\n**Developers:** Domonkos Nagy, Balázs Nagy, Zoltán Barta  \n**Date:** 2026-02-17  \n**Version:** 2025-26/2\n\n[<img src=\"https://colab.research.google.com/assets/colab-badge.svg\">](https://colab.research.google.com/github/BartaZoltan/deep-reinforcement-learning-course/blob/main/notebooks/sessions/session_01_k_armed_bandit/session_01_k_armed_bandit_empty.ipynb)\n\n# Practice 1: K-armed Bandit\n\n## Summary\n\nThis notebook introduces the **k-armed bandit** setting and the exploration-exploitation tradeoff in reinforcement learning.\n\nContent outline:\n- k-armed bandit problem formulation and evaluation setup,\n- strategy interface and environment construction,\n- epsilon-greedy action selection,\n- upper-confidence-bound (UCB) action selection,\n- gradient bandit methods,\n- comparative experiments and result interpretation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n",
        "\n",
        "Consider the following learning problem. You are faced repeatedly with a choice among\n",
        "$k$ different options, or actions. After each choice, you receive a numerical reward\n",
        "from a stationary probability distribution that depends on the action you selected. Your\n",
        "objective is to maximize the expected total reward over some time period, for example,\n",
        "over 1000 action selections, or time steps.\n",
        "\n",
        "This is called the $k$-armed bandit problem. You can visualize this problem as having to\n",
        "choose between $k$ slot machines (also known as one-armed bandits) at each time step,\n",
        "each of which has a different probability distribution for rewards - that is where the name comes from.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/BartaZoltan/deep-reinforcement-learning-course/main/notebooks/sessions/session_01_k_armed_bandit/assets/k_armed_bandit.png\" width=\"500\"/>\n",
        "\n",
        "The $k$-armed bandit problem illustrates an important problem in reinforcement\n",
        "learning: **exploration vs. exploitation**. At each time step $t$, the agent has to make a decision:\n",
        "take the action with the highest expected reward according to its current knowledge of the environment, \n",
        "or choose a different action to get a better estimation of the value of that action. The former is called an\n",
        "*exploitation* step, because it exploits the current knowledge of the agent in order to obtain a high reward.\n",
        "The latter is called an *exploration* step, since it involves trying out an action in order to have a better\n",
        "estimation of its value, thereby exploring the environment.\n",
        "\n",
        "This notebook introduces a few common strategies to tackle this problem and puts them to the test by simulating\n",
        "multiple test runs, and comparing the results.\n",
        "\n",
        "This notebook follows Chapter 2 of Sutton & Barto {cite}`sutton2018`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d06156ef-5366-4790-af7c-a5b9d31278c6",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom abc import ABC, abstractmethod\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import trange\nimport seaborn as sns\nimport time\n\nimport random\n\nSEED = 42\nnp.random.seed(SEED)\nrandom.seed(SEED)\n\ntry:\n    import torch\n    torch.manual_seed(SEED)\nexcept Exception:\n    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13c49f5e",
      "metadata": {},
      "source": [
        "### Why the seed setup matters\n",
        "\n",
        "Bandit methods are stochastic in two ways: the environment generates random rewards and the policy often explores randomly. \n",
        "Setting a fixed seed makes experiments reproducible and comparable across strategy variants.\n",
        "\n",
        "This follows the testbed philosophy in Chapter 2: compare methods under controlled conditions and average over many runs {cite}`sutton2018`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "184eca01-435a-4c1e-a58d-4374e4b75123",
      "metadata": {},
      "source": [
        "## Strategy setup\n",
        "\n",
        "The `Strategy` base class is used to implement startegies for action selection. An action is selected by the `act` method, and then the `update` method is used\n",
        "to update the inner state after receiving a reward for the selected action. After an episode (a \"run\" consisting of $n$ steps, 1000 for example) is over, the `reset` method is called to reset the inner state of the class. The `name` propery is used get a name for the strategy in a visual representation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3f1ff81-334f-4c0b-bf3d-eade83e9c488",
      "metadata": {},
      "outputs": [],
      "source": [
        "# DO NOT MODIFY THIS CELL\n\nclass Strategy(ABC):\n\n    def __init__(self, k):\n        self.k = k  # Number of actions\n         \n        self.rewards_history = {i: [] for i in range(self.k)}  # Store observed rewards per action\n\n    @property\n    @abstractmethod\n    def name():\n        pass\n\n    @abstractmethod\n    def act(self):\n        pass\n\n    @abstractmethod\n    def update(self, action, reward):\n        self.rewards_history[action].append(reward)  # Update rewards history\n        pass\n\n    @abstractmethod\n    def reset(self):\n        self.rewards_history = {i: [] for i in range(self.k)}  # Reset reward history\n        pass\n\n    def plot_estimated_distributions(self):\n        \"\"\"\n        Plots the estimated reward distributions for each action\n        based on the rewards observed during training.\n        \"\"\"\n        plt.figure(figsize=(12, 5))\n        for action, rewards in self.rewards_history.items():\n            if rewards:\n                sns.kdeplot(rewards, label=f\"Action {action+1}\", fill=True, alpha=0.5)\n        \n        plt.xlabel(\"Estimated Reward\")\n        plt.ylabel(\"Density\")\n        plt.title(f\"Estimated Reward Distributions - {self.name}\")\n        plt.legend()\n        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0528443-6a12-4109-b9cd-a42fc724919e",
      "metadata": {},
      "source": [
        "Numpy's `np.argmax` will choose the smallest index in case there are multiple\n",
        "maximal values. This function breaks these ties randomly instead, which is\n",
        "desirable in many cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b48f7a0-8634-4980-8503-1d1567b5033f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# DO NOT MODIFY THIS CELL\n\n# Argmax function that breaks ties randomly\ndef argmax(arr):\n    arr_max = np.max(arr)\n    return np.random.choice(np.where(arr == arr_max)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "330c7d13",
      "metadata": {},
      "source": [
        "## Environment Setup\n",
        "\n",
        "We test our strategies by trying them out in multiple runs, and then averaging out the received reward at each time step. After that, we plot the results to\n",
        "compare the strategies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aac7ab88",
      "metadata": {},
      "outputs": [],
      "source": [
        "class KArmedBandit:\n    def __init__(self, K, mean=0, std_dev=1):\n        # TODO: initialize bandit parameters and true action values\n        pass\n\n    def get_reward(self, action):\n        # TODO: sample stochastic reward for the selected action\n        pass\n\n    def get_optimal_action(self):\n        # TODO: return current best action index\n        pass\n\n    def reset(self):\n        # TODO: reset true action values using initialization distribution\n        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2537cbc4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_bandit_distributions(bandit, num_samples=10000):\n    \"\"\"\n    Plots the reward distributions for all K actions in a K-armed bandit using a violin plot.\n\n    Parameters:\n    - bandit (KArmedBandit): An instance of the KArmedBandit class.\n    - num_samples (int): Number of reward samples to generate for each action.\n    \"\"\"\n    K = bandit.K  # Number of actions\n    rewards = {action: [bandit.get_reward(action) for _ in range(num_samples)] for action in range(K)}\n\n    # Convert to data format suitable for seaborn\n    reward_data = []\n    action_labels = []\n    \n    for action, reward_list in rewards.items():\n        reward_data.extend(reward_list)\n        action_labels.extend([action + 1] * num_samples)  # Convert 0-indexed to 1-indexed for display\n\n    # Create violin plot\n    plt.figure(figsize=(12, 5))\n    sns.violinplot(x=action_labels, y=reward_data, inner=None, color=\"lightblue\", linewidth=1.5)\n\n    # Add scatter points for true action values\n    plt.scatter(range(0, K ), bandit.optimal_action_values, color='blue', s=50)\n\n    # Formatting\n    plt.axhline(0, linestyle='dotted', color='black', linewidth=1)  # Dashed line at 0\n    plt.xlabel(\"Actions\")\n    plt.ylabel(\"Expected Reward\")\n    plt.title(\"Action Reward Distributions in K-Armed Bandit\")\n    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78397eb7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a 10-armed bandit\nbandit = KArmedBandit(K=10)\n\n# Plot the full action reward distributions\nplot_bandit_distributions(bandit)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b57832c",
      "metadata": {},
      "source": [
        "### Evaluation protocol\n\nThe `simulate(...)` function implements the main empirical protocol:\n- independent runs with fresh bandits,\n- fixed number of steps per run,\n- averaged reward curves,\n- frequency of selecting the optimal action.\n\nThese are the two canonical diagnostics used throughout Sutton & Barto’s Chapter 2 comparisons {cite}`sutton2018`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ffca1be",
      "metadata": {},
      "outputs": [],
      "source": [
        "def simulate(strategies, K, bandit_mean=0, bandit_std=1, runs=2000, n_steps=1000):\n    \"\"\"TODO: run repeated bandit experiments and return mean reward / optimal-action curves.\"\"\"\n    # Expected outputs:\n    # - mean_rewards shape: (n_strategies, n_steps)\n    # - mean_best_action_choices shape: (n_strategies, n_steps)\n    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a337157",
      "metadata": {},
      "source": [
        "To examine the results a plot function is defined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4691eaed",
      "metadata": {},
      "outputs": [],
      "source": [
        "# DO NOT MODIFY THIS CELL\n\ndef plotResults(strategies, rewards, best_action_choices):\n  fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(14, 5))\n\n  for strategy, reward in zip(strategies, rewards):\n      ax1.plot(reward, label=f\"{strategy.name}\", zorder=2)\n  ax1.set_xlabel('Steps')\n  ax1.set_ylabel('Average reward')\n  ax1.grid(alpha=0.8, linestyle=':', zorder=0)\n  ax1.set_title('Average reward of strategies')\n  ax1.legend()\n\n  for strategy, choices in zip(strategies, best_action_choices):\n      ax2.plot(choices, label=f\"{strategy.name}\")\n  ax2.set_xlabel('Steps')\n  ax2.set_ylabel('% Optimal action')\n  ax2.grid(alpha=0.8, linestyle=':', zorder=0)\n  ax2.set_title('% Optimal action choices of strategies')\n  ax2.legend()\n\n  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79096804-a20e-499d-abdc-4a187ab3ac01",
      "metadata": {},
      "source": [
        "## $\\varepsilon$-greedy Action Selection\n",
        "\n",
        "With this method, the agent will select a random action with an $\\varepsilon$ probability ($0 \\le \\varepsilon \\le 1$), and act greedily (select the best action according to its knowledge) otherwise. The action values are calculated using the *sample-averages* method: the value of an action is the average of all the rewards received after taking that action.\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/BartaZoltan/deep-reinforcement-learning-course/main/notebooks/sessions/session_01_k_armed_bandit/assets/epsilon_greedy.png\" width=\"700\"/>\n",
        "\n",
        "*Pseudocode adapted from Sutton & Barto {cite}`sutton2018` (p. 32).*\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04f8fd71",
      "metadata": {},
      "outputs": [],
      "source": [
        "class EpsilonGreedy(Strategy):\n    def __init__(self, k, epsilon=0, initial=0, step_size=None):\n        super().__init__(k)\n        # TODO: initialize epsilon-greedy state\n        pass\n\n    @property\n    def name(self):\n        # TODO: return readable strategy name\n        pass\n\n    def act(self):\n        # TODO: implement epsilon-greedy action selection (with random tie-break)\n        pass\n\n    def update(self, action, reward):\n        # TODO: update action-value estimate (sample-average or constant step-size)\n        pass\n\n    def reset(self):\n        # TODO: reset internal state\n        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "089671d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\n\ndef plot_and_save(strategy, step,save_path = \"assets/bandit_training.png\"):\n    \"\"\"\n    Creates and updates the figure, then saves it to disk.\n\n    Parameters:\n    - strategy (EpsilonGreedy): The current strategy with reward history.\n    - step (int): Current training step.\n    \"\"\"\n    K = strategy.k\n\n    # Create new figure inside the function\n    fig, ax = plt.subplots(figsize=(12, 5))\n\n    # Prepare data\n    reward_data = []\n    action_labels = []\n\n    for action, reward_list in strategy.rewards_history.items():\n        reward_data.extend(reward_list)\n        action_labels.extend([action+1] * len(reward_list))\n\n    # Create violin plot\n    sns.violinplot(x=action_labels, y=reward_data, inner=None, density_norm=\"width\", color=\"lightcoral\", linewidth=1.5, ax=ax)\n\n    # Scatter plot for estimated means\n    ax.scatter(range(K), strategy.q_estimations, color='red', s=50, zorder=3, label=\"Estimated Means\")\n\n    ax.axhline(0, linestyle='dotted', color='black', linewidth=1)\n    ax.set_xlabel(\"Actions\")\n    ax.set_ylabel(\"Estimated Reward\")\n    ax.set_title(f\"Training Step: {step} - Estimated Reward Distributions\")\n    ax.legend([\"Estimated Means\"])\n\n    # Save the figure to the same file (overwrite each time)\n    fig.savefig(save_path, format=\"png\", dpi=150)\n    plt.close(fig)  # Close figure to free memory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f55938e",
      "metadata": {},
      "source": [
        "Visualize, how the $\\varepsilon$-greedy Action Selection updates it's estimations of the reward distributions of the K-Armed bandit problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fadba576",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize bandit and strategy\nfrom pathlib import Path\nfrom PIL import Image\nfrom IPython.display import Image as IPyImage, display\n\nK = 10  # Number of arms\nbandit = KArmedBandit(K)\nstrategy = EpsilonGreedy(k=K, epsilon=0.2)\n\n# Training loop with in-memory frame capture for GIF export\nn_steps = 5000\nplot_interval = n_steps // 50\n\n# Plot initial bandit distributions before training\nplot_bandit_distributions(bandit)\n\ntmp_frame = Path(\"assets/.tmp_bandit_frame.png\")\nframes = []\n\nfor step in range(n_steps + 1):\n    action = strategy.act()\n    reward = bandit.get_reward(action)\n    strategy.update(action, reward)\n\n    if step % plot_interval == 0:\n        plot_and_save(strategy, step, save_path=str(tmp_frame))\n        frames.append(Image.open(tmp_frame).copy())\n\n# Build GIF from captured frames (for website use)\ngif_path = Path(\"assets/bandit_training.gif\")\nif frames:\n    frames[0].save(\n        gif_path,\n        save_all=True,\n        append_images=frames[1:],\n        duration=120,\n        loop=0,\n    )\n\nfor fr in frames:\n    fr.close()\n\ntmp_frame.unlink(missing_ok=True)\n\nprint(f\"Saved GIF to: {gif_path}\")\ndisplay(IPyImage(filename=str(gif_path)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c97e4fcb",
      "metadata": {},
      "source": [
        "Test of $\\varepsilon$-greedy Action Selection."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf718f1a",
      "metadata": {},
      "source": [
        "### Epsilon sensitivity\n\nCompare greedy (`epsilon=0`) and exploratory (`epsilon>0`) behavior.\n\nWhat to check while reading the plots:\n- short-term reward vs long-term reward,\n- how quickly each method discovers good arms,\n- whether pure greedy gets stuck because it under-explores.\n\nThis directly matches the exploration-exploitation discussion in Section 2.1–2.2 {cite}`sutton2018`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14bee753",
      "metadata": {},
      "outputs": [],
      "source": [
        "K = 3  # Number of actions\n\n# List of strategies to test\nstrategies = [EpsilonGreedy(K,epsilon=0.0), \n              EpsilonGreedy(K, epsilon=0.1), \n              EpsilonGreedy(K, epsilon=0.01)]\n\n# Evaluate strategies\nrewards, best_action_choices = simulate(strategies, K=K, runs=200, n_steps=500)\n\nplotResults(strategies, rewards, best_action_choices)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddb244f0",
      "metadata": {},
      "source": [
        "### Optimistic initialization and step-size\n\nHere the initial action-value estimates are intentionally high (`initial > 0`), which induces early exploration even with small `epsilon`.\n\nAlso note the role of `step_size`:\n- sample-average update (`1/N`) gives long memory,\n- constant step-size gives faster adaptation.\n\nSee the action-value update discussion in Chapter 2 {cite}`sutton2018`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e238295e",
      "metadata": {},
      "outputs": [],
      "source": [
        "K = 10  # Number of actions\n\n# List of strategies to test\nstrategies = [EpsilonGreedy(K,epsilon=0.0, initial=10,step_size=0.1), \n              EpsilonGreedy(K, epsilon=0.1,initial=3,step_size=0.1), \n              EpsilonGreedy(K, epsilon=0.1, initial=0,step_size=0.1)]\n\n# Evaluate strategies\nrewards, best_action_choices = simulate(strategies, K=K, runs=200, n_steps=500)\n\nplotResults(strategies, rewards, best_action_choices)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "496a7400-b006-4b32-a3b4-dff353377a61",
      "metadata": {},
      "source": [
        "## Upper-Confidence-Bound (UCB) Action Selection\n",
        "\n",
        "The UCB action selection method offers a way to select an action by taking both the estimated value, as well as the accuracy of those estimates into account.\n",
        "It uses the following formula:\n",
        "\n",
        "$$ A_t := \\underset{a}{\\arg\\max} \\left[ Q_t(a) + c \\sqrt{\\frac{\\ln(t)}{N_t(a)}} \\right] $$\n",
        "\n",
        "Where $Q_t(a)$ denotes the value of action $a$ (calculated using the *sample-averages* method), $N_t(a)$ denotes the number of times that action $a$ has\n",
        "been selected prior to time $t$, and the number $c > 0$ controls\n",
        "the degree of exploration. If $N_t(a) = 0$, then $a$ is considered to be a maximizing action.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f904719",
      "metadata": {},
      "outputs": [],
      "source": [
        "class UCB(Strategy):\n    def __init__(self, k, c=1, initial=0, step_size=None):\n        super().__init__(k)\n        # TODO: initialize UCB parameters and internal state\n        pass\n\n    @property\n    def name(self):\n        # TODO: return readable strategy name\n        pass\n\n    def act(self):\n        # TODO: implement UCB action selection\n        # hint: ensure each action is selected at least once\n        pass\n\n    def update(self, action, reward):\n        # TODO: update counts and action-value estimate\n        pass\n\n    def reset(self):\n        # TODO: reset internal state\n        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ab366d3",
      "metadata": {},
      "source": [
        "Test of UCB."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfd49fc8",
      "metadata": {},
      "source": [
        "### UCB exploration coefficient\n\nUCB uses an explicit uncertainty bonus, controlled by `c`.\n\nInterpretation goal:\n- small `c`: more exploitation,\n- large `c`: more exploration pressure.\n\nThis is the confidence-bound approach from Section 2.7 {cite}`sutton2018`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33bfc6e9",
      "metadata": {},
      "outputs": [],
      "source": [
        "K = 3  # Number of actions\n\n# List of strategies to test\nstrategies = [UCB(K),\n              UCB(K, c=2),\n              UCB(K, c=5)]\n\n# Evaluate strategies\nrewards, best_action_choices = simulate(strategies, K=K, runs=200, n_steps=500)\n\nplotResults(strategies, rewards, best_action_choices)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "651c4cb1-b701-4a1e-840f-10ab0f3ccee1",
      "metadata": {},
      "source": [
        "## Gradient Bandit Algorithms\n",
        "\n",
        "Instead of estimating action values, this method learns a numerical *preference*, denoted $H_t(a)$ for each action. The larger the preference, the more often that action is taken, but the preference has no interpretation in terms of reward. Action probabilites are determined using the *soft-max* function:\n",
        "\n",
        "$$ \\pi_t(a) := \\Pr\\{A_t = a\\} := \\frac{e^{H_t(a)}}{\\sum_{b=1}^k e^{H_t(b)}} $$\n",
        "\n",
        "Here we have also introduced a useful new notation, $\\pi_t(a)$, for the probability of\n",
        "taking action $a$ at time $t$. Note that this function defines a probability distribution over the set of all actions. On each step, after selecting action $A_t$ and receiving the reward $R_t$, the\n",
        "action preferences are updated by:\n",
        "\n",
        "$$ H_{t+1}(a) := H_t(a) + \\alpha(R_t - \\bar{R}_t)(\\mathbb{1}_{a=A_t} - \\pi_t(a)) $$\n",
        "\n",
        "Where $\\alpha > 0$ is a step-size parameter, and $\\bar{R}_t \\in \\mathbb{R}$ is the average of all the rewards up\n",
        "through and including time $t$. \n",
        "The $\\bar{R}_t$ term serves as a\n",
        "baseline with which the reward is compared.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5fa8dd6",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Gradient(Strategy):\n    def __init__(self, k, step_size=0.1, use_baseline=True):\n        super().__init__(k)\n        # TODO: initialize gradient bandit state\n        pass\n\n    @property\n    def name(self):\n        # TODO: return readable strategy name\n        pass\n\n    def act(self):\n        # TODO: sample action from softmax(preferences)\n        pass\n\n    def update(self, action, reward):\n        # TODO: implement gradient update with optional baseline\n        pass\n\n    def reset(self):\n        # TODO: reset preferences, baseline and timestep\n        pass\n\n    def softmax(self, x):\n        # TODO: implement numerically stable softmax\n        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29255ed2",
      "metadata": {},
      "source": [
        "Test of Gradient Bandit Algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bae4267a",
      "metadata": {},
      "source": [
        "### Gradient bandit hyperparameters\n\nGradient bandits optimize action preferences via a softmax policy.\n\nFocus points:\n- effect of `alpha` (step-size) on stability/speed,\n- effect of using a baseline (variance reduction).\n\nThis corresponds to Section 2.8 (gradient bandit algorithm) {cite}`sutton2018`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4da57d3b",
      "metadata": {},
      "outputs": [],
      "source": [
        "K = 3  # Number of actions\n\n# List of strategies to test\nstrategies = [Gradient(K,step_size=0.1),\n              Gradient(K, step_size=0.4),\n              Gradient(K, step_size=0.01)]\n\n# Evaluate strategies\nrewards, best_action_choices = simulate(strategies, K=K, runs=200, n_steps=500)\n\nplotResults(strategies, rewards, best_action_choices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de04de2a",
      "metadata": {},
      "outputs": [],
      "source": [
        "K = 10  # Number of actions\n\n# List of strategies to test\nstrategies = [Gradient(K,step_size=0.1),\n             #Gradient(K, step_size=0.4),\n              Gradient(K, step_size=0.1, use_baseline=False),\n             #Gradient(K, step_size=0.4, use_baseline=False)\n             ]\n\n# Evaluate strategies\nrewards, best_action_choices = simulate(strategies, K=K,bandit_mean=4, runs=200, n_steps=1000)\n\nplotResults(strategies, rewards, best_action_choices)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf9f32e2",
      "metadata": {},
      "source": [
        "## Comprehensive test\n",
        "\n",
        "In this final section let's run a longer comprehensive test with more actions.\n",
        "\n",
        "The comparison setup is aligned with the classical ten-armed testbed style in {cite}`zhang_ten_armed`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53774a19",
      "metadata": {},
      "outputs": [],
      "source": [
        "K = 5  # Number of actions\n\n# List of strategies to test\nstrategies = [\n        EpsilonGreedy(K),\n        EpsilonGreedy(K, epsilon=0.1),\n        UCB(K, c=2),\n        Gradient(K)\n    ]\n\n# Evaluate strategies\nrewards, best_action_choices = simulate(strategies, K=K, runs=2000, n_steps=500)\n\nplotResults(strategies, rewards, best_action_choices)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "379b3b1e",
      "metadata": {},
      "source": [
        "## Non-stationary bandits and constant learning rate\n",
        "\n",
        "So far we used stationary bandits, where each arm has a fixed true value.\n",
        "In a **non-stationary** setting, the true action values drift over time.\n",
        "\n",
        "In this case, sample-average updates (step-size `1/N`) can react too slowly, because they keep very long memory.\n",
        "A constant step-size `\\alpha` tracks recent changes better, as discussed in Sutton & Barto {cite}`sutton2018`.\n",
        "\n",
        "Below we compare:\n",
        "- `\\varepsilon`-greedy with sample-average updates,\n",
        "- `\\varepsilon`-greedy with constant step-size updates.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df71796b",
      "metadata": {},
      "source": [
        "### Why non-stationarity changes the conclusion\n",
        "\n",
        "When true action values drift, old data becomes less reliable.\n",
        "\n",
        "Key takeaway to verify in plots:\n",
        "- sample-average update can lag behind changing optima,\n",
        "- constant `alpha` tracks drift better because it weights recent rewards more.\n",
        "\n",
        "This is the main motivation behind constant step-size updates in non-stationary bandits (Section 2.5) {cite}`sutton2018`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f84813c",
      "metadata": {},
      "outputs": [],
      "source": [
        "class NonStationaryKArmedBandit:\n    def __init__(self, K, mean=0.0, std_dev=1.0, drift_std=0.01, reward_std=1.0):\n        # TODO: initialize non-stationary bandit\n        pass\n\n    def step_dynamics(self):\n        # TODO: apply random-walk drift to true action values\n        pass\n\n    def get_reward(self, action):\n        # TODO: sample reward for selected action\n        pass\n\n    def get_optimal_action(self):\n        # TODO: return best action index\n        pass\n\n\ndef simulate_nonstationary(strategies, K, runs=200, n_steps=2000, drift_std=0.01, reward_std=1.0):\n    # TODO: implement non-stationary evaluation loop\n    pass\n\n\nclass EpsilonGreedyWithLabel(EpsilonGreedy):\n    def __init__(self, *args, label_suffix='', **kwargs):\n        super().__init__(*args, **kwargs)\n        self.label_suffix = label_suffix\n\n    @property\n    def name(self):\n        # TODO: readable label for plot legends\n        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a66cce0",
      "metadata": {},
      "source": [
        "## References\n```{bibliography}\n```\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "drl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "state": {
          "07ebbb9edfac425baf9a2dbb7d34c67a": {
            "model_module": "@jupyter-widgets/controls",
            "model_module_version": "2.0.0",
            "model_name": "HTMLStyleModel",
            "state": {
              "description_width": "",
              "font_size": null,
              "text_color": null
            }
          },
          "08766d7e7ba9483e889112dc5d026a10": {
            "model_module": "@jupyter-widgets/controls",
            "model_module_version": "2.0.0",
            "model_name": "HTMLModel",
            "state": {
              "layout": "IPY_MODEL_e5ecb38705794f79a3a1e5e6a308e4eb",
              "style": "IPY_MODEL_9687d120372c482cb74b71c7826d0cc6",
              "value": "100%"
            }
          },
          "0af20d17977d4ef3bc8b423bb4f994dd": {
            "model_module": "@jupyter-widgets/base",
            "model_module_version": "2.0.0",
            "model_name": "LayoutModel",
            "state": {}
          },
          "14742ccc27e140a894ca7581a8837a8e": {
            "model_module": "@jupyter-widgets/controls",
            "model_module_version": "2.0.0",
            "model_name": "HTMLStyleModel",
            "state": {
              "description_width": "",
              "font_size": null,
              "text_color": null
            }
          },
          "1a4843195585449ca0cf41a8532ce173": {
            "model_module": "@jupyter-widgets/controls",
            "model_module_version": "2.0.0",
            "model_name": "ProgressStyleModel",
            "state": {
              "description_width": ""
            }
          },
          "1e0e04f407834c56a3da5d5e777b2285": {
            "model_module": "@jupyter-widgets/base",
            "model_module_version": "2.0.0",
            "model_name": "LayoutModel",
            "state": {}
          },
          "1e3d3c8b492a42de9eed9aec4d252529": {
            "model_module": "@jupyter-widgets/controls",
            "model_module_version": "2.0.0",
            "model_name": "ProgressStyleModel",
            "state": {
              "description_width": ""
            }
          },
          "2b9c79b37c5242d3a9da596fbec0ea22": {
            "model_module": "@jupyter-widgets/controls",
            "model_module_version": "2.0.0",
            "model_name": "HTMLStyleModel",
            "state": {
              "description_width": "",
              "font_size": null,
              "text_color": null
            }
          },
          "317dc691dcac4d9681d5a0c2713f1db1": {
            "model_module": "@jupyter-widgets/base",
            "model_module_version": "2.0.0",
            "model_name": "LayoutModel",
            "state": {}
          },
          "4805e0f1568c4584a5fa7db25627e6a6": {
            "model_module": "@jupyter-widgets/controls",
            "model_module_version": "2.0.0",
            "model_name": "FloatProgressModel",
            "state": {
              "bar_style": "success",
              "layout": "IPY_MODEL_0af20d17977d4ef3bc8b423bb4f994dd",
              "max": 2000,
              "style": "IPY_MODEL_1a4843195585449ca0cf41a8532ce173",
              "value": 2000
            }
          },
          "548e34805d784045ab2d6b810edc9ebf": {
            "model_module": "@jupyter-widgets/controls",
            "model_module_version": "2.0.0",
            "model_name": "FloatProgressModel",
            "state": {
              "bar_style": "success",
              "layout": "IPY_MODEL_590dd8bd235343ca98c7715a0ded63fb",
              "max": 2000,
              "style": "IPY_MODEL_dcb0315e8f8e496ea3d3e80a0f6c6d4f",
              "value": 2000
            }
          },
          "590dd8bd235343ca98c7715a0ded63fb": {
            "model_module": "@jupyter-widgets/base",
            "model_module_version": "2.0.0",
            "model_name": "LayoutModel",
            "state": {}
          },
          "5ecda2fcb2684cef9d35f7062dd2af53": {
            "model_module": "@jupyter-widgets/base",
            "model_module_version": "2.0.0",
            "model_name": "LayoutModel",
            "state": {}
          },
          "6ab2a3b1a4944bdfbf95ba0dbe11cd12": {
            "model_module": "@jupyter-widgets/controls",
            "model_module_version": "2.0.0",
            "model_name": "HTMLStyleModel",
            "state": {
              "description_width": "",
              "font_size": null,
              "text_color": null
            }
          },
          "6b8afc7c172f410d937031881e6bf8cf": {
            "model_module": "@jupyter-widgets/base",
            "model_module_version": "2.0.0",
            "model_name": "LayoutModel",
            "state": {}
          },
          "6d27ff26fef344a6882007ad47030213": {
            "model_module": "@jupyter-widgets/base",
            "model_module_version": "2.0.0",
            "model_name": "LayoutModel",
            "state": {}
          },
          "75513d022da1452a952aff7f074f9964": {
            "model_module": "@jupyter-widgets/controls",
            "model_module_version": "2.0.0",
            "model_name": "HTMLStyleModel",
            "state": {
              "description_width": "",
              "font_size": null,
              "text_color": null
            }
          },
          "755413d3284f4720a61398776719798e": {
            "model_module": "@jupyter-widgets/base",
            "model_module_version": "2.0.0",
            "model_name": "LayoutModel",
            "state": {}
          },
          "75c9a46c4e3b4a429e09a4a05ddec642": {
            "model_module": "@jupyter-widgets/controls",
            "model_module_version": "2.0.0",
            "model_name": "HTMLStyleModel",
            "state": {
              "description_width": "",
              "font_size": null,
              "text_color": null
            }
          },
          "7b11c9fb1e2f4cc9b1fa70edd23a3301": {
            "model_module": "@jupyter-widgets/controls",
            "model_module_version": "2.0.0",
            "model_name": "HTMLModel",
            "state": {
              "layout": "IPY_MODEL_c78ffad4cdc042f3afe5efc727eabb31",
              "style": "IPY_MODEL_2b9c79b37c5242d3a9da596fbec0ea22",
              "value": "100%"
            }
          },
          "7ccb1685b35e419e9d43abfe152bd306": {
            "model_module": "@jupyter-widgets/base",
            "model_module_version": "2.0.0",
            "model_name": "LayoutModel",
            "state": {}
          },
          "8952aaa7bb334b1ab795d51432c1fe9a": {
            "model_module": "@jupyter-widgets/controls",
            "model_module_version": "2.0.0",
            "model_name": "HTMLModel",
            "state": {
              "layout": "IPY_MODEL_7ccb1685b35e419e9d43abfe152bd306",
              "style": "IPY_MODEL_e8d730fe69314cd78fddf187d4662416",
              "value": " 2000/2000 [01:04&lt;00:00, 31.99it/s]"
            }
          },
          "8db846b5eb7043999faa9bbb9cfa82b8": {
            "model_module": "@jupyter-widgets/controls",
            "model_module_version": "2.0.0",
            "model_name": "ProgressStyleModel",
            "state": {
              "description_width": ""
            }
          },
          "9687d120372c482cb74b71c7826d0cc6": {
            "model_module": "@jupyter-widgets/controls",
            "model_module_version": "2.0.0",
            "model_name": "HTMLStyleModel",
            "state": {
              "description_width": "",
              "font_size": null,
              "text_color": null
            }
          },
          "a7760f48fe6c4720a61c8cf8573c2634": {
            "model_module": "@jupyter-widgets/base",
            "model_module_version": "2.0.0",
            "model_name": "LayoutModel",
            "state": {}
          },
          "a7e8cca8ac1646cfa2f2202f07f0f135": {
            "model_module": "@jupyter-widgets/controls",
            "model_module_version": "2.0.0",
            "model_name": "HBoxModel",
            "state": {
              "children": [
                "IPY_MODEL_08766d7e7ba9483e889112dc5d026a10",
                "IPY_MODEL_c2762225efb04bc0af5daf619bd9989f",
                "IPY_MODEL_8952aaa7bb334b1ab795d51432c1fe9a"
              ],
              "layout": "IPY_MODEL_b9ad02240b1b4daca607fb7de9765cb7"
            }
          },
          "ae082241625f42b49f7d0ea17aabf42e": {
            "model_module": "@jupyter-widgets/controls",
            "model_module_version": "2.0.0",
            "model_name": "FloatProgressModel",
            "state": {
              "bar_style": "success",
              "layout": "IPY_MODEL_6b8afc7c172f410d937031881e6bf8cf",
              "max": 2000,
              "style": "IPY_MODEL_1e3d3c8b492a42de9eed9aec4d252529",
              "value": 2000
            }
          },
          "b12033f10b0c41bd91a85f257d33034a": {
            "model_module": "@jupyter-widgets/base",
            "model_module_version": "2.0.0",
            "model_name": "LayoutModel",
            "state": {}
          },
          "b491177b51504242b2150455e40391e8": {
            "model_module": "@jupyter-widgets/controls",
            "model_module_version": "2.0.0",
            "model_name": "HTMLModel",
            "state": {
              "layout": "IPY_MODEL_5ecda2fcb2684cef9d35f7062dd2af53",
              "style": "IPY_MODEL_07ebbb9edfac425baf9a2dbb7d34c67a",
              "value": " 2000/2000 [01:50&lt;00:00, 18.25it/s]"
            }
          },
          "b7eacbdc3f1049d282a40f13db58a1db": {
            "model_module": "@jupyter-widgets/controls",
            "model_module_version": "2.0.0",
            "model_name": "HBoxModel",
            "state": {
              "children": [
                "IPY_MODEL_c855a5a97f354c72ada6fc3d6907fd85",
                "IPY_MODEL_548e34805d784045ab2d6b810edc9ebf",
                "IPY_MODEL_cdf8f62184534aa39a6fa121d64ec6c0"
              ],
              "layout": "IPY_MODEL_b12033f10b0c41bd91a85f257d33034a"
            }
          },
          "b9ad02240b1b4daca607fb7de9765cb7": {
            "model_module": "@jupyter-widgets/base",
            "model_module_version": "2.0.0",
            "model_name": "LayoutModel",
            "state": {}
          },
          "c2762225efb04bc0af5daf619bd9989f": {
            "model_module": "@jupyter-widgets/controls",
            "model_module_version": "2.0.0",
            "model_name": "FloatProgressModel",
            "state": {
              "bar_style": "success",
              "layout": "IPY_MODEL_dddb73e1746e4faa8caa1f1c092e7094",
              "max": 2000,
              "style": "IPY_MODEL_8db846b5eb7043999faa9bbb9cfa82b8",
              "value": 2000
            }
          },
          "c78ffad4cdc042f3afe5efc727eabb31": {
            "model_module": "@jupyter-widgets/base",
            "model_module_version": "2.0.0",
            "model_name": "LayoutModel",
            "state": {}
          },
          "c855a5a97f354c72ada6fc3d6907fd85": {
            "model_module": "@jupyter-widgets/controls",
            "model_module_version": "2.0.0",
            "model_name": "HTMLModel",
            "state": {
              "layout": "IPY_MODEL_317dc691dcac4d9681d5a0c2713f1db1",
              "style": "IPY_MODEL_75513d022da1452a952aff7f074f9964",
              "value": "100%"
            }
          },
          "cdf8f62184534aa39a6fa121d64ec6c0": {
            "model_module": "@jupyter-widgets/controls",
            "model_module_version": "2.0.0",
            "model_name": "HTMLModel",
            "state": {
              "layout": "IPY_MODEL_755413d3284f4720a61398776719798e",
              "style": "IPY_MODEL_6ab2a3b1a4944bdfbf95ba0dbe11cd12",
              "value": " 2000/2000 [01:06&lt;00:00, 28.54it/s]"
            }
          },
          "d85d7d1fcfd14f778606850604811de6": {
            "model_module": "@jupyter-widgets/controls",
            "model_module_version": "2.0.0",
            "model_name": "HBoxModel",
            "state": {
              "children": [
                "IPY_MODEL_7b11c9fb1e2f4cc9b1fa70edd23a3301",
                "IPY_MODEL_4805e0f1568c4584a5fa7db25627e6a6",
                "IPY_MODEL_dd6de22286e741abb1d66494a2b2192f"
              ],
              "layout": "IPY_MODEL_6d27ff26fef344a6882007ad47030213"
            }
          },
          "dcb0315e8f8e496ea3d3e80a0f6c6d4f": {
            "model_module": "@jupyter-widgets/controls",
            "model_module_version": "2.0.0",
            "model_name": "ProgressStyleModel",
            "state": {
              "description_width": ""
            }
          },
          "dd6de22286e741abb1d66494a2b2192f": {
            "model_module": "@jupyter-widgets/controls",
            "model_module_version": "2.0.0",
            "model_name": "HTMLModel",
            "state": {
              "layout": "IPY_MODEL_1e0e04f407834c56a3da5d5e777b2285",
              "style": "IPY_MODEL_14742ccc27e140a894ca7581a8837a8e",
              "value": " 2000/2000 [01:33&lt;00:00, 22.06it/s]"
            }
          },
          "dddb73e1746e4faa8caa1f1c092e7094": {
            "model_module": "@jupyter-widgets/base",
            "model_module_version": "2.0.0",
            "model_name": "LayoutModel",
            "state": {}
          },
          "e5ecb38705794f79a3a1e5e6a308e4eb": {
            "model_module": "@jupyter-widgets/base",
            "model_module_version": "2.0.0",
            "model_name": "LayoutModel",
            "state": {}
          },
          "e8d730fe69314cd78fddf187d4662416": {
            "model_module": "@jupyter-widgets/controls",
            "model_module_version": "2.0.0",
            "model_name": "HTMLStyleModel",
            "state": {
              "description_width": "",
              "font_size": null,
              "text_color": null
            }
          },
          "eaa0458b4e3a4ff2b49022e1620b53c4": {
            "model_module": "@jupyter-widgets/base",
            "model_module_version": "2.0.0",
            "model_name": "LayoutModel",
            "state": {}
          },
          "fb4e5dc2536e4f8eb9c70fd4cc3b311b": {
            "model_module": "@jupyter-widgets/controls",
            "model_module_version": "2.0.0",
            "model_name": "HTMLModel",
            "state": {
              "layout": "IPY_MODEL_a7760f48fe6c4720a61c8cf8573c2634",
              "style": "IPY_MODEL_75c9a46c4e3b4a429e09a4a05ddec642",
              "value": "100%"
            }
          },
          "fc1e38ec9e2d42b29633452a708016af": {
            "model_module": "@jupyter-widgets/controls",
            "model_module_version": "2.0.0",
            "model_name": "HBoxModel",
            "state": {
              "children": [
                "IPY_MODEL_fb4e5dc2536e4f8eb9c70fd4cc3b311b",
                "IPY_MODEL_ae082241625f42b49f7d0ea17aabf42e",
                "IPY_MODEL_b491177b51504242b2150455e40391e8"
              ],
              "layout": "IPY_MODEL_eaa0458b4e3a4ff2b49022e1620b53c4"
            }
          }
        },
        "version_major": 2,
        "version_minor": 0
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
